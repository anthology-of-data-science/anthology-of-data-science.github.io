[
  {
    "objectID": "notebooks/deconfounding.html",
    "href": "notebooks/deconfounding.html",
    "title": "Deconfounding explained",
    "section": "",
    "text": "The original material for this demonstration was written in R by Jeroen de Mast. His original code was ported to Python by Daniel Kapitan."
  },
  {
    "objectID": "notebooks/deconfounding.html#credits",
    "href": "notebooks/deconfounding.html#credits",
    "title": "Deconfounding explained",
    "section": "",
    "text": "The original material for this demonstration was written in R by Jeroen de Mast. His original code was ported to Python by Daniel Kapitan."
  },
  {
    "objectID": "notebooks/deconfounding.html#setting-the-scene",
    "href": "notebooks/deconfounding.html#setting-the-scene",
    "title": "Deconfounding explained",
    "section": "Setting the scene",
    "text": "Setting the scene\nSuppose that we want to test whether \\(X\\) has a causal effect on \\(Y\\):\n\\[X \\longrightarrow Y\\]\nAnd also we have 1000 \\((X, Y)\\) tuples as our data and that we want to build a regressions model.\n\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n\n# setting up our experiment\nnp.random.sample(1973)\nN = 1000\nC = np.random.normal(loc=0.0, scale=1.0, size=N)\nerror_x = np.random.normal(loc=0.0, scale=1.0, size=N)\nerror_y = np.random.normal(loc=0.0, scale=0.01, size=N)\nX = 10 + 5*C + error_x\nY = 1 + 0.5*C + error_y\ndf = pd.DataFrame({'X': X, 'Y': Y, 'C': C})\n\n\nconfounded = smf.ols(\"Y ~ X\", data=df).fit()\nconfounded.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nY\nR-squared:\n0.960\n\n\nModel:\nOLS\nAdj. R-squared:\n0.960\n\n\nMethod:\nLeast Squares\nF-statistic:\n2.385e+04\n\n\nDate:\nThu, 21 Dec 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n23:47:40\nLog-Likelihood:\n854.10\n\n\nNo. Observations:\n1000\nAIC:\n-1704.\n\n\nDf Residuals:\n998\nBIC:\n-1694.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0446\n0.007\n6.389\n0.000\n0.031\n0.058\n\n\nX\n0.0958\n0.001\n154.420\n0.000\n0.095\n0.097\n\n\n\n\n\n\nOmnibus:\n1.331\nDurbin-Watson:\n1.855\n\n\nProb(Omnibus):\n0.514\nJarque-Bera (JB):\n1.406\n\n\nSkew:\n0.080\nProb(JB):\n0.495\n\n\nKurtosis:\n2.911\nCond. No.\n24.2\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nSo our first (confounded) model yields a result that \\(Y = 0.03 + 0.1X\\). Note there can be small differences each time you re-run this notebook. But most importantly the fitted model has a high \\(R^2 = 0.95\\) and high significance \\(p = 0.0\\)!\nHowever, if you look closely at the Python code, you see that the real model has a confounder \\(C\\):\n\\[C \\longrightarrow X\\] \\[C \\longrightarrow Y\\]\nIn other words, X and Y are both causally affected by C. As a consequence, X and Y are correlated, but they do not causally affect each other. So, the regression analysis above is actually wrong, and the correlation between X and Y is called spurious. C is called a confounder.\nNow here is the great deconfounding trick: suppose that we include both X and C in the regression analysis and fit the following modelL\n\\[ Y = \\beta_0 + \\beta_1 X + \\beta_2 C + ϵ\\]\n\ndeconfounded = smf.ols(\"Y ~ X + C\", data=df).fit()\ndeconfounded.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nY\nR-squared:\n1.000\n\n\nModel:\nOLS\nAdj. R-squared:\n1.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n1.261e+06\n\n\nDate:\nThu, 21 Dec 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n23:47:40\nLog-Likelihood:\n3164.9\n\n\nNo. Observations:\n1000\nAIC:\n-6324.\n\n\nDf Residuals:\n997\nBIC:\n-6309.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.0055\n0.003\n323.164\n0.000\n0.999\n1.012\n\n\nX\n-0.0005\n0.000\n-1.671\n0.095\n-0.001\n9.05e-05\n\n\nC\n0.5023\n0.002\n316.780\n0.000\n0.499\n0.505\n\n\n\n\n\n\nOmnibus:\n2.983\nDurbin-Watson:\n2.042\n\n\nProb(Omnibus):\n0.225\nJarque-Bera (JB):\n3.073\n\n\nSkew:\n-0.063\nProb(JB):\n0.215\n\n\nKurtosis:\n3.240\nCond. No.\n122.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNote that, by including \\(C\\) as an independent variable in the regression analysis, suddenly X has stopped being significant (p=0.36)!\nThis holds in general: if the true causal relationships are as given in the second diagram, then including the confounder C in the regression analysis gives the direct effect of X onto Y (if any such direct effect exists), and the part of the correlation that is induced by the confounder C is now entirely attributed to C and not to X. This approach is called “deconfounding”."
  },
  {
    "objectID": "notebooks/overfitting.html",
    "href": "notebooks/overfitting.html",
    "title": "Demonstration of overfitting and underfitting",
    "section": "",
    "text": "import altair as alt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures"
  },
  {
    "objectID": "notebooks/overfitting.html#python-explainer---how-does-the-polynomialfeatures-function-work",
    "href": "notebooks/overfitting.html#python-explainer---how-does-the-polynomialfeatures-function-work",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Python explainer - How does the ‘PolynomialFeatures()’ function work?",
    "text": "Python explainer - How does the ‘PolynomialFeatures()’ function work?\nIn this notebook, we use the PolynomialFeatures function from sklearn, which generates n-th order polynomial features, expanding a given X matrix. Depending on the value assigned to the parameter interaction_only, the features include all main terms and interaction terms (False; default), or only the first order main terms and the interaction term only (True). So, for example PolynomialFeatures(degree = 2) generates a matrix with a column for each term in the following equation:\n\\[1 + X_1 + X_2 + X_3 + X_1^2 + X_2^2 + X_3^2 + X_1X_2 + X_1X_3 + X_2X3.\\]\nNote, the default value for degree is 2. Below, we demonstrate the workings of PolynomialFeatures() for a simple dummy matrix.\n\n# Create a dummy X matrix (3x3).\nX = np.arange(9).reshape(3, 3)\n\nprint(\"X matrix:\")\nprint(X)\nprint(\"\")\n\n# Expanded matrix containing the first order main features and the interaction features. \npoly = PolynomialFeatures(degree = 2, interaction_only=True)\nprint(\"Expanded X matrix with first order and interaction features:\")\nprint(poly.fit_transform(X))\nprint(\"\")\n\n# Expanded matrix containing the first and second order main features and the interaction features.\npoly = PolynomialFeatures(degree = 2, interaction_only=False)\nprint(\"Expanded X matrix with first and second order and interaction features:\")\nprint(poly.fit_transform(X))\n\nX matrix:\n[[0 1 2]\n [3 4 5]\n [6 7 8]]\n\nExpanded X matrix with first order and interaction features:\n[[ 1.  0.  1.  2.  0.  0.  2.]\n [ 1.  3.  4.  5. 12. 15. 20.]\n [ 1.  6.  7.  8. 42. 48. 56.]]\n\nExpanded X matrix with first and second order and interaction features:\n[[ 1.  0.  1.  2.  0.  0.  0.  1.  2.  4.]\n [ 1.  3.  4.  5.  9. 12. 15. 16. 20. 25.]\n [ 1.  6.  7.  8. 36. 42. 48. 49. 56. 64.]]\n\n\nWe observe that when we set interaction_only to False, we also get the quadratic main terms. In addition, we also observe that the first column is filled with ones. Why is that?"
  },
  {
    "objectID": "notebooks/overfitting.html#initialize-objects",
    "href": "notebooks/overfitting.html#initialize-objects",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Initialize objects",
    "text": "Initialize objects\n\n# Maximum number of degrees in polynomial.\nMAX_DEGREE = 20"
  },
  {
    "objectID": "notebooks/overfitting.html#define-functions",
    "href": "notebooks/overfitting.html#define-functions",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Define functions",
    "text": "Define functions\nWe define a number of functions that are used in this notebook down below: (1) Create and evaluate models for different levels of flexibility. (2) Plot fitted data. (3a) Plot RMSE - Evaluate model based full data set (3b) Plot RMSE - Evaluate model based on train and test set\nFunction descriptions were added using docstring in Numpy format. In case you want an explanation about a function, simply type help(function name) and you will get to see the description.\n\n1 - Create and evaluate models for different levels of flexibility\n\ndef f_create_and_evaluate_model(df_data, b_split):\n\n    \"\"\"Creates and_evaluates models for different degrees of flexibility.\n\n    Parameters\n    ----------\n    df_data : dataframe\n        Dataframe with attribute 'Y' in the first column and X features in the second column and onwards (if applicable).\n    b_split : bool\n        Should the models be based on the full data set (b_split = False) or on a train/test split (b_split = True).\n\n    Returns\n    -------\n    dataframe\n        a dataframe with features, including: degree, RMSE, and predictions.\n    \"\"\"  \n\n    # Initialize.\n    results = []\n\n    if b_split:\n        X_train, X_val, y_train, y_val = train_test_split(df_data.iloc[:, 1:], df_data.Y, test_size = 0.2, random_state = 42)\n\n    # Go through each degree of freedom from 1 to MAX_DEGREE and fit each polynomial to X features. \n    for n in range(1, MAX_DEGREE + 1):\n        \n        # Create object with framework for MAX_DEGREE polynomials.\n        poly = PolynomialFeatures(n, interaction_only=False)\n\n        # Apply X features to each of the MAX_DEGREE polynomials.\n        Xp_all = poly.fit_transform(df_data.iloc[:, 1:])\n\n        if not b_split:\n\n            # Fit MAX_DEGREE polynomials to the same response 'Y'.\n            fit = LinearRegression().fit(Xp_all, df_data.Y)\n\n            # Use fit object to calculate predicted 'Y' for all X's.\n            Y_hat_all = fit.predict(Xp_all)\n\n            # Append performance metrics to results list.\n            results.append(\n                {\n                    \"degree\":  n,\n                    \"rmse\":    round(np.sqrt(mean_squared_error(df_data.Y, Y_hat_all))),\n                    \"X_hat\":   Xp_all[:,1],\n                    \"Y_hat\":   Y_hat_all\n                }\n            )\n\n        else:\n\n            # Apply X_train to each of the MAX_DEGREE polynomials.\n            Xp_train = poly.fit_transform(X_train)\n\n            # Apply X_val to each of the MAX_DEGREE polynomials.\n            Xp_val = poly.fit_transform(X_val)\n\n            # Fit the training data to y_train.\n            fit = LinearRegression().fit(Xp_train, y_train)\n\n            # Use fit object to calculate predicted Y for training set.\n            Y_hat_train = fit.predict(Xp_train)\n\n            # Use fit object to calculate predicted Y for test set.\n            Y_hat_val = fit.predict(Xp_val)\n\n            # Use fit object to calculate predicted 'Y' for full set.\n            Y_hat_all = fit.predict(Xp_all)\n\n            # Extend with list of performance metrics to results list, 'results'.\n            results.extend(\n                [\n                    {\n                        \"degree\": n,\n                        \"fold\":   \"train\",\n                        \"rmse\":   np.sqrt(mean_squared_error(y_train, Y_hat_train)),\n                        \"X_hat\":  Xp_train[:,1],\n                        \"Y_hat\":  Y_hat_train\n                    },\n\n                    {\n                        \"degree\": n,\n                        \"fold\":   \"test\",\n                        \"rmse\":   np.sqrt(mean_squared_error(y_val, Y_hat_val)),\n                        \"X_hat\":  Xp_val[:,1],\n                        \"Y_hat\":  Y_hat_val\n                    },\n\n                    {\n                        \"degree\": n,\n                        \"fold\":   \"full_set\",\n                        \"X_hat\":  Xp_all[:,1],\n                        \"Y_hat\":  Y_hat_all,\n                        \n                    },\n                ]\n            )\n\n    # Convert list to data frame.\n    df_results = pd.DataFrame.from_records(results)\n\n    return df_results\n\n\n\n2 - Plot fitted data\n\ndef f_plot_data_and_two_models(df_data, df_results, n_i, n_ii, b_split):\n\n    \"\"\"Make a plot of data and of two selected fitted models.\n\n    Parameters\n    ----------\n    df_data : dataframe\n        Dataframe with attribute 'Y' in the first column and X features in the second column and onwards (if applicable).\n    df_results : dataframe\n        Dataframe with results from f_create_and_evaluate_model().\n    n_i : int\n        Degrees of flexibility (polynomial order) you want to plot predict model for (first model).\n    n_ii : int\n        Degrees of flexibility (polynomial order) you want to plot predict model for (second model).\n    b_split : bool\n        Should the models be based on the full data set (b_split = False) or on a train/test split (b_split = True).\n\n    Returns\n    -------\n    plot\n        a plot of the data with two predicted models.\n    \"\"\"\n\n    # In case of the full data set each row in df_result is an iteration.\n    # In case of a train/test split each three rows in df_result constitute an iteration.\n    if b_split:\n\n        index_X_hat    = 2\n        index_Y_hat_i  = (n_i  - 1) * 3 + 2\n        index_Y_hat_ii = (n_ii - 1) * 3 + 2\n    \n    else:\n\n        index_X_hat    = 0\n        index_Y_hat_i  = n_i  - 1\n        index_Y_hat_ii = n_ii - 1\n\n\n    # Define plot results data frame.\n    df_plot_results = pd.DataFrame({\n        \n        \"X_hat\":    df_results.loc[index_X_hat,    \"X_hat\"],  \n        \"Y_hat_i\":  df_results.loc[index_Y_hat_i,  \"Y_hat\"],\n        \"Y_hat_ii\": df_results.loc[index_Y_hat_ii, \"Y_hat\"]\n    })\n\n\n    # POINTS: Plot point markers\n    plot_data = alt.Chart(df_data).mark_point(color = \"grey\").encode(x = \"X1\", y = \"Y\")\n\n    # LINES: Plot first fitted model\n    plot_n_i = alt.Chart(df_plot_results).mark_line(color = \"blue\").encode(x = \"X_hat\", y = \"Y_hat_i\")\n\n    plot_n_ii = alt.Chart(df_plot_results).mark_line(color = \"green\").encode(x = \"X_hat\", y = \"Y_hat_ii\")\n\n    # Simply layer the three plots into one.\n    return plot_data + plot_n_i + plot_n_ii\n\n\n\n3 - Plot RMSE\nCreate a plot of the RMSE against the flexibility of the model (degrees of freedom).\n\n3a - Plot RMSE - Evaluate model based full data set\n\ndef f_plot_rmse_full_dataset(df_results):\n\n    \"\"\"Make a plot of RMSE against the flexibility of the model (degrees of freedom)\n    in case of evaluating the model based on the full data set.\n\n    Parameters\n    ----------\n    df_results : dataframe\n        Dataframe with results from f_create_and_evaluate_model().\n\n    Returns\n    -------\n    plot\n        a plot of the RMSE.\n    \"\"\"\n    \n    return alt.Chart(df_results).mark_line(point = alt.OverlayMarkDef()).encode(x = \"degree\", y = \"rmse\", tooltip = [\"degree\", \"rmse\"])\n\n\n\n3b - Plot RMSE - Evaluate model based on train and test set\n\n# Define function to plot performance metric (RMSE)\ndef f_plot_rmse_train_test_set(df_results):\n\n    \"\"\"Make a plot of RMSE against the flexibility of the model (degrees of freedom)\n    in case of evaluating the model based on the train/test set.\n\n    Parameters\n    ----------\n    df_results : dataframe\n        Dataframe with results from f_create_and_evaluate_model().\n\n    Returns\n    -------\n    plot\n        a plot of the RMSE.\n    \"\"\"\n\n    # Remove full_set data, leaving only the train and test data.\n    df_results = df_results[df_results.fold != \"full_set\"]\n\n    # Define base line chart.\n    base = alt.Chart(df_results).mark_line(\n            \n            point=alt.OverlayMarkDef()\n\n        ).encode(x = \"degree\",y = \"rmse\", color = \"fold\"\n    )\n\n    label = alt.selection_point(\n\n        encodings = ['x'],       # Limit selection to x-axis value\n        on        = 'mouseover', # Select on mouseover events\n        nearest   = True,        # Select data point nearest to the cursor\n        empty     = 'none'       # Empty selection includes no data points\n    )\n\n    return alt.layer(\n\n        # Base line chart.\n        base, \n        \n        ######################################################################################\n        #\n        # I '#'-ed the definition of label.  I think the \n        ######################################################################################\n        \n        # Add a rule mark to serve as a guide line\n        alt.Chart().mark_rule(color = '#aaa').encode(x = 'degree').transform_filter(label),\n        \n        # Add circle marks for selected time points, hide unselected points\n        base.mark_circle().encode(\n            \n            opacity = alt.condition(label, alt.value(1), alt.value(0))\n\n        ).add_params(label),\n\n        # Add white stroked text to provide a legible background for labels\n        base.mark_text(\n            align       = 'left',\n            dx          = 5,\n            dy          = -5,\n            stroke      = 'white',\n            strokeWidth = 2\n        ).encode(text='rmse:Q').transform_filter(label),\n\n        # Add text labels for stock prices.\n        base.mark_text(\n            align = 'left',\n            dx    = 5,\n            dy    = -5\n        ).encode(text='rmse:Q').transform_filter(label),\n        \n        data = df_results\n    )"
  },
  {
    "objectID": "notebooks/overfitting.html#section-1---overfitting",
    "href": "notebooks/overfitting.html#section-1---overfitting",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Section 1 - Overfitting",
    "text": "Section 1 - Overfitting\nWe simulate a true function \\(Y\\) as a third-order polynomial of \\(X1\\). We show that the error of the fitted function decreases as you increase the complexity of the fitted functions, i.e. fitting polynomials with degree &gt; 3.\n\n# Number of data\nn_data = 50\n\n# We set a fixed random seed to reproduce our results below. In which cases would we not want to set a fixed seed?\nnp.random.seed(123)\n\n# Define X, error, and model including error.\nx = np.sort(np.random.normal(loc = 10, scale = 5, size = n_data))\ne = np.random.normal(loc = 0, scale = 2000, size = n_data)\ny = (2530 + 20*x - 10*(x**2) + 5*(x**3) + e)\n\n# Combine X1 and Y in a data frame.\ndf_data1 = pd.DataFrame({\"Y\": y, \"X1\": x})\n\n# Create and evaluate models of various flexibility.\ndf_results1 = f_create_and_evaluate_model(df_data = df_data1, b_split = False)\n\n\n# Plot data and two fitted models. Select fitted models by updating 'n_i' and 'n_ii'.\nf_plot_data_and_two_models(df_data = df_data1, df_results = df_results1, n_i = 1, n_ii = 4, b_split = False)\n\n\n\n\n\n\n\n\n# Plot RMSE against model flexibility.\nf_plot_rmse_full_dataset(df_results = df_results1)\n\n\n\n\n\n\n\nWe observe that the RMSE increaes again at degree &gt; 16. This is due to the function becoming overly flexible that it starts missing (overshooting) data points.\nQuestion - Why does the error not drop below 2000 up to 20 degrees of freedom?"
  },
  {
    "objectID": "notebooks/overfitting.html#section-2---overfitting-with-random-variables",
    "href": "notebooks/overfitting.html#section-2---overfitting-with-random-variables",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Section 2 - Overfitting with random variables",
    "text": "Section 2 - Overfitting with random variables\nWe now add features \\(X2, X3\\) but keep the true function \\(Y\\) unchanged, i.e. only dependent on \\(X1\\). We show that these random variables may lead to even more severe overfitting.\n\n# Make a copy of df_data1.\ndf_data2 = df_data1.copy()\n\n# Add two more features, X2 and X3. Response variable Y is independent of X2 and X3.\nfor i in [2, 3]:\n\n    df_data2[f\"X{str(i)}\"] = np.random.normal(loc = 10, scale = 5, size = n_data)\n\n# Create and evaluate models of various flexibility.\ndf_results2 = f_create_and_evaluate_model(df_data = df_data2, b_split = False)\n\n\n# Plot data and two fitted models. Select fitted models by updating 'n_i' and 'n_ii'.\nf_plot_data_and_two_models(df_data = df_data2, df_results = df_results2, n_i = 1, n_ii = 2, b_split = False)\n\n\n\n\n\n\n\n\n# Plot RMSE against model flexibility.\nf_plot_rmse_full_dataset(df_results2)\n\n\n\n\n\n\n\nWith additional features we need fewer degrees of freedom in the polynomial. The additional features result in more flexibility in the model."
  },
  {
    "objectID": "notebooks/overfitting.html#section-3---testing-to-prevent-overfitting",
    "href": "notebooks/overfitting.html#section-3---testing-to-prevent-overfitting",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Section 3 - Testing to prevent overfitting",
    "text": "Section 3 - Testing to prevent overfitting\nWe follow example from sklearn library, with cosine as true function.\n\n# Define function with the true model.\ndef true_fun(x, e):\n    return np.cos(1.5 * np.pi * x) + e\n\n# We set a fixed random seed to reproduce our results below. In which cases would we not want to set a fixed seed?\nnp.random.seed(12)\n\n# Number of data.\nn_data = 50\n\n# Define X, error, and model including error.\nx = np.sort(np.random.rand(n_data))\ne = np.random.randn(n_data) * 0.1\ny = true_fun(x, e)\n\n# Put data in data frame.\ndf_data3 = pd.DataFrame({\"Y\": y, \"X1\": x})\n\n# Create and evaluate models of various flexibility.\ndf_results3 = f_create_and_evaluate_model(df_data = df_data3, b_split = True)\n\n\n# Plot data and two fitted models. Select fitted models by updating 'n_i' and 'n_ii'.\n# The plot shows all data (train + test), while the models are fitted on the training data.\n# Try out different n_ii: start at 16 and increase with steps of 1. What do we see?\nf_plot_data_and_two_models(df_data = df_data3, df_results = df_results3, n_i = 1, n_ii = 5, b_split = True)\n\n\n\n\n\n\n\n\n# Plot RMSE against model flexibility.\nf_plot_rmse_train_test_set(df_results3)\n\n/Users/macstudio/opt/anaconda3/envs/py310/lib/python3.10/site-packages/altair/vegalite/v5/api.py:355: AltairDeprecationWarning: The value of 'empty' should be True or False.\n  warnings.warn("
  },
  {
    "objectID": "notebooks/overfitting.html#section-4---traintest-split-can-still-lead-to-overfitting",
    "href": "notebooks/overfitting.html#section-4---traintest-split-can-still-lead-to-overfitting",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Section 4 - Train/test split can still lead to overfitting",
    "text": "Section 4 - Train/test split can still lead to overfitting\nTrain/test split isn’t a 100% safe-guard against overfitting. Given a quadratic (second order) true function, testing still results in a fourth order fitted function.\n\n# Set random seed, to reproduce results.\nnp.random.seed(456)\n\n# Number of data.\nn_data = 150\n\n# Define X, error, and model including error.\nx = np.random.normal(-10, 6, n_data)\ne = np.random.normal(0, 75, n_data)\ny = 25 + 2*x - 4*(x**2) + e\n\n# Bring X and y together in a data frame.\ndf_data4 = pd.DataFrame({\"Y\": y, \"X1\": x})\n\n# Create and evaluate models of various flexibility.\ndf_results4 = f_create_and_evaluate_model(df_data = df_data4, b_split = True)\n\n\n# Plot data and two fitted models. Select fitted models by updating 'n_i' and 'n_ii'.\n# The plot shows all data (train + test), while the models are fitted on the training data.\nf_plot_data_and_two_models(df_data = df_data4, df_results = df_results4, n_i = 1, n_ii = 2, b_split = True)\n\n\n\n\n\n\n\n\n# Plot RMSE against model flexibility.\nf_plot_rmse_train_test_set(df_results4)\n\n/Users/macstudio/opt/anaconda3/envs/py310/lib/python3.10/site-packages/altair/vegalite/v5/api.py:355: AltairDeprecationWarning: The value of 'empty' should be True or False.\n  warnings.warn("
  },
  {
    "objectID": "notebooks/overfitting.html#cross-validation---work-in-progress",
    "href": "notebooks/overfitting.html#cross-validation---work-in-progress",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Cross-validation - Work in Progress",
    "text": "Cross-validation - Work in Progress\nBy using k-fold cross-validation, we have a better safeguard against overfitting. We reproduce the example from Will Koehrsen.\nNB: WORK IN PROGRESS. Example is not the best in terms of train-validation curves.\n\n# def fit_poly(train, y_train, test, y_test, degrees, plot='train', return_scores=False):\n    \n#     # Create a polynomial transformation of features\n#     features = PolynomialFeatures(degree=degrees, include_bias=False)\n    \n#     # Reshape training features for use in scikit-learn and transform features\n#     train = train.reshape((-1, 1))\n#     train_trans = features.fit_transform(train)\n    \n#     # Create the linear regression model and train\n#     model = LinearRegression()\n#     model.fit(train_trans, y_train)\n    \n#     # Calculate the cross validation score\n#     cross_valid = cross_val_score(model, train_trans, y_train, scoring='neg_mean_squared_error', cv = 5)\n    \n#     # Training predictions and error\n#     train_predictions = model.predict(train_trans)\n#     training_error    = mean_squared_error(y_train, train_predictions)\n    \n#     # Format test features\n#     test = test.reshape((-1, 1))\n#     test_trans = features.fit_transform(test)\n    \n#     # Test set predictions and error\n#     test_predictions = model.predict(test_trans)\n#     testing_error = mean_squared_error(y_test, test_predictions)\n    \n#     # Find the model curve and the true curve\n#     x_curve = np.linspace(0, 1, 100)\n#     x_curve = x_curve.reshape((-1, 1))\n#     x_curve_trans = features.fit_transform(x_curve)\n    \n#     # Model curve\n#     model_curve = model.predict(x_curve_trans)\n    \n#     # True curve\n#     y_true_curve = true_fun(x_curve[:, 0])\n\n#      # Return the metrics\n#     if return_scores:\n#         return training_error, testing_error, -np.mean(cross_valid)\n\n\n# x = np.sort(np.random.rand(120))\n# y = true_fun(x) + 0.1 * np.random.randn(len(x))\n\n# # Random indices for creating training and testing sets\n# random_ind = np.random.choice(list(range(120)), size = 120, replace=False)\n# xt = x[random_ind]\n# yt = y[random_ind]\n\n# # Training and testing observations\n# train = xt[:int(0.7 * len(x))]\n# test = xt[int(0.7 * len(x)):]\n\n# y_train = yt[:int(0.7 * len(y))]\n# y_test = yt[int(0.7 * len(y)):]\n\n# # Model the true curve\n# x_linspace = np.linspace(0, 1, 1000)\n# y_true = true_fun(x_linspace)\n\n# # Range of model degrees to evaluate\n# degrees = [int(x) for x in np.linspace(1, 40, 40)]\n\n# # Results dataframe\n# results5 = pd.DataFrame(0, columns = ['train_error', 'test_error', 'cross_valid'], index = degrees)\n\n# # Try each value of degrees for the model and record results\n# for degree in degrees:\n#     degree_results = fit_poly(train, y_train, test, y_test, degree, plot=False, return_scores=True)\n#     results5.loc[degree, 'train_error'] = degree_results[0]\n#     results5.loc[degree, 'test_error'] = degree_results[1]\n#     results5.loc[degree, 'cross_valid'] = degree_results[2]\n\n# # print('10 Lowest Cross Validation Errors\\n')\n# # train_eval = results5.sort_values('cross_valid').reset_index(level=0).rename(columns={'index': 'degrees'})\n# # train_eval.loc[:,['degrees', 'cross_valid']].head(10)\n\n\n# import matplotlib.pyplot as plt\n\n# plt.plot(results5.index, results5['train_error'], 'b-o', ms=6, label = 'Training Error')\n# plt.plot(results5.index, results5['test_error'], 'r-*', ms=6, label = 'Testing Error')\n# plt.legend(loc=2); plt.xlabel('Degrees'); plt.ylabel('Mean Squared Error'); plt.title('Training and Testing Curves');\n# plt.ylim(0, 0.05); plt.show()\n\n# print('\\nMinimum Training Error occurs at {} degrees.'.format(int(np.argmin(results5['train_error']))))\n# print('Minimum Testing Error occurs at {} degrees.\\n'.format(int(np.argmin(results5['test_error']))))"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#objectives",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#objectives",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Objectives",
    "text": "Objectives\n\nExample end-to-end supervised learning workflow with Ames Housing dataset\nFocus on conceptual understanding of machine learning\nDemonstrate use of Predictive Power Score (PPS)\nDemonstrate capabilities of low-code tools"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#attribution",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#attribution",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Attribution",
    "text": "Attribution\n\nDataset\n\nAmes Housing dataset paper (original paper)\nKaggle competition advanced regression techniques (link)\n\n\n\nPython libraries\n\nAltair (docs)\nydata-profiling (docs)\nPredictive Power Score (PPS, GitHub, blog)\nPyCaret: open-source, low-code machine learning library in Python that automates machine learning workflows (link)\n\n\nimport altair as alt\nimport pandas as pd\nimport ppscore as pps\nfrom pycaret.regression import *\nfrom ydata_profiling import ProfileReport\n\n\n# customize Altair\ndef y_axis():\n    return {\n        \"config\": {\n            \"axisX\": {\"grid\": False},\n            \"axisY\": {\n                \"domain\": False,\n                \"gridDash\": [2, 4],\n                \"tickSize\": 0,\n                \"titleAlign\": \"right\",\n                \"titleAngle\": 0,\n                \"titleX\": -5,\n                \"titleY\": -10,\n            },\n            \"view\": {\n                \"stroke\": \"transparent\",\n                # To keep the same height and width as the default theme:\n                \"continuousHeight\": 300,\n                \"continuousWidth\": 400,\n            },\n        }\n    }\n\n\nalt.themes.register(\"y_axis\", y_axis)\nalt.themes.enable(\"y_axis\")\n\n\ndef get_descriptions():\n    \"Parse descriptions of columns of Ames Housing dataset\"\n    with open(\"data_description.txt\") as reader:\n        descriptions = {}\n        for line in reader.readlines():\n            if \":\" in line and \"2nd level\" not in line:\n                descriptions[line.split(\": \")[0].strip()] = line.split(\": \")[1].strip()\n    return pd.Series(descriptions).rename(\"descriptions\")\n\n\ndescriptions = get_descriptions()"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#read-and-explore-the-data",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#read-and-explore-the-data",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Read and explore the data",
    "text": "Read and explore the data\n\n%%time\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\nprofile = ProfileReport(train, minimal=True, title=\"Ames Housing Profiling Report\")\nprofile.to_file(\"ames-housing-profiling-report-minimal.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 45 s, sys: 1.78 s, total: 46.7 s\nWall time: 16.4 s\n\n\n\nprofile.to_notebook_iframe()"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Investigate features with largest predictive power",
    "text": "Investigate features with largest predictive power\nWe use the Predictive Power Score to evaluate which features have the highest predictive power with respect to SalePrice.\n\npredictors = (\n    pps.predictors(train, \"SalePrice\")\n    .round(3)\n    .iloc[:, :-1]\n    .merge(descriptions, how=\"left\", left_on=\"x\", right_index=True)\n)\nbase = (\n    alt.Chart(predictors)\n    .encode(\n        x=alt.Y(\"x:N\").sort(\"-y\"),\n        y=\"ppscore\",\n        tooltip=[\"x\", \"ppscore\", \"descriptions\"],\n    )\n    .transform_filter(\"datum.ppscore &gt; 0\")\n)\nbase.mark_bar() + base.mark_text(align=\"center\", dy=-5)"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-colinearity",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-colinearity",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Investigate colinearity",
    "text": "Investigate colinearity\n\npps_matrix = (\n    pps.matrix(\n        train.loc[:, predictors.query(\"ppscore &gt; 0\")[\"x\"].tolist()],\n    )\n    .loc[:, [\"x\", \"y\", \"ppscore\"]]\n    .round(3)\n)\n(\n    alt.Chart(pps_matrix)\n    .mark_rect()\n    .encode(\n        x=\"x:O\",\n        y=\"y:O\",\n        color=\"ppscore:Q\",\n        tooltip=[\"x\", \"y\", \"ppscore\"])\n)"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#build-models",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#build-models",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Build models",
    "text": "Build models\nWe select the 30 features that have the highest predictive power score\n\nselected_predictors = (\n    predictors.sort_values(\"ppscore\", ascending=False).head(30)[\"x\"].to_list()\n)\nreg = setup(data = train.loc[:, selected_predictors + [\"SalePrice\"]], \n             target = 'SalePrice',\n             numeric_imputation = 'mean',\n             categorical_features =  list(train.loc[:, selected_predictors].select_dtypes(\"object\").columns), \n             feature_selection = False,\n             pca=False,\n             remove_multicollinearity=True,\n             remove_outliers = False,\n             normalize = True,\n             )\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n8378\n\n\n1\nTarget\nSalePrice\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(1460, 31)\n\n\n4\nTransformed data shape\n(1460, 116)\n\n\n5\nTransformed train set shape\n(1021, 116)\n\n\n6\nTransformed test set shape\n(439, 116)\n\n\n7\nOrdinal features\n1\n\n\n8\nNumeric features\n16\n\n\n9\nCategorical features\n14\n\n\n10\nRows with missing values\n94.7%\n\n\n11\nPreprocess\nTrue\n\n\n12\nImputation type\nsimple\n\n\n13\nNumeric imputation\nmean\n\n\n14\nCategorical imputation\nmode\n\n\n15\nMaximum one-hot encoding\n25\n\n\n16\nEncoding method\nNone\n\n\n17\nRemove multicollinearity\nTrue\n\n\n18\nMulticollinearity threshold\n0.900000\n\n\n19\nNormalize\nTrue\n\n\n20\nNormalize method\nzscore\n\n\n21\nFold Generator\nKFold\n\n\n22\nFold Number\n10\n\n\n23\nCPU Jobs\n-1\n\n\n24\nUse GPU\nFalse\n\n\n25\nLog Experiment\nFalse\n\n\n26\nExperiment Name\nreg-default-name\n\n\n27\nUSI\n81f6\n\n\n\n\n\n\n%%time\nselected_models = [model for model in models().index if model not in [\"lar\", \"lr\", \"ransac\"]]\nbest_model = compare_models(sort='RMSLE', include=selected_models)\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\nlightgbm\nLight Gradient Boosting Machine\n18267.8967\n969345616.0929\n30245.0381\n0.8400\n0.1474\n0.1051\n0.3780\n\n\ngbr\nGradient Boosting Regressor\n18349.4461\n1064907228.1139\n31464.4286\n0.8221\n0.1497\n0.1059\n0.0810\n\n\nrf\nRandom Forest Regressor\n18834.6022\n1052157810.7295\n31669.8884\n0.8263\n0.1530\n0.1091\n0.1370\n\n\npar\nPassive Aggressive Regressor\n18695.3332\n1145943934.1128\n32527.7429\n0.8093\n0.1535\n0.1061\n0.0560\n\n\nen\nElastic Net\n19941.1185\n1212771199.2709\n33679.9238\n0.8018\n0.1536\n0.1131\n0.0370\n\n\net\nExtra Trees Regressor\n19749.8604\n1158574471.9795\n33510.6172\n0.8073\n0.1591\n0.1138\n0.1370\n\n\nhuber\nHuber Regressor\n18580.7407\n1172797296.1965\n32571.8573\n0.8024\n0.1602\n0.1069\n0.0420\n\n\nbr\nBayesian Ridge\n20557.3468\n1251454965.3245\n34036.3809\n0.7934\n0.1715\n0.1191\n0.0380\n\n\nard\nAutomatic Relevance Determination\n20446.5401\n1229331466.4696\n33711.3986\n0.7969\n0.1747\n0.1193\n0.2740\n\n\nomp\nOrthogonal Matching Pursuit\n21882.7966\n1294135379.9217\n34955.8947\n0.7847\n0.1849\n0.1296\n0.0340\n\n\nada\nAdaBoost Regressor\n24866.3282\n1379609584.9159\n36498.7175\n0.7707\n0.2036\n0.1621\n0.0580\n\n\nknn\nK Neighbors Regressor\n26571.2016\n1730405638.7521\n40931.3774\n0.7200\n0.2050\n0.1518\n0.0360\n\n\ndt\nDecision Tree Regressor\n27747.5148\n2157234242.4490\n45330.0191\n0.6512\n0.2169\n0.1564\n0.0350\n\n\nllar\nLasso Least Angle Regression\n21458.2025\n1320695830.3446\n35006.4301\n0.7809\n0.2187\n0.1268\n0.0380\n\n\nlasso\nLasso Regression\n21455.6793\n1320742178.3808\n35006.1951\n0.7809\n0.2189\n0.1268\n0.2100\n\n\nridge\nRidge Regression\n21439.2241\n1318937040.3720\n34981.0548\n0.7812\n0.2196\n0.1266\n0.0360\n\n\nsvm\nSupport Vector Regression\n55543.3805\n6417749387.4994\n79739.3850\n-0.0524\n0.3979\n0.3195\n0.0450\n\n\ndummy\nDummy Regressor\n57352.4774\n6133919031.8184\n78021.7431\n-0.0086\n0.4061\n0.3635\n0.0340\n\n\ntr\nTheilSen Regressor\n29178.3219\n2564758742.0908\n49572.3895\n0.5667\n0.4258\n0.1978\n4.0290\n\n\nkr\nKernel Ridge\n182040.0692\n34133507087.4154\n184731.2672\n-4.7500\n1.7994\n1.1623\n0.0380\n\n\nmlp\nMLP Regressor\n166456.5847\n32851392125.7179\n181040.0031\n-4.4796\n2.7703\n0.9182\n0.2890\n\n\n\n\n\n\n\n\nCPU times: user 4.09 s, sys: 446 ms, total: 4.53 s\nWall time: 1min 3s"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#evaluation",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#evaluation",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Evaluation",
    "text": "Evaluation\n\nWith a standard, AutoML-like workflow, we achive RMSLE of 0.13 - 0.14 (over different runs), which is already in the top 25% of the 4,200 submissions on the leaderboard\nWe can now make predictions on the test set\n\n\npredictions = (\n    predict_model(best_model, data=test)\n    .rename(columns={\"prediction_label\": \"SalePrice\"})\n    .loc[:, [\"Id\", \"SalePrice\"]]\n)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nId\nSalePrice\n\n\n\n\n0\n1461\n126951.931078\n\n\n1\n1462\n142402.002648\n\n\n2\n1463\n185086.014955\n\n\n3\n1464\n191718.590497\n\n\n4\n1465\n186412.972060\n\n\n\n\n\n\n\n\nPipeline\n\nplot_model(best_model, 'pipeline')\n\n\n\n\n\nplot_model(best_model, 'feature')\n\n\n\n\n\n\n\n\nplot_model(best_model, 'residuals')"
  },
  {
    "objectID": "posts/predictive-power-score.html",
    "href": "posts/predictive-power-score.html",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "",
    "text": "It is Friday afternoon and your boss tells you that the data delivery surprisingly arrived early — after only 4 weeks of back and forth. This was the missing piece for your predictive model. You’re excited but also a little bit anxious because you know what’s next: exploring the data. All. 45. Columns. This will take many hours but you know it’s worth it because without data understanding you are walking blind. One obvious step is to have a look at all the univariate column distributions. But this won’t be enough.\n\nYou ask yourself: what relationships exist between columns?\n\nTo answer this question, you just repeat the typical drill: calculate a correlation matrix and check for some surprising relationships. Whenever you are surprised, you take a moment to plot a scatterplot of the two columns at hand and see if you can make any sense of it. Hopefully you can but too often you cannot because you don’t even know what the columns mean in the first place. But this is a story for another day.\n\nAfter inspecting the correlation matrix you move on and you don’t even know what you don’t know (scary).\n\nLet’s take a moment to review the correlation. The score ranges from -1 to 1 and indicates if there is a strong linear relationship — either in a positive or negative direction. So far so good. However, there are many non-linear relationships that the score simply won’t detect. For example, a sinus wave, a quadratic curve or a mysterious step function. The score will just be 0, saying: “Nothing interesting here”. Also, correlation is only defined for numeric columns. So, let’s drop all the categoric columns. In my last project more than 60% of the columns were categoric, but hey. Never mind. And no, I won’t convert the columns because they are not ordinal and OneHotEncoding will create a matrix that has more values than there are atoms in the universe.\n\nIf you are a little bit too well educated you know that the correlation matrix is symmetric. So you basically can throw away one half of it. Great, we saved ourselves some work there! Or did we? Symmetry means that the correlation is the same whether you calculate the correlation of A and B or the correlation of B and A. However, relationships in the real world are rarely symmetric. More often, relationships are asymmetric. Here is an example: The last time I checked, my zip code of 60327 tells strangers quite reliably that I am living in Frankfurt, Germany. But when I only tell them my city, somehow they are never able to deduce the correct zip code. Pff … amateurs. Another example is this: a column with 3 unique values will never be able to perfectly predict another column with 100 unique values. But the opposite might be true. Clearly, asymmetry is important because it is so common in the real world.\n\nThinking about those shortcomings of correlation, I started to wonder: can we do better?\n\nThe requirements: One day last year, I was dreaming about a score that would tell me if there is any relationship between two columns — no matter if the relationship is linear, non-linear, gaussian or only known by aliens. Of course, the score should be asymmetric because I want to detect all the weird relationships between cities and zip codes. The score should be 0 if there is no relationship and the score should be 1 if there is a perfect relationship. And as the icing on the cake, the score should be able to handle categoric and numeric columns out of the box. Summing it up for all my academic friends: an asymmetric, data-type-agnostic score for predictive relationships between two columns that ranges from 0 to 1."
  },
  {
    "objectID": "posts/predictive-power-score.html#too-many-problems-with-the-correlation",
    "href": "posts/predictive-power-score.html#too-many-problems-with-the-correlation",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "",
    "text": "It is Friday afternoon and your boss tells you that the data delivery surprisingly arrived early — after only 4 weeks of back and forth. This was the missing piece for your predictive model. You’re excited but also a little bit anxious because you know what’s next: exploring the data. All. 45. Columns. This will take many hours but you know it’s worth it because without data understanding you are walking blind. One obvious step is to have a look at all the univariate column distributions. But this won’t be enough.\n\nYou ask yourself: what relationships exist between columns?\n\nTo answer this question, you just repeat the typical drill: calculate a correlation matrix and check for some surprising relationships. Whenever you are surprised, you take a moment to plot a scatterplot of the two columns at hand and see if you can make any sense of it. Hopefully you can but too often you cannot because you don’t even know what the columns mean in the first place. But this is a story for another day.\n\nAfter inspecting the correlation matrix you move on and you don’t even know what you don’t know (scary).\n\nLet’s take a moment to review the correlation. The score ranges from -1 to 1 and indicates if there is a strong linear relationship — either in a positive or negative direction. So far so good. However, there are many non-linear relationships that the score simply won’t detect. For example, a sinus wave, a quadratic curve or a mysterious step function. The score will just be 0, saying: “Nothing interesting here”. Also, correlation is only defined for numeric columns. So, let’s drop all the categoric columns. In my last project more than 60% of the columns were categoric, but hey. Never mind. And no, I won’t convert the columns because they are not ordinal and OneHotEncoding will create a matrix that has more values than there are atoms in the universe.\n\nIf you are a little bit too well educated you know that the correlation matrix is symmetric. So you basically can throw away one half of it. Great, we saved ourselves some work there! Or did we? Symmetry means that the correlation is the same whether you calculate the correlation of A and B or the correlation of B and A. However, relationships in the real world are rarely symmetric. More often, relationships are asymmetric. Here is an example: The last time I checked, my zip code of 60327 tells strangers quite reliably that I am living in Frankfurt, Germany. But when I only tell them my city, somehow they are never able to deduce the correct zip code. Pff … amateurs. Another example is this: a column with 3 unique values will never be able to perfectly predict another column with 100 unique values. But the opposite might be true. Clearly, asymmetry is important because it is so common in the real world.\n\nThinking about those shortcomings of correlation, I started to wonder: can we do better?\n\nThe requirements: One day last year, I was dreaming about a score that would tell me if there is any relationship between two columns — no matter if the relationship is linear, non-linear, gaussian or only known by aliens. Of course, the score should be asymmetric because I want to detect all the weird relationships between cities and zip codes. The score should be 0 if there is no relationship and the score should be 1 if there is a perfect relationship. And as the icing on the cake, the score should be able to handle categoric and numeric columns out of the box. Summing it up for all my academic friends: an asymmetric, data-type-agnostic score for predictive relationships between two columns that ranges from 0 to 1."
  },
  {
    "objectID": "posts/predictive-power-score.html#calculating-the-predictive-power-score-pps",
    "href": "posts/predictive-power-score.html#calculating-the-predictive-power-score-pps",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Calculating the Predictive Power Score (PPS)",
    "text": "Calculating the Predictive Power Score (PPS)\n\nFirst of all, there is not the one and only way to calculate the predictive power score. In fact, there are many possible ways to calculate a score that satisfies the requirements mentioned before. So, let’s rather think of the predictive power score as a framework for a family of scores.\n\nLet’s say we have two columns and want to calculate the predictive power score of A predicting B. In this case, we treat B as our target variable and A as our (only) feature. We can now calculate a cross-validated Decision Tree and calculate a suitable evaluation metric. When the target is numeric we can use a Decision Tree Regressor and calculate the Mean Absolute Error (MAE). When the target is categoric, we can use a Decision Tree Classifier and calculate the weighted F1. You might also use other scores like the ROC etc but let’s put those doubts aside for a second because we have another problem:\n\nMost evaluation metrics are meaningless if you don’t compare them to a baseline\n\nI guess you all know the situation: you tell your grandma that your new model has a F1 score of 0.9 and somehow she is not as excited as you are. In fact, this is very smart of her because she does not know if anyone can score 0.9 or if you are the first human being who ever scored higher than 0.5 after millions of awesome KAGGLErs tried. So, we need to “normalize” our evaluation score. And how do you normalize a score? You define a lower and an upper limit and put the score into perspective. So what should the lower and upper limit be? Let’s start with the upper limit because this is usually easier: a perfect F1 is 1. A perfect MAE is 0. Boom! Done. But what about the lower limit? Actually, we cannot answer this in absolute terms.\n\nThe lower limit depends on the evaluation metric and your data set. It is the value that a naive predictor achieves.\n\nIf you achieve a F1 score of 0.9 this might be super bad or really good. If your super fancy cancer detection model always predicts “benign” and it still scores 0.9 on that highly skewed dataset then 0.9 is obviously not so good. So, we need to calculate a score for a very naive model. But what is a naive model? For a classification problem, always predicting the most common class is pretty naive. For a regression problem, always predicting the median value is pretty naive.\n\nLet’s have a look at a detailed, fictional example:\nGetting back to the example of the zip codes and the city name. Imagine both columns are categoric. First, we want to calculate the PPS of zip code to city. We use the weighted F1 score because city is categoric. Our cross-validated Decision Tree Classifier achieves a score of 0.95 F1. We calculate a baseline score via always predicting the most common city and achieve a score of 0.1 F1. If you normalize the score, you will get a final PPS of 0.94 after applying the following normalization formula: (0.95–0.1) / (1–0.1). As we can see, a PPS score of 0.94 is rather high, so the zip code seems to have a good predictive power towards the city. However, if we calculate the PPS in the opposite direction, we might achieve a PPS of close to 0 because the Decision Tree Classifier is not substantially better than just always predicting the most common zip code.\n\nPlease note: the normalization formula for the MAE is different from the F1. For MAE lower is better and the best value is 0."
  },
  {
    "objectID": "posts/predictive-power-score.html#comparing-the-pps-to-correlation",
    "href": "posts/predictive-power-score.html#comparing-the-pps-to-correlation",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Comparing the PPS to correlation",
    "text": "Comparing the PPS to correlation\nIn order to get a better feeling for the PPS and its differences to the correlation, let’s have a look at the following two examples:\n\npps2Example 1: Non-linear effects and asymmetry\n\nLet’s use a typical quadratic relationship: the feature x is a uniform variable ranging from -2 to 2 and the target y is the square of x plus some error. In this case, x can predict y very well because there is a clear non-linear, quadratic relationship — after all that’s how we generated the data. However, this is not true in the other direction from y to x. For example, if y is 4, it is impossible to predict whether x was roughly 2 or -2. Thus, the predictive relationship is asymmetric and the scores should reflect this.\nWhat are the values of the scores in this example? If you don’t already know what you are looking for, the correlation will leave you hanging because the correlation is 0. Both from x to y and from y to x because the correlation is symmetric. However, the PPS from x to y is 0.67, detecting the non-linear relationship and saving the day. Nevertheless, the PPS is not 1 because there exists some error in the relationship. In the other direction, the PPS from y to x is 0 because your prediction cannot be better than the naive baseline and thus the score is 0.\n\n\nExample 2: Categorical columns and hidden patterns\nLet’s compare the correlation matrix to the PPS matrix on the Titanic dataset. “The Titanic dataset? Again??” I know, you probably think you already have seen everything about the Titanic dataset but maybe the PPS will give you some new insights.\n\n\n\nTwo findings about the correlation matrix:\n\nThe correlation matrix is smaller and leaves out many interesting relationships. Of course, that makes sense because columns like Sex, TicketID or Port are categoric and the correlation cannot be computed for them.\nThe correlation matrix shows a negative correlation between TicketPrice and Class of medium strength (-0.55). We can double-check this relationship if we have a look at the PPS. We will see that the TicketPrice is a strong predictor for the Class (0.9 PPS) but not vice versa. The Class only predicts the TicketPrice with a PPS of 0.2. This makes sense because whether your ticket did cost 5.000$ or 10.000$ you were most likely in the highest class. In contrast, if you know that someone was in the highest class you cannot say whether they paid 5.000$ or 10.000$ for their ticket. In this scenario, the asymmetry of the PPS shines again.\n\n\n\nFour findings about the PPS matrix:\n\nThe first row of the matrix tells you that the best univariate predictor of the column Survived is the column Sex. This makes sense because women were prioritized during the rescue. (We could not find this information in the correlation matrix because the column Sex was dropped.)\nIf you have a look at the column for TicketID, you can see that TicketID is a fairly good predictor for a range of columns. If you further dig into this pattern, you will find out that multiple persons had the same TicketID. Thus, the TicketID is actually referencing a latent group of passengers who bought the ticket together, for example the big Italian Rossi family that turns any evening into a spectacle. Thus, the PPS helped me to detect a hidden pattern.\nWhat’s even more surprising than the strong predictive power of TicketID is the strong predictive power of TicketPrice across a wide range of columns. Especially, the fact that the TicketPrice is fairly good at predicting the TicketID (0.67) and vice versa (0.64). Upon further research you will find out that tickets often had unique prices. For example, only the Italian Rossi family paid a price of 72,50$. This is a critical insight! It means that the TicketPrice contains information about the TicketID and thus about our Italian family. An information that you need to have when considering potential information leakage.\nLooking at the PPS matrix, we can see effects that might be explained by causal chains. (Did he just say causal? — Of course, those causal hypotheses have to be treated carefully but this is beyond the scope of this article.) For example, you might be surprised why the TicketPrice has predictive power on the survival rate (PPS 0.39). But if you know that the Class influences your survival rate (PPS 0.36) and that the TicketPrice is a good predictor for your Class (PPS 0.9), then you might have found an explanation."
  },
  {
    "objectID": "posts/predictive-power-score.html#applications-of-the-pps-and-the-pps-matrix",
    "href": "posts/predictive-power-score.html#applications-of-the-pps-and-the-pps-matrix",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Applications of the PPS and the PPS matrix",
    "text": "Applications of the PPS and the PPS matrix\nAfter we learned about the advantages of the PPS, let’s see where we can use the PPS in the real life.\n\nDisclaimer: There are use cases for both the PPS and the correlation. The PPS clearly has some advantages over correlation for finding predictive patterns in the data. However, once the patterns are found, the correlation is still a great way of communicating found linear relationships.\n\n\nFind patterns in the data: The PPS finds every relationship that the correlation finds — and more. Thus, you can use the PPS matrix as an alternative to the correlation matrix to detect and understand linear or nonlinear patterns in your data. This is possible across data types using a single score that always ranges from 0 to 1.\nFeature selection: In addition to your usual feature selection mechanism, you can use the predictive power score to find good predictors for your target column. Also, you can eliminate features that just add random noise. Those features sometimes still score high in feature importance metrics. In addition, you can eliminate features that can be predicted by other features because they don’t add new information. Besides, you can identify pairs of mutually predictive features in the PPS matrix — this includes strongly correlated features but will also detect non-linear relationships.\nDetect information leakage: Use the PPS matrix to detect information leakage between variables — even if the information leakage is mediated via other variables.\nData Normalization: Find entity structures in the data via interpreting the PPS matrix as a directed graph. This might be surprising when the data contains latent structures that were previously unknown. For example: the TicketID in the Titanic dataset is often an indicator for a family."
  },
  {
    "objectID": "posts/predictive-power-score.html#how-to-use-the-pps-in-your-own-python-project",
    "href": "posts/predictive-power-score.html#how-to-use-the-pps-in-your-own-python-project",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "How to use the PPS in your own (Python) project",
    "text": "How to use the PPS in your own (Python) project\nIf you are still following along you are one of the rare human beings who still have an attention span — you crazy beast! If you can’t wait to see what the PPS will reveal on your own data, we have some good news for you: we open-sourced an implementation of the PPS as a Python library named ppscore.\n\nBefore using the Python library, please take a moment to read through the calculation details\n\nInstalling the package is as simple as\n\npip install ppscore\n\nCalculating the PPS for a given pandas dataframe:\n\nimport ppscore as pps\npps.score(df, \"feature_column\", \"target_column\")\n\nYou can also calculate the whole PPS matrix:\n\npps.matrix(df)\n\n\nHow fast is the PPS in comparison to the correlation?\nAlthough the PPS has many advantages over the correlation, there is some drawback: it takes longer to calculate. But how bad is it? Does it take multiple weeks or are we done in a couple of minutes or even seconds? When calculating a single PPS using the Python library, the time should be no problem because it usually takes around 10–500ms. The calculation time mostly depends on the data types, the number of rows and the used implementation. However, when calculating the whole PPS matrix for 40 columns this results in 40*40=1600 individual calculations which might take 1–10 minutes. So you might want to start the calculation of the PPS matrix in the background and go on that summer vacation you always dreamed of! 🏖 ️For our projects and datasets the computational performance was always good enough but of course there is room for improvement. Fortunately, we see many ways how the calculation of the PPS can be improved to achieve speed gains of a factor of 10–100. For example, using intelligent sampling, heuristics or different implementations of the PPS. If you like the PPS and are in need of a faster calculation, please reach out to us."
  },
  {
    "objectID": "posts/predictive-power-score.html#limitations",
    "href": "posts/predictive-power-score.html#limitations",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Limitations",
    "text": "Limitations\nWe made it — you are excited and want to show the PPS to your colleagues. However, you know they are always so critical about new methods. That’s why you better be prepared to know the limitations of the PPS:\n\nThe calculation is slower than the correlation (matrix).\nThe score cannot be interpreted as easily as the correlation because it does not tell you anything about the type of relationship that was found. Thus, the PPS is better for finding patterns but the correlation is better to communicate found linear relationships.\nYou cannot compare the scores for different target variables in a strict mathematical way because they are calculated using different evaluation metrics. The scores are still valuable in the real world, but you need to keep this in mind.\nThere are limitations of the components used underneath the hood. Please remember: you might exchange the components e.g. using a GLM instead of a Decision Tree or using ROC instead of F1 for binary classifications.\nIf you use the PPS for feature selection you still want to perform forward and backward selection in addition. Also, the PPS cannot detect interaction effects between features towards your target."
  },
  {
    "objectID": "posts/predictive-power-score.html#conclusion",
    "href": "posts/predictive-power-score.html#conclusion",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Conclusion",
    "text": "Conclusion\nAfter years of using the correlation we were so bold (or crazy?) to suggest an alternative that can detect linear and non-linear relationships. The PPS can be applied to numeric and categoric columns and it is asymmetric. We proposed an implementation and open-sourced a Python package. In addition, we showed the differences to the correlation on some examples and discussed some new insights that we can derive from the PPS matrix. Now it is up to you to decide what you think about the PPS and if you want to use it on your own projects.\nGithub: https://github.com/8080labs/ppscore."
  },
  {
    "objectID": "posts/precision-recall.html",
    "href": "posts/precision-recall.html",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "",
    "text": "A receiver operating characteristic (ROC) curve displays how well a model can classify binary outcomes. An ROC curve is generated by plotting the false positive rate of a model against its true positive rate, for each possible cutoff value. Often, the area under the curve (AUC) is calculated and used as a metric showing how well a model can classify data points.\nIf you’re interest in learning more about ROC and AUC, I recommend this short Medium blog, which contains this neat graphic.\n\nDariya Sydykova, graduate student at the Wilke lab at the University of Texas at Austin, shared some great visual animations of how model accuracy and model cutoffs alter the ROC curve and the AUC metric. The quotes and animations are from the associated github repository."
  },
  {
    "objectID": "posts/precision-recall.html#roc-auc-precision-and-recall-visually-explained",
    "href": "posts/precision-recall.html#roc-auc-precision-and-recall-visually-explained",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "",
    "text": "A receiver operating characteristic (ROC) curve displays how well a model can classify binary outcomes. An ROC curve is generated by plotting the false positive rate of a model against its true positive rate, for each possible cutoff value. Often, the area under the curve (AUC) is calculated and used as a metric showing how well a model can classify data points.\nIf you’re interest in learning more about ROC and AUC, I recommend this short Medium blog, which contains this neat graphic.\n\nDariya Sydykova, graduate student at the Wilke lab at the University of Texas at Austin, shared some great visual animations of how model accuracy and model cutoffs alter the ROC curve and the AUC metric. The quotes and animations are from the associated github repository."
  },
  {
    "objectID": "posts/precision-recall.html#roc-auc",
    "href": "posts/precision-recall.html#roc-auc",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "ROC & AUC",
    "text": "ROC & AUC\nThe plot on the left shows the distributions of predictors for the two outcomes, and the plot on the right shows the ROC curve for these distributions. The vertical line that travels left-to-right is the cutoff value. The red dot that travels along the ROC curve corresponds to the false positive rate and the true positive rate for the cutoff value given in the plot on the left.\nThe traveling cutoff demonstrates the trade-off between trying to classify one outcome correctly and trying to classify the other outcome correcly. When we try to increase the true positive rate, we also increase the false positive rate. When we try to decrease the false positive rate, we decrease the true positive rate.\n\nThe shape of an ROC curve changes when a model changes the way it classifies the two outcomes.\nThe animation [below] starts with a model that cannot tell one outcome from the other, and the two distributions completely overlap (essentially a random classifier). As the two distributions separate, the ROC curve approaches the left-top corner, and the AUC value of the curve increases. When the model can perfectly separate the two outcomes, the ROC curve forms a right angle and the AUC becomes 1."
  },
  {
    "objectID": "posts/precision-recall.html#precision-recall",
    "href": "posts/precision-recall.html#precision-recall",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Precision-recall",
    "text": "Precision-recall\nTwo other metrics that are often used to quantify model performance are precision and recall.\nPrecision (also called positive predictive value) is defined as the number of true positives divided by the total number of positive predictions. Hence, precision quantifies what percentage of the positive predictions were correct: How correct your model’s positive predictions were.\nRecall (also called sensitivity) is defined as the number of true positives divided by the total number of true postives and false negatives (i.e. all actual positives). Hence, recall quantifies what percentage of the actual positives you were able to identify: How sensitive your model was in identifying positives.\nDariya also made some visualizations of precision-recall curves: precision-recall curves also displays how well a model can classify binary outcomes. However, it does it differently from the way an ROC curve does. Precision-recall curve plots true positive rate (recall or sensitivity) against the positive predictive value (precision).\nIn the middle, here below, the ROC curve with AUC. On the right, the associated precision-recall curve. Similarly to the ROC curve, when the two outcomes separate, precision-recall curves will approach the top-right corner. Typically, a model that produces a precision-recall curve that is closer to the top-right corner is better than a model that produces a precision-recall curve that is skewed towards the bottom of the plot."
  },
  {
    "objectID": "posts/precision-recall.html#class-imbalance",
    "href": "posts/precision-recall.html#class-imbalance",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Class imbalance",
    "text": "Class imbalance\nClass imbalance happens when the number of outputs in one class is different from the number of outputs in another class. For example, one of the distributions has 1000 observations and the other has 10. An ROC curve tends to be more robust to class imbalanace that a precision-recall curve.\nIn this animation [below], both distributions start with 1000 outcomes. The blue one is then reduced to 50. The precision-recall curve changes shape more drastically than the ROC curve, and the AUC value mostly stays the same. We also observe this behaviour when the other disribution is reduced to 50.\n\nHere’s the same, but now with the red distribution shrinking to just 50 samples."
  },
  {
    "objectID": "posts/precision-recall.html#further-reading",
    "href": "posts/precision-recall.html#further-reading",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Further reading",
    "text": "Further reading\n\nSaito & Rehmsmeier (2015), The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets"
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Getting into python",
    "section": "",
    "text": "Python has emerged over the last couple decades as a first-class tool for scientific computing tasks, including the analysis and visualization of large datasets. This may have come as a surprise to early proponents of the Python language: the language itself was not specifically designed with data analysis or scientific computing in mind. The usefulness of Python for data science stems primarily from the large and active ecosystem of third-party packages, particularly:\n\nNumPy for manipulation of homogeneous array-based data;\nPandas for manipulation of heterogeneous and labeled data, and the more recent high-performace dataprocessing libraries such as polars and ibis;\nSciPy for common scientific computing tasks, Matplotlib for publication-quality visualizations;\nIPython for interactive execution and sharing of code;\nScikit-Learn for machine learning."
  },
  {
    "objectID": "python/index.html#why-python-for-data-science",
    "href": "python/index.html#why-python-for-data-science",
    "title": "Getting into python",
    "section": "",
    "text": "Python has emerged over the last couple decades as a first-class tool for scientific computing tasks, including the analysis and visualization of large datasets. This may have come as a surprise to early proponents of the Python language: the language itself was not specifically designed with data analysis or scientific computing in mind. The usefulness of Python for data science stems primarily from the large and active ecosystem of third-party packages, particularly:\n\nNumPy for manipulation of homogeneous array-based data;\nPandas for manipulation of heterogeneous and labeled data, and the more recent high-performace dataprocessing libraries such as polars and ibis;\nSciPy for common scientific computing tasks, Matplotlib for publication-quality visualizations;\nIPython for interactive execution and sharing of code;\nScikit-Learn for machine learning."
  },
  {
    "objectID": "python/index.html#how-much-python-should-i-know",
    "href": "python/index.html#how-much-python-should-i-know",
    "title": "Getting into python",
    "section": "How much Python should I know?",
    "text": "How much Python should I know?\nAs with any other (programming) language, it takes years to master it fluently which is beyond the scope this anthology. Instead, our objective is to have a working knowledge of Python to be able to learn and apply machine learning. To make this explicit we take the following book and online resources as our point of reference.\n\nA Whirlwind Tour of Python (pages number from the pdf version):\n\nKnow how to install and use Python on your own computer (pages 1 to 13)\nKnow basic semantics of variables, objects and operators (pages 13 to 24)\nKnow built-in simple values and data structures (pages 24 to 37)\nKnow how to use control flow and functions (pages 37 to 45)\nKnow how to iterate and use list comprehensions (pages 52 to 61)\n\nPython for Data Analysis\n\nKnow how to manipulate data with pandas, with Python for Data Analysis, Third Edition as your reference guide\n\n\n\n\n\n\n\n\nPCEP™ – Certified Entry-Level Python Programmer\n\n\n\nThe learning path proposed here is similar to the PCEP™ – Certified Entry-Level Python Programmer certification. The PCEP™ certification is a good way to assess your current Python knowledge and to prepare for the Machine Learning Foundation course. The certification is offered by the Python Institute. You may opt to obtain this certificate."
  },
  {
    "objectID": "python/index.html#how-should-i-learn-python",
    "href": "python/index.html#how-should-i-learn-python",
    "title": "Getting into python",
    "section": "How should I learn Python?",
    "text": "How should I learn Python?\nRealPython.com is the recommended online learning environment for Python. We have collated a learning path for data science."
  },
  {
    "objectID": "python/index.html#which-python-environment-should-i-use",
    "href": "python/index.html#which-python-environment-should-i-use",
    "title": "Getting into python",
    "section": "Which Python environment should I use?",
    "text": "Which Python environment should I use?\nOptions how to start using Python are listed below.\n\nOnline environmentLocal environment\n\n\nFor those new to Python, it is probably easiest to start with one of these online tools:\n\nDeepnote: there is a generous free-tier. If you decide to upgrade, you can collaborate and share notebooks privately.\nGoogle Colab:\n\nActivate a Google account if you haven’t got one yet.\nWork your way through the Colab introduction notebook.\n\n\nOnce you have gained some traction, you can move on to install Python on your local machine.\n\n\nYou can setup your local machine/laptop for data science and machine learning as a follows\n\nInstall Anaconda (recommended package manager for data science).\nInstall Visual Studio Code including the Python extension (recommended integrated development environment (IDE))."
  },
  {
    "objectID": "python/index.html#guidelines-for-using-python-for-data-science",
    "href": "python/index.html#guidelines-for-using-python-for-data-science",
    "title": "Getting into python",
    "section": "Guidelines for using Python for data science",
    "text": "Guidelines for using Python for data science\nUsing Python for data science is inherently different than using it for, say, building a website. To provide you with some guidance to the many different ways c.q. styles of using Python, please consider the following:\n\nFocus on using existing data science libraries, instead of writing your own basic functions. If you find yourself spending a lot of time reading documentation, you are on the right track.\nTake a functional approach to programming instead of an object-oriented approach. The former is more fitting for data science, where it is common to structure your work in terms of pipelines and think about each processing step as a function. The latter is more suitable for application development.\n\nFor those wanting to further develop their Python skills for data science, the following books are recommended:\n\nPython for Data Analysis 3rd Edition by Wes McKinney, the creator of pandas.\nData Science With Python Core Skills on Real Python provides an extensive learning path.\nHands-On Machine Learning with Scikit-Learn, Keras and Tensorflow (2nd edition) by Aurélien Géron. You will need to purchase the book, but the notebooks with example code are freely available.\nEffective Python: 90 Specific Ways to Write Better Python (second edition)."
  },
  {
    "objectID": "python/index.html#more-on-python",
    "href": "python/index.html#more-on-python",
    "title": "Getting into python",
    "section": "More on Python",
    "text": "More on Python\n\n\n\n\n\n\n\n\n\n\nGetting into Python with RealPython.com\n\n\nWe have compiled a learning path for those who are new to Python, using a selection of chapters from Real Python.\n\n\n\nDaniel Kapitan\n\n\nNov 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Python and idiomatic pandas\n\n\nGuidelines on how to continue to develop your skills to write effective Python and use pandas in an idiomatic way.\n\n\n\nDaniel Kapitan\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Whirlwind Tour of Python\n\n\nA brief but comprehensive tour of the Python language for those who have (sometimes extensive) backgrounds in computing in some language.\n\n\n\nJake VanderPlas\n\n\nAug 10, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "python/effective-python-and-idiomatic-pandas.html",
    "href": "python/effective-python-and-idiomatic-pandas.html",
    "title": "Effective Python and idiomatic pandas",
    "section": "",
    "text": "After you have gained basic working knowledge of Python, it is essential that you continue to develop your skills to write effective Python and use pandas in an idiomatic way. This notebook provides guidelines how to do this. It is very opinionated, and based on a functional programming approach to Python. Functional programming defines a computation using expressions and evaluation—often encapsulated in function definitions. It de-emphasizes or avoids the complexity of state change and mutable objects. This tends to create programs that are more succinct and expressive. Since you can’t easily create purely functional programs in Python, we take a hybrid approach, using functional features where we can add clariy or simplify the code. There are several key features of functional programming that are available in Python which we will use.\nFunctional programming is often a very good fit for data analysis.\n\nUse functions as first-class objects, meaning you can assign functions to variables and pass functions as arguments in other functions allowing for for composability of complex functions for data pipelines using smaller, more simple ones.\nUse of higher-order functions that operate on lists (iterables), like sorted(), min() and max(). NumPy and pandas follow this same principle with so-called universal functions, being functions that operate on ndarrays in an element-by-element fashion, supporting array broadcasting, type casting, and several other standard features.\nFunctional programmes often exploits immutable data structures. Python offers tuples, namedtuples and frozen dataclass as complex but immutable objects. Although NumPy arrays and pandas DataFrames are mutable, we will adhere to the principle of immutable in data pipelines as much as possible."
  },
  {
    "objectID": "python/effective-python-and-idiomatic-pandas.html#a-functional-approach-to-python",
    "href": "python/effective-python-and-idiomatic-pandas.html#a-functional-approach-to-python",
    "title": "Effective Python and idiomatic pandas",
    "section": "",
    "text": "After you have gained basic working knowledge of Python, it is essential that you continue to develop your skills to write effective Python and use pandas in an idiomatic way. This notebook provides guidelines how to do this. It is very opinionated, and based on a functional programming approach to Python. Functional programming defines a computation using expressions and evaluation—often encapsulated in function definitions. It de-emphasizes or avoids the complexity of state change and mutable objects. This tends to create programs that are more succinct and expressive. Since you can’t easily create purely functional programs in Python, we take a hybrid approach, using functional features where we can add clariy or simplify the code. There are several key features of functional programming that are available in Python which we will use.\nFunctional programming is often a very good fit for data analysis.\n\nUse functions as first-class objects, meaning you can assign functions to variables and pass functions as arguments in other functions allowing for for composability of complex functions for data pipelines using smaller, more simple ones.\nUse of higher-order functions that operate on lists (iterables), like sorted(), min() and max(). NumPy and pandas follow this same principle with so-called universal functions, being functions that operate on ndarrays in an element-by-element fashion, supporting array broadcasting, type casting, and several other standard features.\nFunctional programmes often exploits immutable data structures. Python offers tuples, namedtuples and frozen dataclass as complex but immutable objects. Although NumPy arrays and pandas DataFrames are mutable, we will adhere to the principle of immutable in data pipelines as much as possible."
  },
  {
    "objectID": "python/effective-python-and-idiomatic-pandas.html#structure-of-notebook",
    "href": "python/effective-python-and-idiomatic-pandas.html#structure-of-notebook",
    "title": "Effective Python and idiomatic pandas",
    "section": "Structure of notebook",
    "text": "Structure of notebook\nThis notebook is structred in a couple of chapters with a number of related items. Feel free to jump between the items. Each item contains concise and specific guidance explaining how you can write effective, functional Python. We have used the following books and online resources, which are recommended for further reading:\n\nEffective Python, 2nd edition, by Brett Slatkin (eBook: EUR 30). Referenced as EP for items that have been taken from this book (which is warmly recommended).\nBlog posts on Modern pandas by Tom Augspurger.\nFunctional Python Programming by Steven F. Lott (eBook: EUR 5).\nIntermediate tutorials for data science on Real Python."
  },
  {
    "objectID": "python/effective-python-and-idiomatic-pandas.html#pythonic-thinking",
    "href": "python/effective-python-and-idiomatic-pandas.html#pythonic-thinking",
    "title": "Effective Python and idiomatic pandas",
    "section": "Pythonic thinking",
    "text": "Pythonic thinking\n\n#1. Follow the PEP-8 style guide (EP #2)\nPython Enhancement Proposal #8, otherwise known as PEP-8, is the style guide for how to format Python code. You are welcome to write Python code any way you want, as long as it has valid syntax. However, using a consistent style makes your code more approachable and easier to read. Sharing a common style with other Python programmers in the larger community facilitates collaboration on projects. But even if you are the only one who will ever read your code, following the style guide will make it easier for you to change things later, and can help you avoid many common errors.\nPEP-8 provides a wealth of details about how to write clear Python code. It continues to be updated as the Python language evolves. It’s worth reading the whole guide online. Here are a few rules you should be sure to follow.\n\nWhitespace\n\nUse spaces instead of tabs for indentation.\nUse four spaces for each level of syntactically significant indenting.\nLines should be 79 characters in length or less.\nContinuations of long expressions onto additional lines should be indented by four extra spaces from their normal indentation level.\nIn a file, functions and classes should be separated by two blank lines.\nIn a class, methods should be separated by one blank line.\nIn a dictionary, put no whitespace between each key and colon, and put a single space before the corresponding value if it fits on the same line.\nPut one—and only one—space before and after the = operator in a variable assignment.\nFor type annotations, ensure that there is no separation between the variable name and the colon, and use a space before the type information.\n\n\n\nNaming\nPEP 8 suggests unique styles of naming for different parts in the language. These conventions make it easy to distinguish which type corresponds to each name when reading code. Follow these guidelines related to naming:\n\nFunctions, variables, and attributes should be in lowercase_underscore format.\nProtected instance attributes should be in _leading_underscore format.\nPrivate instance attributes should be in __double_leading_underscore format.\nClasses (including exceptions) should be in CapitalizedWord format.\nModule-level constants should be in ALL_CAPS format.\nInstance methods in classes should use self, which refers to the object, as the name of the first parameter.\nClass methods should use cls, which refers to the class, as the name of the first parameter.\n\n\n\nExpressions and Statements\nThe Zen of Python states: “There should be one—and preferably only one—obvious way to do it.” PEP 8 attempts to codify this style in its guidance for expressions and statements:\n\nUse inline negation (if a is not b) instead of negation of positive expressions (if not a is b).\nDon’t check for empty containers or sequences (like [] or '') by comparing the length to zero (if len(somelist) == 0). Use if not somelist and assume that empty values will implicitly evaluate to False.\nThe same thing goes for non-empty containers or sequences (like [1] or 'hi'). The statement if somelist is implicitly True for non-empty values.\nAvoid single-line if statements, for and while loops, and except compound statements. Spread these over multiple lines for clarity.\nIf you can’t fit an expression on one line, surround it with parentheses and add line breaks and indentation to make it easier to read.\nPrefer surrounding multiline expressions with parentheses over using the \\ line continuation character.\n\n\n\nImports\nPEP 8 suggests some guidelines for how to import modules and use them in your code:\n\nAlways put import statements (including from x import y) at the top of a file.\nAlways use absolute names for modules when importing them, not names relative to the current module’s own path. For example, to import the foo module from within the bar package, you should use from bar import foo, not just import foo.\nIf you must do relative imports, use the explicit syntax from . import foo.\nImports should be in sections in the following order:\n\nstandard library modules\nthird-party modules\nyour own modules.\n\nEach subsection should have imports in alphabetical order.\n\n\n\nUse code formatters\nIt is recommended to use a code formatter like pylint or Black to handle these conventions automatically. There is also Jupyter Black for doing this in notebooks.\n\n\n\n#2. Prefer interpolated f-strings over C-style formatting (EP #4)\nBefore f-strings were introduced in Python 3.6, you had to do this to format your strings:\n\nfirst_name = \"Eric\"\nlast_name = \"Idle\"\nage = 74\nprofession = \"comedian\"\naffiliation = \"Monty Python\"\nprint(\n    \"Hello, %s %s. You are %s. You are a %s. You were a member of %s.\"\n    % (first_name, last_name, age, profession, affiliation)\n)\n\nHello, Eric Idle. You are 74. You are a comedian. You were a member of Monty Python.\n\n\nAlthough the str.format() syntax is definitely better, it is still verbose:\n\nprint(\n    \"Hello {} {}, You are a {}. You are a{}. You were a member of {}\".format(\n        first_name, last_name, age, profession, affiliation\n    )\n)\n\nHello Eric Idle, You are a 74. You are acomedian. You were a member of Monty Python\n\n\nWith f-strings this becomes a lot more readable.\n\nprint(\n    f\"Hello {first_name} {last_name}. You are {age}. You are a {profession}. You were a member of {affiliation}\"\n)\n\nHello Eric Idle. You are 74. You are a comedian. You were a member of Monty Python\n\n\nYou can read more about using f-strings in this Real Python tutorial.\n\n\n#3. Write helper functions instead of complex expressions (EP #5)\nPython’s pithy syntax makes it easy to write single-line expressions that implement a lot of logic. For example, say that I want to decode the query string from a URL. Here, each query string parameter represents an integer value:\n\nfrom urllib.parse import parse_qs\n\nmy_values = parse_qs(\"red=5&blue=0&green=\", keep_blank_values=True)\nprint(repr(my_values))\n\n{'red': ['5'], 'blue': ['0'], 'green': ['']}\n\n\nSome query string parameters may have multiple values, some may have single values, some may be present but have blank values, and some may be missing entirely. Using the get (more on that in item 11) method on the result dictionary will return different values in each circumstance:\n\nprint(\"Red:     \", my_values.get(\"red\"))\nprint(\"Green:   \", my_values.get(\"green\"))\nprint(\"Opacity: \", my_values.get(\"opacity\"))\n\nRed:      ['5']\nGreen:    ['']\nOpacity:  None\n\n\nIt’d be nice if a default value of 0 were assigned when a parameter isn’t supplied or is blank. I might choose to do this with Boolean expressions because it feels like this logic doesn’t merit a whole if statement or helper function quite yet.\nPython’s syntax makes this choice all too easy. The trick here is that the empty string, the empty list, and zero all evaluate to False implicitly. Thus, the expressions below will evaluate to the subexpression after the or operator when the first subexpression is False:\n\nred = my_values.get(\"red\", [\"\"])[0] or 0\ngreen = my_values.get(\"green\", [\"\"])[0] or 0\nopacity = my_values.get(\"opacity\", [\"\"])[0] or 0\nprint(f\"Red:     {red!r}\")\nprint(f\"Green:   {green!r}\")\nprint(f\"Opacity: {opacity!r}\")\n\nRed:     '5'\nGreen:   0\nOpacity: 0\n\n\nThe red case works because the key is present in the my_values dictionary. The value is a list with one member: the string '5'. This string implicitly evaluates to True, so red is assigned to the first part of the or expression.\nThe green case works because the value in the my_values dictionary is a list with one member: an empty string. The empty string implicitly evaluates to False, causing the or expression to evaluate to 0.\nThe opacity case works because the value in the my_values dictionary is missing altogether. The behavior of the get method is to return its second argument if the key doesn’t exist in the dictionary (see item 11: “Prefer get over in and KeyError to handle missing dictionary keys”). The default value in this case is a list with one member: an empty string. When opacity isn’t found in the dictionary, this code does exactly the same thing as the green case.\nHowever, this expression is difficult to read, and it still doesn’t do everything I need. I’d also want to ensure that all the parameter values are converted to integers so I can immediately use them in mathematical expressions. To do that, I’d wrap each expression with the int built-in function to parse the string as an integer:\n\nred = int(my_values.get(\"red\", [\"\"])[0] or 0)\ngreen = int(my_values.get(\"green\", [\"\"])[0] or 0)\nopacity = int(my_values.get(\"opacity\", [\"\"])[0] or 0)\nprint(f\"Red:     {red}\")\nprint(f\"Green:   {green}\")\nprint(f\"Opacity: {opacity}\")\n\nRed:     5\nGreen:   0\nOpacity: 0\n\n\nYou see this get unwieldly quite quickly. Furthermore, if you need to reuse this logic repeatedly - even just to or three times, as in this example, - then writing a helper function is the way to go:\n\ndef get_first_int(values, key, default=0):\n    found = values.get(key, [\"\"])\n    if found[0]:\n        return int(found[0])\n    return default\n\n\ngreen = get_first_int(my_values, \"green\")\nprint(f\"Green:   {green!r}\")\n\nGreen:   0\n\n\nAs soon as expressions get complicated, it’s time to consider splitting them into smaller pieces and moving logic into helper functions. What you gain in readability always outweighs what brevity may have afforded you. Avoid letting Python’s pithy syntax for complex expressions from getting you into a mess like this. Follow the DRY principle: Don’t repeat yourself.\n\n\n#4. Prefer multiple assignment unpacking over indexing (EP #6)\nPython also has syntax for unpacking, which allows for assigning multiple values in a single statement. The patterns that you specify in unpacking assignments look a lot like trying to mutate tuples—which isn’t allowed—but they actually work quite differently. For example, if you know that a tuple is a pair, instead of using indexes to access its values, you can assign it to a tuple of two variable names:\n\nitem = (\"Peanut butter\", \"Jelly\")\nfirst, second = item  # Unpacking\nprint(first, \"and\", second)\n\nPeanut butter and Jelly\n\n\nUnpacking has less visual noise than accessing the tuple’s indexes, and it often requires fewer lines. The same pattern matching syntax of unpacking works when assigning to lists, sequences, and multiple levels of arbitrary iterables within iterables. Newcomers to Python may be surprised to learn that unpacking can even be used to swap values in place without the need to create temporary variables. Here, I use typical syntax with indexes to swap the values between two positions in a list as part of an ascending order sorting algorithm (using bubble sort):\n\ndef bubble_sort(a):\n    for _ in range(len(a)):\n        for i in range(1, len(a)):\n            if a[i] &lt; a[i - 1]:\n                temp = a[i]\n                a[i] = a[i - 1]\n                a[i - 1] = temp\n\n\nnames = [\"pretzels\", \"carrots\", \"arugula\", \"bacon\"]\nbubble_sort(names)\nprint(names)\n\n['arugula', 'bacon', 'carrots', 'pretzels']\n\n\nHowever, with unpacking syntax, it’s possible to swap indexes in a single line:\n\ndef bubble_sort(a):\n    for _ in range(len(a)):\n        for i in range(1, len(a)):\n            if a[i] &lt; a[i - 1]:\n                a[i - 1], a[i] = a[i], a[i - 1]  # Swap\n\n\nnames = [\"pretzels\", \"carrots\", \"arugula\", \"bacon\"]\nbubble_sort(names)\nprint(names)\n\n['arugula', 'bacon', 'carrots', 'pretzels']\n\n\nThe way this swap works is that the right side of the assignment (a[i], a[i-1]) is evaluated first, and its values are put into a new temporary, unnamed tuple (such as ('carrots', 'pretzels') on the first iteration of the loops). Then, the unpacking pattern from the left side of the assignment (a[i-1], a[i]) is used to receive that tuple value and assign it to the variable names a[i-1] and a[i], respectively. This replaces 'pretzels' with 'carrots' at index 0 and 'carrots' with 'pretzels' at index 1. Finally, the temporary unnamed tuple silently goes away.\nAnother valuable application of unpacking is in the target list of for loops and similar constructs, such as comprehensions and generator expressions (see item 13: “Use comprehensions instead of map and filter” for those). As an example for contrast, here I iterate over a list of snacks without using unpacking:\n\nsnacks = [(\"bacon\", 350), (\"donut\", 240), (\"muffin\", 190)]\nfor i in range(len(snacks)):\n    item = snacks[i]\n    name = item[0]\n    calories = item[1]\n    print(f\"#{i+1}: {name} has {calories} calories\")\n\n#1: bacon has 350 calories\n#2: donut has 240 calories\n#3: muffin has 190 calories\n\n\nThis works, but it’s noisy. There are a lot of extra characters required in order to index into the various levels of the snacks structure. Here, I achieve the same output by using unpacking along with the enumerate built-in function (see item 5: “Prefer enumerate Over range”):\n\nfor rank, (name, calories) in enumerate(snacks, 1):\n    print(f\"#{rank}: {name} has {calories} calories\")\n\n#1: bacon has 350 calories\n#2: donut has 240 calories\n#3: muffin has 190 calories\n\n\nUsing unpacking wisely will enable you to avoid indexing when possible, resulting in clearer and more Pythonic code. So remember:\n\nPython has special syntax called unpacking for assigning multiple values in a single statement.\nUnpacking is generalized in Python and can be applied to any iterable, including many levels of iterables within iterables.\nReduce visual noise and increase code clarity by using unpacking to avoid explicitly indexing into sequences.\n\n ### #5. Prefer enumerate over range (EP #7)\nWhen you have a data structure to iterate over, like a list of strings, you can loop directly over the sequence:\n\nflavor_list = [\"vanilla\", \"chocolate\", \"pecan\", \"strawberry\"]\nfor flavor in flavor_list:\n    print(f\"{flavor} is delicious\")\n\nvanilla is delicious\nchocolate is delicious\npecan is delicious\nstrawberry is delicious\n\n\nOften, you’ll want to iterate over a list and also know the index of the current item in the list. For example, say that I want to print the ranking of my favorite ice cream flavors. One way to do it is by using range:\n\nfor i in range(len(flavor_list)):\n    flavor = flavor_list[i]\n    print(f\"{i + 1}: {flavor}\")\n\n1: vanilla\n2: chocolate\n3: pecan\n4: strawberry\n\n\nThis looks clumsy compared with the other example of iterating over flavor_list. I have to get the length of the list. I have to index into the array. The multiple steps make it harder to read.\nPython provides the enumerate built-in function to address this situation. enumerate wraps any iterator with a lazy generator (read more on generators in this Real Python article). enumerate yields pairs of the loop index and the next value from the given iterator. Here, I manually advance the returned iterator with the next built-in function to demonstrate what it does:\n\nfor i, flavor in enumerate(flavor_list):\n    print(f\"{i + 1}: {flavor}\")\n\n1: vanilla\n2: chocolate\n3: pecan\n4: strawberry\n\n\nI can make this even shorter by specifying the number from which enumerate should begin counting (1 in this case) as the second parameter:\n\nfor i, flavor in enumerate(flavor_list, 1):\n    print(f\"{i}: {flavor}\")\n\n1: vanilla\n2: chocolate\n3: pecan\n4: strawberry\n\n\n\n\n#6. Use zip to process iterators in parallel (EP #8)\nOften in Python you find yourself with many lists of related objects. List comprehensions make it easy to take a source list and get a derived list by applying an expression (see item 13: “Use comprehensions instead of map and filter”):\n\nnames = [\"Cecilia\", \"Lise\", \"Marie\"]\ncounts = [len(n) for n in names]\nprint(counts)\n\n[7, 4, 5]\n\n\nThe items in the derived list are related to the items in the source list by their indexes. To iterate over both lists in parallel, I can iterate over the length of the names source list:\n\nlongest_name = None\nmax_count = 0\n\nfor i in range(len(names)):\n    count = counts[i]\n    if count &gt; max_count:\n        longest_name = names[i]\n        max_count = count\n\nprint(longest_name)\n\nCecilia\n\n\nThe problem is that this whole loop statement is visually noisy. The indexes into names and counts make the code hard to read. Indexing into the arrays by the loop index i happens twice. Using enumerate (see item 5: “Prefer enumerate Over range”) improves this slightly, but it’s still not ideal:\n\nlongest_name = None\nmax_count = 0\nfor i, name in enumerate(names):\n    count = counts[i]\n    if count &gt; max_count:\n        longest_name = name\n        max_count = count\n\nprint(longest_name)\n\nCecilia\n\n\nTo make this code clearer, Python provides the zip built-in function. zip wraps two or more iterators with a lazy generator. The zip generator yields tuples containing the next value from each iterator. These tuples can be unpacked directly within a for statement (see item 4: “Prefer multiple assignment unpacking over indexing”). The resulting code is much cleaner than the code for indexing into multiple lists:\n\nlongest_name = None\nmax_count = 0\nfor name, count in zip(names, counts):\n    if count &gt; max_count:\n        longest_name = name\n        max_count = count\n\nprint(longest_name)\n\nCecilia\n\n\nzip consumes the iterators it wraps one item at a time, which means it can be used with infinitely long inputs without risk of a program using too much memory and crashing.\nHowever, beware of zip’s behavior when the input iterators are of different lengths. For example, say that I add another item to names above but forget to update counts. Running zip on the two input lists will have an unexpected result:\n\nnames.append(\"Rosalind\")\nfor name, count in zip(names, counts):\n    print(name)\n\nCecilia\nLise\nMarie\n\n\nThe new item for 'Rosalind' isn’t there. Why not? This is just how zip works. It keeps yielding tuples until any one of the wrapped iterators is exhausted. Its output is as long as its shortest input. This approach works fine when you know that the iterators are of the same length, which is often the case for derived lists created by list comprehensions. But in many other cases, the truncating behavior of zip is surprising and bad. If you don’t expect the lengths of the lists passed to zip to be equal, consider using the zip_longest function from the itertools built-in module instead:\n\nimport itertools\n\nfor name, count in itertools.zip_longest(names, counts):\n    print(f\"{name}: {count}\")\n\nCecilia: 7\nLise: 4\nMarie: 5\nRosalind: None\n\n\nzip_longest replaces missing values—the length of the string 'Rosalind' in this case—with whatever fillvalue is passed to it, which defaults to None.\nNote that the itertools implements more iterator building block that come from functional programming. Read more in this Real Python article."
  },
  {
    "objectID": "python/effective-python-and-idiomatic-pandas.html#lists-dictionairies-and-dataclasses",
    "href": "python/effective-python-and-idiomatic-pandas.html#lists-dictionairies-and-dataclasses",
    "title": "Effective Python and idiomatic pandas",
    "section": "Lists, dictionairies and dataclasses",
    "text": "Lists, dictionairies and dataclasses\n\n#7. Use dataclass if you need to create a data container\nThe dataclass class was introduced in Python 3.7. Given our functional approach, you should first ask yourself whether you really need a new container object for your data. In the words of Alan J. Perlis:\n\nIt is better to have 100 functions operate on one data structure than to have 10 functions operate on 10 data structures.\n\nIf you have consciously decided you do want a new data object, then dataclass is your friend. It is created using the new @dataclass decorator as follows:\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass DataClassCard:\n    rank: str\n    suit: str\n\nA dataclass comes with basic functionality already implemented. For instance, you can instantiate, print, and compare dataclass instances straight out of the box:\n\nqueen_of_hearts = DataClassCard(\"Q\", \"Hearts\")\nprint(queen_of_hearts.rank)\n\nQ\n\n\n\nqueen_of_hearts\n\nDataClassCard(rank='Q', suit='Hearts')\n\n\n\nqueen_of_hearts == DataClassCard(\"Q\", \"Hearts\")\n\nTrue\n\n\nRead more on dataclass and how to use them in this Real Python tutorial\n\n\n#8. Prefer catch-all unpacking over slicing (EP #13)\nOne limitation of basic unpacking (see item 4) is that you must know the length of the sequences you’re unpacking in advance. For example, here I have a list of the ages of cars that are being traded in at a dealership. When I try to take the first two items of the list with basic unpacking, an exception is raised at runtime:\n\ncar_ages = [0, 9, 4, 8, 7, 20, 19, 1, 6, 15]\ncar_ages_descending = sorted(car_ages, reverse=True)\noldest, second_oldest = car_ages_descending\n\nValueError: too many values to unpack (expected 2)\n\n\nNewcomers to Python often rely on indexing and slicing for this situation. For example, here I extract the oldest, second oldest, and other car ages from a list of at least two items:\n\noldest = car_ages_descending[0]\nsecond_oldest = car_ages_descending[1]\nothers = car_ages_descending[2:]\nprint(oldest, second_oldest, others)\n\n20 19 [15, 9, 8, 7, 6, 4, 1, 0]\n\n\nHowever, to unpack assignments that contain a starred expression, you must have at least one required part, or else you’ll get a SyntaxError. You can’t use a catch-all expression on its own:\n\n*others = car_ages_descending\n\nSyntaxError: starred assignment target must be in a list or tuple (&lt;ipython-input-30-77c6f344fe32&gt;, line 1)\n\n\nYou also can’t use multiple catch-all expressions in a single-level unpacking pattern:\n\nfirst, *middle, *second_middle, last = [1, 2, 3, 4]\n\nSyntaxError: multiple starred expressions in assignment (&lt;ipython-input-31-77dccc131ad1&gt;, line 1)\n\n\nBut it is possible to use multiple starred expressions in an unpacking assignment statement, as long as they’re catch-alls for different parts of the multilevel structure being unpacked. I don’t recommend doing the following (see item 11: “Never unpack more than three variables when functions return multiple values” for related guidance), but understanding it should help you develop an intuition for how starred expressions can be used in unpacking assignments:\n\ncar_inventory = {\n    \"Downtown\": (\"Silver Shadow\", \"Pinto\", \"DMC\"),\n    \"Airport\": (\"Skyline\", \"Viper\", \"Gremlin\", \"Nova\"),\n}\n\n((loc1, (best1, *rest1)), (loc2, (best2, *rest2))) = car_inventory.items()\nprint(f\"Best at {loc1} is {best1}, {len(rest1)} others\")\nprint(f\"Best at {loc2} is {best2}, {len(rest2)} others\")\n\nBest at Downtown is Silver Shadow, 2 others\nBest at Airport is Skyline, 3 others\n\n\nStarred expressions become list instances in all cases. If there are no leftover items from the sequence being unpacked, the catch-all part will be an empty list. This is especially useful when you’re processing a sequence that you know in advance has at least \\(N\\) elements:\n\nshort_list = [1, 2]\nfirst, second, *rest = short_list\nprint(first, second, rest)\n\n1 2 []\n\n\nBut with the addition of starred expressions, the value of unpacking iterators becomes clear. For example, here I have a generator that yields the rows of a CSV file containing all car orders from the dealership this week:\n\ndef generate_csv():\n    yield (\"Date\", \"Make\", \"Model\", \"Year\", \"Price\")\n    for i in range(100):\n        yield (\"2019-03-25\", \"Honda\", \"Fit\", \"2010\", \"$3400\")\n        yield (\"2019-03-26\", \"Ford\", \"F150\", \"2008\", \"$2400\")\n\nProcessing the results of this generator using indexes and slices is fine, but it requires multiple lines and is visually noisy:\n\nall_csv_rows = list(generate_csv())\nheader = all_csv_rows[0]\nrows = all_csv_rows[1:]\nprint(\"CSV Header:\", header)\nprint(\"Row count: \", len(rows))\n\nCSV Header: ('Date', 'Make', 'Model', 'Year', 'Price')\nRow count:  200\n\n\nUnpacking with a starred expression makes it easy to process the first row—the header—separately from the rest of the iterator’s contents. This is much clearer:\n\nit = generate_csv()\nheader, *rows = it\nprint(\"CSV Header:\", header)\nprint(\"Row count: \", len(rows))\n\nCSV Header: ('Date', 'Make', 'Model', 'Year', 'Price')\nRow count:  200\n\n\nKeep in mind, however, that because a starred expression is always turned into a list, unpacking an iterator also risks the potential of using up all of the memory on your computer and causing your program to crash. So you should only use catch-all unpacking on iterators when you have good reason to believe that the result data will all fit in memory.\nFinally, note that the unpacking operators * for lists and ** for dicts is often used in functions. Read this tutorial on Real Python that demystifies *args and **kwargs. Note that args and kwargs are just names that are used by convention to refer to positional arguments and keyword arguments, respectively. You could use any other name if you wanted to. The magic lies in the * and ** unpacking operators, as is shown with this trick from Real Python’s homepage:\n\nx = {\"a\": 1, \"b\": 2}\ny = {\"c\": 3, \"d\": 4}\nz = {**x, **y}\n\nWhen unpacking dicts with overlapping keys, the last value is kept:\n\ny = {\"b\": 3, \"c\": 4}\n{**x, **y}\n\n{'a': 1, 'b': 3, 'c': 4}\n\n\nUsing **kwargs you can easily re-use parameters are the same for different calls to the same function:\n\nimport pandas as pd\n\ncsv_kwargs = {\"sep\": \";\", \"encoding\": \"utf-8\"}\names = pd.read_csv(\n    \"https://github.com/eaisi/discover-projects/blob/main/ames-housing/AmesHousing.csv?raw=true\",\n    **csv_kwargs\n)\npima = pd.read_csv(\n    \"https://github.com/eaisi/discover-projects/blob/main/pima-indians-diabetes/diabetes.csv?raw=true\",\n    **csv_kwargs\n)\n\n\n\n#9. Sort by complex criteria using the key parameter (EP #14)\nThe list built-in type provides a sort method for ordering the items in a list instance based on a variety of criteria. By default, sort will order a list’s contents by the natural ascending order of the items. For example, here I sort a list of integers from smallest to largest:\n\nnumbers = [93, 86, 11, 68, 70]\nnumbers.sort()\nprint(numbers)\n\n[11, 68, 70, 86, 93]\n\n\nThe sort method works for nearly all built-in types (strings, floats, etc.) that have a natural ordering to them. What does sort do with objects? Often there’s an attribute on the object that you’d like to use for sorting. To support this use case, the sort method accepts a key parameter that’s expected to be a function. The key function is passed a single argument, which is an item from the list that is being sorted. The return value of the key function should be a comparable value (i.e., with a natural ordering) to use in place of an item for sorting purposes.\nFor example, here I define a dataclass to represent various tools. To sort a the list of Tool object alphabetically by name, I use the lambda keyword to define a function for the key parameter:\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Tool:\n    name: str\n    weight: float\n\n\ntools = [\n    Tool(\"level\", 3.5),\n    Tool(\"hammer\", 1.25),\n    Tool(\"screwdriver\", 0.5),\n    Tool(\"chisel\", 0.25),\n]\n\n\nprint(\"\\nUnsorted:\\n\", tools)\ntools.sort(key=lambda x: x.name)\nprint(\"\\nSorted:\\n\", tools)\n\n\nUnsorted:\n [Tool(name='level', weight=3.5), Tool(name='hammer', weight=1.25), Tool(name='screwdriver', weight=0.5), Tool(name='chisel', weight=0.25)]\n\nSorted:\n [Tool(name='chisel', weight=0.25), Tool(name='hammer', weight=1.25), Tool(name='level', weight=3.5), Tool(name='screwdriver', weight=0.5)]\n\n\nI can just as easily define another lambda function to sort by weight and pass it as the key parameter to the sort method:\n\ntools.sort(key=lambda x: x.weight)\nprint(\"\\nBy weight:\\n\", tools)\n\n\nBy weight:\n [Tool(name='chisel', weight=0.25), Tool(name='screwdriver', weight=0.5), Tool(name='hammer', weight=1.25), Tool(name='level', weight=3.5)]\n\n\nWithin the lambda function passed as the key parameter you can access attributes of items as I’ve done here, index into items (for sequences, tuples, and dictionaries), or use any other valid expression.\nFor basic types like strings, you may even want to use the key function to do transformations on the values before sorting. For example, here I apply the lower method to each item in a list of place names to ensure that they’re in alphabetical order, ignoring any capitalization (since in the natural lexical ordering of strings, capital letters come before lowercase letters):\n\nplaces = [\"home\", \"work\", \"New York\", \"Paris\"]\nplaces.sort()\nprint(\"Case sensitive: \", places)\nplaces.sort(key=lambda x: x.lower())\nprint(\"Case insensitive:\", places)\n\nCase sensitive:  ['New York', 'Paris', 'home', 'work']\nCase insensitive: ['home', 'New York', 'Paris', 'work']\n\n\nSometimes you may need to use multiple criteria for sorting. For example, say that I have a list of power tools and I want to sort them first by weight and then by name. How can I accomplish this?\n\npower_tools = [\n    Tool(\"drill\", 4),\n    Tool(\"circular saw\", 5),\n    Tool(\"jackhammer\", 40),\n    Tool(\"sander\", 4),\n]\n\nThe simplest solution in Python is to use the tuple type. Tuples are immutable sequences of arbitrary Python values. Tuples are comparable by default and have a natural ordering, meaning that they implement all of the special methods, such as __lt__, that are required by the sort method. Tuples implement these special method comparators by iterating over each position in the tuple and comparing the corresponding values one index at a time. Here, I show how this works when one tool is heavier than another:\n\nsaw = (5, \"circular saw\")\njackhammer = (40, \"jackhammer\")\n(jackhammer &lt; saw)  # False\n\nFalse\n\n\nIf the first position in the tuples being compared are equal—weight in this case—then the tuple comparison will move on to the second position, and so on. You can take advantage of this tuple comparison behavior in order to sort the list of power tools first by weight and then by name. Here, I define a key function that returns a tuple containing the two attributes that I want to sort on in order of priority:\n\npower_tools.sort(key=lambda x: (x.weight, x.name))\nprint(power_tools)\n\n[Tool(name='drill', weight=4), Tool(name='sander', weight=4), Tool(name='circular saw', weight=5), Tool(name='jackhammer', weight=40)]\n\n\nOne limitation of having the key function return a tuple is that the direction of sorting for all criteria must be the same (either all in ascending order, or all in descending order). If I provide the reverse parameter to the sort method, it will affect both criteria in the tuple the same way (note how ’sander' now comes before 'drill' instead of after):\n\npower_tools.sort(\n    key=lambda x: (x.weight, x.name), reverse=True\n)  # Makes all criteria descending\nprint(power_tools)\n\n[Tool(name='jackhammer', weight=40), Tool(name='circular saw', weight=5), Tool(name='sander', weight=4), Tool(name='drill', weight=4)]\n\n\nFor situations where you do want to have different sorting orders, Python provides a stable sorting algorithm. The sort method of the list type will preserve the order of the input list when the key function returns values that are equal to each other. This means that I can call sort multiple times on the same list to combine different criteria together. Here, I produce the same sort ordering of weight descending and nameb ascending as I did above but by using two separate calls to sort:\n\npower_tools.sort(key=lambda x: x.name)  # Name ascending\npower_tools.sort(key=lambda x: x.weight, reverse=True)  # Weight descending\nprint(power_tools)\n\n[Tool(name='jackhammer', weight=40), Tool(name='circular saw', weight=5), Tool(name='drill', weight=4), Tool(name='sander', weight=4)]\n\n\nThis same approach can be used to combine as many different types of sorting criteria as you’d like in any direction, respectively. You just need to make sure that you execute the sorts in the opposite sequence of what you want the final list to contain. In this example, I wanted the sort order to be by weight descending and then by name ascending, so I had to do the name sort first, followed by the weight sort.\nThat said, the approach of having the key function return a tuple, and using unary negation to mix sort orders for numbers, is simpler to read and requires less code. I recommend only using multiple calls to sort if it’s absolutely necessary.\n ### #10. Prefer get over in and KeyError to handle missing dictionairy keys (EP 16)\nThe three fundamental operations for interacting with dictionaries are accessing, assigning, and deleting keys and their associated values. The contents of dictionaries are dynamic, and thus it’s entirely possible—even likely—that when you try to access or delete a key, it won’t already be present.\nFor example, say that I’m trying to determine people’s favorite type of bread to devise the menu for a sandwich shop. Here, I define a dictionary of counters with the current votes for each style:\n\ncounters = {\n    \"pumpernickel\": 2,\n    \"sourdough\": 1,\n}\n\nTo increment the counter for a new vote, I need to see if the key exists, insert the key with a default counter value of zero if it’s missing, and then increment the counter’s value. This requires accessing the key two times and assigning it once. Here, I accomplish this task using an if statement with an in expression that returns True when the key is present:\n\nkey = \"wheat\"\n\nif key in counters:\n    count = counters[key]\nelse:\n    count = 0\n\ncounters[key] = count + 1\n\nAnother way to accomplish the same behavior is by relying on how dictionaries raise a KeyError exception when you try to get the value for a key that doesn’t exist. This approach is more efficient because it requires only one access and one assignment:\n\ntry:\n    count = counters[key]\nexcept KeyError:\n    count = 0\n\ncounters[key] = count + 1\n\nThis flow of fetching a key that exists or returning a default value is so common that the dict built-in type provides the get method to accomplish this task. The second parameter to get is the default value to return in the case that the key—the first parameter—isn’t present. This also requires only one access and one assignment, but it’s much shorter than the KeyError example:\n\ncount = counters.get(key, 0)\ncounters[key] = count + 1\n\nThus, for a dictionary with simple types, using the get method is the shortest and clearest option.\n\nNote: ff you’re maintaining dictionaries of counters like this, it’s worth considering the Counter class from the collections built-in module, which provides most of the facilities you are likely to need.\n\nWhat if the values of the dictionary are a more complex type, like a list? For example, say that instead of only counting votes, I also want to know who voted for each type of bread. Here, I do this by associating a list of names with each key:\n\nvotes = {\n    \"baguette\": [\"Bob\", \"Alice\"],\n    \"ciabatta\": [\"Coco\", \"Deb\"],\n}\nkey = \"brioche\"\nwho = \"Elmer\"\n\nSimilarly, you can use the get method to fetch a list value when the key is present, or do one fetch and one assignment if the key isn’t present:\n\nnames = votes.get(key)\nif names is None:\n    votes[key] = names = []\nnames.append(who)\nprint(votes)\n\n{'baguette': ['Bob', 'Alice'], 'ciabatta': ['Coco', 'Deb'], 'brioche': ['Elmer']}\n\n\nThe approach that involves using get to fetch list values can further be shortened by one line if you use an assignment expression with the walrus operator which was introduced in Python 3.8. Read more about that on Real Python. For now, we will just leave you with the example:\n\n# note: this will throw error on Google Colab, which is Python 3.6\nif (names := votes.get(key)) is None:\n    votes[key] = names = []\nnames.append(who)\nprint(votes)\n\n{'baguette': ['Bob', 'Alice'], 'ciabatta': ['Coco', 'Deb'], 'brioche': ['Elmer', 'Elmer']}"
  },
  {
    "objectID": "python/effective-python-and-idiomatic-pandas.html#functions",
    "href": "python/effective-python-and-idiomatic-pandas.html#functions",
    "title": "Effective Python and idiomatic pandas",
    "section": "Functions",
    "text": "Functions\n ### #11. Never unpack more than three variables when functions return multiple values (EP #19)\nOne effect of the unpacking syntax (see item 4) is that it allows Python functions to seemingly return more than one value. For example, say that I’m trying to determine various statistics for a population of alligators. Given a list of lengths, I need to calculate the minimum and maximum lengths in the population. Here, I do this in a single function that appears to return two values:\n\ndef get_stats(numbers):\n    minimum = min(numbers)\n    maximum = max(numbers)\n    return minimum, maximum\n\n\nlengths = [63, 73, 72, 60, 67, 66, 71, 61, 72, 70]\nminimum, maximum = get_stats(lengths)  # Two return values\nprint(f\"Min: {minimum}, Max: {maximum}\")\n\nMin: 60, Max: 73\n\n\nThe way this works is that multiple values are returned together in a two-item tuple. The calling code then unpacks the returned tuple by assigning two variables. Here, I use an even simpler example to show how an unpacking statement and multiple-return function work the same way:\n\nfirst, second = 1, 2\nassert first == 1\nassert second == 2\n\n\ndef my_function():\n    return [1, 2]\n\n\nfirst, second = my_function()\nassert first == 1\nassert second == 2\n\n\n_, second = my_function()\n\nMultiple return values can also be received by starred expressions for catch-all unpacking. For example, say I need another function that calculates how big each alligator is relative to the population average. This function returns a list of ratios, but I can receive the longest and shortest items individually by using a starred expression for the middle portion of the list:\n\ndef get_avg_ratio(numbers):\n    average = sum(numbers) / len(numbers)\n    scaled = [x / average for x in numbers]\n    scaled.sort(reverse=True)\n    return scaled\n\n\nlongest, *middle, shortest = get_avg_ratio(lengths)\nprint(f\"Longest: {longest:&gt;4.0%}\")\nprint(f\"Shortest: {shortest:&gt;4.0%}\")\n\nLongest: 108%\nShortest:  89%\n\n\nNow, imagine that the program’s requirements change, and I need to also determine the average length, median length, and total population size of the alligators. I can do this by expanding the get_stats function to also calculate these statistics and return them in the result tuple that is unpacked by the caller:\n\ndef get_stats(numbers):\n    minimum = min(numbers)\n    maximum = max(numbers)\n    count = len(numbers)\n    average = sum(numbers) / count\n\n    sorted_numbers = sorted(numbers)\n    middle = count // 2\n    if count % 2 == 0:\n        lower = sorted_numbers[middle - 1]\n        upper = sorted_numbers[middle]\n        median = (lower + upper) / 2\n    else:\n        median = sorted_numbers[middle]\n\n    return minimum, maximum, average, median, count\n\n\nminimum, maximum, average, median, count = get_stats(lengths)\n\nprint(f\"Min: {minimum}, Max: {maximum}\")\nprint(f\"Average: {average}, Median: {median}, Count {count}\")\n\nassert minimum == 60\nassert maximum == 73\nassert average == 67.5\nassert median == 68.5\nassert count == 10\n\nMin: 60, Max: 73\nAverage: 67.5, Median: 68.5, Count 10\n\n\nThere are two problems with this code. First, all the return values are numeric, so it is all too easy to reorder them accidentally (e.g., swapping average and median), which can cause bugs that are hard to spot later. Using a large number of return values is extremely error prone.\nSecond, the line that calls the function and unpacks the values is long, and it likely will need to be wrapped in one of a variety of ways, which hurts readability.\nTo avoid these problems, you should never use more than three variables when unpacking the multiple return values from a function. These could be individual values from a three-tuple, two variables and one catch-all starred expression, or anything shorter. If you need to unpack more return values than that, you’re better off defining a dataclass or namedtuple and having your function return an instance of that instead.\n\n#12. Use None and docstrings to specify dynamic default arguments (EP #24)\nSometimes you need to use a non-static type as a keyword argument’s default value. For example, say that I want to load a value encoded as JSON data; if decoding the data fails, I want an empty dictionary to be returned by default:\n\nimport json\n\n\ndef decode(data, default={}):\n    try:\n        return json.loads(data)\n    except ValueError:\n        return default\n\nThe problem here is that dictionary specified for default will be shared by all calls to decode because default argument values are evaluated only once (at module load time). This can cause extremely surprising behavior:\n\nfoo = decode(\"bad data\")\nfoo[\"stuff\"] = 5\nbar = decode(\"also bad\")\nbar[\"meep\"] = 1\nprint(\"Foo:\", foo)\nprint(\"Bar:\", bar)\n\nFoo: {'stuff': 5, 'meep': 1}\nBar: {'stuff': 5, 'meep': 1}\n\n\nYou might expect two different dictionaries, each with a single key and value. But modifying one seems to also modify the other. The culprit is that foo and bar are both equal to the default parameter. They are the same dictionary object:\n\nfoo is bar\n\nTrue\n\n\nThe fix is to set the keyword argument default value to None and then document the behavior in the function’s docstring:\n\ndef decode(data, default=None):\n    \"\"\"Load JSON data from a string.\n\n    Args:\n         data: JSON data to decode.\n         default: Value to return if decoding fails.\n             Defaults to an empty dictionary.\n    \"\"\"\n    try:\n        return json.loads(data)\n    except ValueError:\n        if default is None:\n            default = {}\n    return default\n\nNow, running the same test code as before produces the expected result:\n\nfoo = decode(\"bad data\")\nfoo[\"stuff\"] = 5\nbar = decode(\"also bad\")\nbar[\"meep\"] = 1\nprint(\"Foo:\", foo)\nprint(\"Bar:\", bar)\n\nFoo: {'stuff': 5}\nBar: {'meep': 1}\n\n\n\nfoo is not bar\n\nTrue\n\n\nSo remember: - A default argument value is evaluated only once: during function definition at module load time. This can cause odd behaviors for dynamic values (like {}, [], or datetime.now()). - Use None as the default value for any keyword argument that has a dynamic value. Document the actual default behavior in the function’s docstring."
  },
  {
    "objectID": "python/effective-python-and-idiomatic-pandas.html#comprehensions",
    "href": "python/effective-python-and-idiomatic-pandas.html#comprehensions",
    "title": "Effective Python and idiomatic pandas",
    "section": "Comprehensions",
    "text": "Comprehensions\n\n#13. Use comprehensions instead of map and filter (EP #27)\nPython provides compact syntax for deriving a new list from another sequence or iterable. These expressions are called list comprehensions. For example, say that I want to compute the square of each number in a list. Here, I do this by using a simple for loop:\n\na = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nsquares = []\nfor x in a:\n    squares.append(x ** 2)\nprint(squares)\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\nWith a list comprehension, I can achieve the same outcome by specifying the expression for my computation along with the input sequence to loop over:\n\nsquares = [x ** 2 for x in a]  # List comprehension\nprint(squares)\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\nUnless you’re applying a single-argument function, list comprehensions are also clearer than the map built-in function for simple cases. map requires the creation of a lambda function for the computation, which is visually noisy:\n\nalt = map(lambda x: x ** 2, a)\n\nUnlike map, list comprehensions let you easily filter items from the input list, removing corresponding outputs from the result. For example, say I want to compute the squares of the numbers that are divisible by 2. Here, I do this by adding a conditional expression to the list comprehension after the loop:\n\neven_squares = [x ** 2 for x in a if x % 2 == 0]\nprint(even_squares)\n\n[4, 16, 36, 64, 100]\n\n\nThe filter built-in function can be used along with map to achieve the same outcome, but it is much harder to read:\n\nalt = map(lambda x: x ** 2, filter(lambda x: x % 2 == 0, a))\nassert even_squares == list(alt)\n\nDictionaries and sets have their own equivalents of list comprehensions (called dictionary comprehensions and set comprehensions, respectively). These make it easy to create other types of derivative data structures when writing algorithms:\n\n# note both dictionairy and set comprehensions use {} which may be confusing at first\neven_squares_dict = {x: x ** 2 for x in a if x % 2 == 0}\nthrees_cubed_set = {x ** 3 for x in a if x % 3 == 0}\nprint(even_squares_dict)\nprint(threes_cubed_set)\n\n{2: 4, 4: 16, 6: 36, 8: 64, 10: 100}\n{216, 729, 27}\n\n\nAchieving the same outcome is possible with map and filter if you wrap each call with a corresponding constructor. These statements get so long that you have to break them up across multiple lines, which is even noisier and should be avoided:\n\nalt_dict = dict(map(lambda x: (x, x ** 2), filter(lambda x: x % 2 == 0, a)))\nalt_set = set(map(lambda x: x ** 3, filter(lambda x: x % 3 == 0, a)))\n\n\n\n#14. Avoid more than two control subexpressions in comprehensions (EP #28)\nBeyond basic usage, comprehensions support multiple levels of looping. For example, say that I want to simplify a matrix (a list containing other list instances) into one flat list of all cells. Here, I do this with a list comprehension by including two for subexpressions. These subexpressions run in the order provided, from left to right:\n\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nflat = [x for row in matrix for x in row]\nprint(flat)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nThis example is simple, readable, and a reasonable usage of multiple loops in a comprehension. Another reasonable usage of multiple loops involves replicating the two-level-deep layout of the input list. For example, say that I want to square the value in each cell of a twodimensional matrix. This comprehension is noisier because of the extra [] characters, but it’s still relatively easy to read:\n\nsquared = [[x ** 2 for x in row] for row in matrix]\nprint(squared)\n\n[[1, 4, 9], [16, 25, 36], [49, 64, 81]]\n\n\nIf this comprehension included another loop, it would get so long that I’d have to split it over multiple lines:\n\nmy_lists = [\n    [[1, 2, 3], [4, 5, 6]],\n]\nflat = [x for sublist1 in my_lists for sublist2 in sublist1 for x in sublist2]\n\nAt this point, the multiline comprehension isn’t much shorter than the alternative. Here, I produce the same result using normal loop statements. The indentation of this version makes the looping clearer than the three-level-list comprehension:\n\nflat = []\nfor sublist1 in my_lists:\n    for sublist2 in sublist1:\n        flat.extend(sublist2)\n\nComprehensions support multiple if conditions. Multiple conditions at the same loop level have an implicit and expression. For example, say that I want to filter a list of numbers to only even values greater than 4. These two list comprehensions are equivalent:\n\na = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nb = [x for x in a if x &gt; 4 if x % 2 == 0]\nc = [x for x in a if x &gt; 4 and x % 2 == 0]\n\nConditions can be specified at each level of looping after the for subexpression. For example, say I want to filter a matrix so the only cells remaining are those divisible by 3 in rows that sum to 10 or higher. Expressing this with a list comprehension does not require a lot of code, but it is extremely difficult to read:\n\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nfiltered = [[x for x in row if x % 3 == 0] for row in matrix if sum(row) &gt;= 10]\nprint(filtered)\n\n[[6], [9]]\n\n\nAlthough this example is a bit convoluted, in practice you’ll see situations arise where such comprehensions seem like a good fit. I strongly encourage you to avoid using list, dict, or set comprehensions that look like this. The resulting code is very difficult for new readers to understand. The potential for confusion is even worse for dict comprehensions since they already need an extra parameter to represent both the key and the value for each item.\nThe rule of thumb is to avoid using more than two control subexpressions in a comprehension. This could be two conditions, two loops, or one condition and one loop. As soon as it gets more complicated than that, you should use normal if and for statements and write a helper function."
  },
  {
    "objectID": "python/effective-python-and-idiomatic-pandas.html#pandas",
    "href": "python/effective-python-and-idiomatic-pandas.html#pandas",
    "title": "Effective Python and idiomatic pandas",
    "section": "pandas",
    "text": "pandas\n\n#15. Be aware of memory consumption and downcast where necessary\nWe already covered this in our discussion of the Ames Housing case. We will only include the code for reference how to reduce your memory footprint of dataframes.\n\ndf = pd.read_csv(\n    \"https://github.com/eaisi/discover-projects/blob/main/ames-housing/AmesHousing.csv?raw=true\"\n)\ndf.memory_usage().sum()\n\n1922208\n\n\n\n# objects to categorical\ndf[df.select_dtypes(include=\"object\").columns] = df.select_dtypes(\n    include=\"object\"\n).astype(\"category\")\n\n# convert integers to smallest unsigned integer and floats to smallest\nfor old, new in [(\"integer\", \"unsigned\"), (\"float\", \"float\")]:\n    for col in df.select_dtypes(include=old).columns:\n        df[col] = pd.to_numeric(df[col], downcast=new)\n\ndf.memory_usage().sum()\n\n419054\n\n\n\n\n#16. Use .loc and .iloc for indexing dataframes (and forget about .ix)\nOne-third of the top-15 voted pandas questions on Stackoverflow are about indexing. Another one-third are about slicing. This seems as good a place as any to start.\nBy indexing, we mean the selection of subsets of a DataFrame or Series. DataFrames (and to a lesser extent, Series) provide a difficult set of challenges:\n\nLike lists, you can index by location.\nLike dictionaries, you can index by label.\nLike NumPy arrays, you can index by boolean masks.\nAny of these indexers could be scalar indexes, or they could be arrays, or they could be slices.\nAny of these should work on the index (row labels) or columns of a DataFrame.\nAnd any of these should work on hierarchical indexes.\n\nThe complexity of pandas’ indexing is a microcosm for the complexity of the pandas API in general. There’s a reason for the complexity (well, most of it), but that’s not much consolation while you’re learning. Still, all of these ways of indexing really are useful enough to justify their inclusion in the library.\nSince pandas 0.12, these tasks have been cleanly separated into two methods:\n\n.loc for label-based indexing\n.iloc for positional indexing\n\nPreviously, .ix was used to handle both cases. You may encounter this in old blog-posts. .ix is deprecated, so stop using it.\nFor more details, please read any (or all) of the following online tutorials: - Using Pandas and Python To Explore Your Dataset on Real Python; - Data Indexing and Slicing notebok by Jake VanderPlas - pandas documentation on indexing and selecting data\n\n\n#17. Use method chaining to make your data preparation more readable\nMethod chaining, where you call methods on an object one after another, is in vogue at the moment. It’s always been a style of programming that’s been possible with pandas:\n\nassign (0.16.0): For adding new columns to a DataFrame in a chain (inspired by dplyr’s mutate)\npipe (0.16.2): For including user-defined methods in method chains.\nrename (0.18.0): For altering axis names (in additional to changing the actual labels as before).\nWindow methods (0.18): Took the top-level pd.rolling_* and pd.expanding_* functions and made them NDFrame methods with a groupby-like API.\nResample (0.18.0) Added a new groupby-like API\nselection by callable (0.18.1): You can pass a callable to the indexing methods, to be evaluated within the DataFrame’s context (like .query, but with code instead of strings).\n\nTo illustrate the potential of method chaining, let’s look at this example with the Titanic dataset which explores the data of passengers that have embarked in Southampton:\n\nimport sys\nimport pandas as pd\n\n\nTITANIC_DATA = \"https://github.com/BindiChen/machine-learning/blob/master/data-analysis/007-method-chaining/data/train.csv?raw=true\"\nbins = [0, 13, 19, 61, sys.maxsize]\nlabels = [\"&lt;12\", \"Teen\", \"Adult\", \"Older\"]\n\npclass_age_map = {\n    1: 37,\n    2: 29,\n    3: 24,\n}\n\n\ndef replace_age_na(x_df, fill_map):\n    cond = x_df[\"Age\"].isna()\n    res = x_df.loc[cond, \"Pclass\"].map(fill_map)\n    x_df.loc[cond, \"Age\"] = res\n    return x_df\n\n\nview_southampton = (\n    pd.read_csv(TITANIC_DATA)\n    .pipe(replace_age_na, pclass_age_map)\n    .query('Embarked == \"S\"')\n    .assign(ageGroup=lambda df: pd.cut(df[\"Age\"], bins=bins, labels=labels))\n    .pivot_table(values=\"Survived\", columns=\"Pclass\", index=\"ageGroup\", aggfunc=\"mean\")\n    .rename_axis(\"\", axis=\"columns\")\n    .rename(\"Class {}\".format, axis=\"columns\")\n    .style.format(\"{:.2%}\")\n)\n\nview_southampton\n\n\n\n\n\n\n\nClass 1\nClass 2\nClass 3\n\n\nageGroup\n \n \n \n\n\n\n\n&lt;12\n75.00%\n100.00%\n37.14%\n\n\nTeen\n80.00%\n40.00%\n17.78%\n\n\nAdult\n58.10%\n40.77%\n16.61%\n\n\nOlder\n25.00%\n33.33%\n50.00%\n\n\n\n\n\n\n_string = 'sdfjghsdfgjk'\n\ndef clean_string(str):\n    return _string.upper().replace('S', 'Z')\n\n\nlambda s: s.upper().replace('S', 'Z')\n\n&lt;function __main__.&lt;lambda&gt;(s)&gt;\n\n\nRead more on method chaining in these tutorials:\n\nBest practice method chaining by B. Chen (with the Titanic example).\nTom Augspurger’s explanation on method chaining goes a bit more in depth on the reasoning and rationale behind it.\n\n\n\n#18. Don’t use inplace=True operations on dataframes\nMost pandas methods have an inplace keyword that’s False by default. In general, you shouldn’t do inplace operations.\nFirst, if you like method chains then you simply can’t use inplace since the return value is None, terminating the chain.\nSecond, I suspect people have a mental model of inplace operations happening, you know, inplace. That is, extra memory doesn’t need to be allocated for the result. But that might not actually be true. Quoting Jeff Reback from that answer\n\nTheir is no guarantee that an inplace operation is actually faster. Often they are actually the same operation that works on a copy, but the top-level reference is reassigned.\n\nThat is, the pandas code might look something like this\ndef dataframe_method(self, inplace=False):\n    data = self.copy()  # regardless of inplace\n    result = ...\n    if inplace:\n        self._update_inplace(data)\n    else:\n        return result\nThere’s a lot of defensive copying in pandas. Part of this comes down to pandas being built on top of NumPy, and not having full control over how memory is handled and shared. Without the copy, adding the columns would modify the input DataFrame, which just isn’t polite.\nFinally, inplace operations don’t make sense in projects like ibis or dask, where you’re manipulating expressions or building up a DAG of tasks to be executed, rather than manipulating the data directly.\n\n\n#19. Use .query and eval for fast, complex indexing\nThe power of the PyData stack is built upon the ability of NumPy and pandas to push basic operations into C via an intuitive syntax: examples are vectorized/broadcasted operations in NumPy, and grouping-type operations in pandas. While these abstractions are efficient and effective for many common use cases, they often rely on the creation of temporary intermediate objects, which can cause undue overhead in computational time and memory use.\nAs of version 0.13 (released January 2014), pandas includes some experimental tools that allow you to directly access C-speed operations without costly allocation of intermediate arrays. These are the eval() and query() functions, which rely on the Numexpr package.\nPlease refer to the following notebooks and documentation on how and when to use them: - High-Performance Pandas: eval() and query() by Jake VanderPlas - pandas user guide on the query() method"
  },
  {
    "objectID": "altair/comet-chart.html",
    "href": "altair/comet-chart.html",
    "title": "Comet charts in Python",
    "section": "",
    "text": "Zan Armstrong’s comet chart has been on my list of hobby projects for a while now. I think it is an elegant solution to visualize statistical mix effects and address Simpson’s paradox, and particularly useful when working with longitudinal data involving different sub-populations. Recently I found a good excuse to spend some time to actually use it as part of a exploratory data analysis on a project.\nSince I mostly work in Python and have recently fallen in love with Altair — for the same reasons as Fernando explains here — I wondered how the comet chart could be implemented using the grammar of interactive graphics. It took me a while to figure out how to actually plot the comets. In a previous version, I had drawn glyphs using Bokeh. While Altair allows you to plot any SVG path in a graph, this felt a bit hacky and not quite in line with the philosophy of using a grammar of graphics.\nThankfully Mattijn was quick to suggest using trail-marks, after which it was almost as easy as pie. So here’s an example using a dataset of 20,000 flights for 59 destination airports.\n\nimport altair as alt\nimport pandas as pd\nimport vega_datasets\n\n\n# Use airline data to assess statistical mix effects of delays\nflights = vega_datasets.data.flights_20k()\naggregation = dict(\n    number_of_flights=(\"destination\", \"count\"),\n    mean_delay=(\"delay\", \"mean\"),\n    mean_distance=(\"distance\", \"mean\"),\n)\n\n# Compare delays by destination between month 1 and 3\ngrouped = flights.groupby(by=[flights.destination, flights.date.dt.month])\ndf = (\n    grouped.agg(**aggregation)\n    .loc[(slice(None), [1, 3]), :]\n    .assign(\n        change_mean_delay=lambda df:\n            df.groupby(\"destination\")[\"mean_delay\"].diff(),\n    )\n    .fillna(method=\"bfill\")\n    .reset_index()\n    .round(2)\n)\n\n# Calculate weigthed average of delays for month 1 and 3\ntotal = (\n    flights.groupby(flights.date.dt.month)\n    .agg(**aggregation)\n    .loc[[1, 3], :]\n    .assign(\n        change_mean_delay=lambda df: df.mean_delay.diff(),\n        destination='TOTAL'\n    )\n    .fillna(method=\"bfill\")\n    .round(2)\n    .reset_index()\n    .loc[:, df.columns]\n)\n\n\ndef comet_chart(df, stroke=\"white\"):\n    return (\n    alt.Chart(df, width=600, height=450)\n    .mark_trail(stroke=stroke)\n    .encode(\n        x=alt.X(\"number_of_flights\", scale=alt.Scale(type=\"log\")),\n        y=alt.Y(\"mean_delay\"),\n        detail=\"destination\",\n        size=alt.Size(\"date\", scale=alt.Scale(range=[0, 10]), legend=None),\n        tooltip=[\n            \"destination\",\n            \"number_of_flights\",\n            \"mean_delay\",\n            \"change_mean_delay\",\n            \"mean_distance\",\n        ],\n        # trails don't support continuous color\n        # see https://github.com/vega/vega/issues/1187\n        # hence use bins\n        color=alt.Color(\n            \"change_mean_delay:Q\",\n            bin=alt.Bin(step=2),\n            scale=alt.Scale(scheme=\"blueorange\"),\n            legend=alt.Legend(orient=\"top\"),\n        ),\n    )\n)\n\n\ncomet_chart(df) + comet_chart(total, stroke=\"black\")\n\nIn the example shown here, each comet represents one destination airport. The head of the comet corresponds to the most recent observation of the number of flight arrivals (x-axis, shown as logarithmic scale to accommodate the wide range of observations) against the mean delay of those flights (y-axis). The tail of the comet represents a similar (x,y) datum, but from an earlier point in time. Finally, the colour of the comet is encoded to show the change in the mean delay for each airport. A tooltip with a summary of the data is shown when hovering over the head of the comet.\nSo-called mix effects can often lead to misinterpretation of aggregate numbers. In the example of flight delays, the fact that only a small change is observed in the mean delay across all airports — visualized with the right-most comet outlined in black — hides the underlying variance between airports. Note that in this example the size of each sub-population (number of flights per airport) remains relatively constant, hence the comets here only go up and down. As explained in the original article, mix effects become harder to interpret when the relative size of the sub-populations change as well as their relative values. In the most extreme case this may lead to Simpson’s paradox.\nWith this base implementation of comet charts in Altair, you can really go to town and combine it with other interactive graphs. Using the overview-detail pattern, you could plot an accompanying density plot of all the flights for a given airport. That way you can quickly zoom in to the lowest level of detail and get a better understanding of the underlying mix effects."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Anthology of Data Science",
    "section": "",
    "text": "An Introduction to Statistical Learning\n\n\nStatistical learning has become a critical toolkit for anyone who wishes to understand data. This book provides a broad and less technical treatment of key topics in…\n\n\n\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Jonathan Taylor\n\n\nJul 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Principles & Practice\n\n\nThis textbook is provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to be able to use them sensibly.\n\n\n\nRob J Hyndman, George Athanasopoulos\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretable Machine Learning\n\n\nThis book is about making machine learning models and their decisions interpretable. It will enable you to select and correctly apply the interpretation method that is most…\n\n\n\nChristoph Molnar\n\n\nMar 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython of Data Analysis, Third Edition\n\n\nThis book is concerned with the nuts and bolts of manipulating, processing, cleaning, and crunching data in Python.\n\n\n\nWes McKinney\n\n\nApr 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR for Data Science, Second Edition\n\n\nThis book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it and visualize.\n\n\n\nHadley Wickham, Mine Çetinkaya-Rundel, Garrett Grolemenund\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Turing Way\n\n\nAn open source, open collaboration, community-driven handbook to reproducible, ethical and collaborative data science.\n\n\n\nThe Turing Way Community\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\nThis book explains the ideas that underlie deep learning, distinguishing it from volumes that cover coding and other practical aspects.\n\n\n\nSimon Prince\n\n\nOct 23, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "books/udl.html",
    "href": "books/udl.html",
    "title": "Understanding Deep Learning",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "books/islp.html",
    "href": "books/islp.html",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An anthology of open access data science materials",
    "section": "",
    "text": "Machine learning is permeating many fields of work. As a new ‘system technology’1, its impact on organizations and society is expected to be of the same magnitude as that of the steam engine or electricity. As such, more and more professionals are seeking to acquire the necessary understanding and skills to apply machine learning in their day-to-day work. Hence more people without a background in either computer science or statistics - let alone both - have a need for high-quality, open access content to explore and learn data science by themselves.\nNow there is a lot of machine learning learning materials out there, so why this anthology? Based on my experience in teaching professional education course on data & AI, I am continouly challenged to:\n\ncurate content for different professional learning paths, combining various existing open access materials that can be readily shared and thus contribute to the democratization of know-how in this field of work;\nfinding a balance between too technical vs. too vague, handwaving or even downright wrong;\ntake a hands-on, problem-based approach. Rather than, say, explaining the principles that underlie regularization, we choose to demonstrate these principles using the simplest algorithms. With a little math, everyone should be able to understand how LASSO performs regularisation for regression models. With this intuitive understanding, you can move on to more complex algorithms and applications, and reason where and how to use regularisation.\n\nThis anthology was born out of a need to scratch my own itch, namely have a central repository where I can continously collate interesting resources and keep the content up-to-date as the best practices evolve. I hope it is also helpful those seeking a stepping stone into the wonderful field of data science. Feel free to drop me a note with suggestions or feedback by reporting an issue."
  },
  {
    "objectID": "index.html#why-this-anthology",
    "href": "index.html#why-this-anthology",
    "title": "An anthology of open access data science materials",
    "section": "",
    "text": "Machine learning is permeating many fields of work. As a new ‘system technology’1, its impact on organizations and society is expected to be of the same magnitude as that of the steam engine or electricity. As such, more and more professionals are seeking to acquire the necessary understanding and skills to apply machine learning in their day-to-day work. Hence more people without a background in either computer science or statistics - let alone both - have a need for high-quality, open access content to explore and learn data science by themselves.\nNow there is a lot of machine learning learning materials out there, so why this anthology? Based on my experience in teaching professional education course on data & AI, I am continouly challenged to:\n\ncurate content for different professional learning paths, combining various existing open access materials that can be readily shared and thus contribute to the democratization of know-how in this field of work;\nfinding a balance between too technical vs. too vague, handwaving or even downright wrong;\ntake a hands-on, problem-based approach. Rather than, say, explaining the principles that underlie regularization, we choose to demonstrate these principles using the simplest algorithms. With a little math, everyone should be able to understand how LASSO performs regularisation for regression models. With this intuitive understanding, you can move on to more complex algorithms and applications, and reason where and how to use regularisation.\n\nThis anthology was born out of a need to scratch my own itch, namely have a central repository where I can continously collate interesting resources and keep the content up-to-date as the best practices evolve. I hope it is also helpful those seeking a stepping stone into the wonderful field of data science. Feel free to drop me a note with suggestions or feedback by reporting an issue."
  },
  {
    "objectID": "index.html#how-is-this-anthology-set-up",
    "href": "index.html#how-is-this-anthology-set-up",
    "title": "An anthology of open access data science materials",
    "section": "How is this anthology set up?",
    "text": "How is this anthology set up?\n\nA selection of the best books\nA selection of what I think are the best textbooks is provided as a digital bookshelf. All the books included are open access, thanks to their respective authors! They should keep you busy for a while.\n\n\nVisualization with Altair\nI am a big fan of the Vega-Altair ecosystem for data visualization, because it not only helps me in creating appealing, interactive visualizations, but it also helps me to reason about the data when I am doing exploratory data analysis. A selection of tutorials, blogpost is provided in the section Visualization with Altair.\n\n\nSelected articles\nVarious topics and perspectives data science. Clearly this is an opinionated and very personal selection.\n\n\nNotebooks\nNotebooks for those seeking hands-on examples and explainers."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "An anthology of open access data science materials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSheikh et al. (2023), Mission AI: the New System Technology, https://doi.org/10.1007/978-3-031-21448-6↩︎"
  },
  {
    "objectID": "altair/index.html",
    "href": "altair/index.html",
    "title": "Data visualization with Vega-Altair",
    "section": "",
    "text": "A data visualization curriculum of interactive notebooks, using Vega-Lite and Altair.\n\n\n\nJeffrey Heer, Dominik Moritz, Jake VanderPlas, Brock Craft\n\n\nMar 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing statistical mix effects and Simpson’s paradox with Altair\n\n\n\nDaniel Kapitan\n\n\nJan 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding your data is critical in creating visualizations. This video outlines Altair’s data types and explains how they can influence the visualization process.\n\n\n\nEitan Lees\n\n\nAug 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis video outlines the visualization grammar Altair is built on. Understanding the ways in which the elements of the visualization grammar interact is important when using…\n\n\n\nEitan Lees\n\n\nAug 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis video describes the Python package Altair and the software stack it is built on.\n\n\n\nEitan Lees\n\n\nAug 4, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "altair/index.html#section",
    "href": "altair/index.html#section",
    "title": "Data visualization with Vega-Altair",
    "section": "",
    "text": "A data visualization curriculum of interactive notebooks, using Vega-Lite and Altair.\n\n\n\nJeffrey Heer, Dominik Moritz, Jake VanderPlas, Brock Craft\n\n\nMar 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing statistical mix effects and Simpson’s paradox with Altair\n\n\n\nDaniel Kapitan\n\n\nJan 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding your data is critical in creating visualizations. This video outlines Altair’s data types and explains how they can influence the visualization process.\n\n\n\nEitan Lees\n\n\nAug 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis video outlines the visualization grammar Altair is built on. Understanding the ways in which the elements of the visualization grammar interact is important when using…\n\n\n\nEitan Lees\n\n\nAug 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis video describes the Python package Altair and the software stack it is built on.\n\n\n\nEitan Lees\n\n\nAug 4, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "python/whirlwind.html",
    "href": "python/whirlwind.html",
    "title": "A Whirlwind Tour of Python",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "python/realpython.html",
    "href": "python/realpython.html",
    "title": "Getting into Python with RealPython.com",
    "section": "",
    "text": "The table below lists the topics that you should have covered _as a bare minimum. You often have a choice to suit your prefered learning style: courses are videos, tutorials are text. Note that the tutorials are free of charge, while the videos are only accessible with a paid subscription.\n\n\n\nchapter\ntopic\ncourse\ntutorial\n\n\n\n\n2. Setting up Python\nInstalling Python 3\nlink\nlink\n\n\n\nPython in VS Code\nlink\n\n\n\n3. Your First Python Program\nCode Your First Python Program\nlink\n\n\n\n\nBasic Data Types in Python\nlink\nlink\n\n\n\nVariables in Python\nlink\nlink\n\n\n4. Strings and String Methods\nStrings and Character Data in Python\nlink\nlink\n\n\n\nReading Input and Writing Output in Python\nlink\nlink\n\n\n5. Numbers and Math\nOperators and Expressions in Python\nlink\nlink\n\n\n\nNumbers in Python\n\nlink\n\n\n6. Functions and Loops\n“for” loops (Definite Iteration)\nlink\nlink\n\n\n\n“while” loops (Indefinite Iteration)\nlink\nlink\n\n\n\nDefining and Calling Python Functions\nlink\nlink\n\n\n8. Conditional Logic and Control Flow\nConditional Statements in Python (if/elis/else)\nlink\nlink\n\n\n9. Tuples, Lists and Dictinairies\nLists and Tuples in Python\nlink\nlink\n\n\n\nDictionairies in Python\nlink\nlink\n\n\n11. Modules and Packages\nPython Modules and Packages: An Introduction\nlink\nlink\n\n\n12. File Input and Output\nReading and Writing Files in Python\nlink"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Selected papers & blogpost",
    "section": "",
    "text": "Stop aggregating away the signal in your data\n\n\nBy aggregating our data in an effort to simplify it, we lose the signal and the context we need to make sense of what we’re seeing. Originally published on &lt;a…\n\n\n\nZan Armstrong\n\n\nMar 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRIP correlation. Introducing the Predictive Power Score.\n\n\nWe define the Predictive Power Score (PPS), an alternative to the correlation that finds more patterns in your data. Originally published on &lt;a…\n\n\n\nFlorian Wetschoreck\n\n\nApr 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nThe Precision-Recall Plot Is More Informative than the ROC Plot\n\n\nAn introduction to performance metrics for binary classification. Originally published on &lt;a…\n\n\n\nPaul van der Laken, Daniel Kapitan\n\n\nAug 16, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/stop-aggregating-signal.html",
    "href": "posts/stop-aggregating-signal.html",
    "title": "Stop aggregating away the signal in your data",
    "section": "",
    "text": "For five years as a data analyst, I forecasted and analyzed Google’s revenue. For six years as a data visualization specialist, I’ve helped clients and colleagues discover new features of the data they know best. Time and time again, I’ve found that by being more specific about what’s important to us and embracing the complexity in our data, we can discover new features in that data. These features can lead us to ask better data-driven questions that change how we analyze our data, the parameters we choose for our models, our scientific processes, or our business strategies.\nMy colleagues Ian Johnson, Mike Freeman, and I recently collaborated on a series of data-driven stories about electricity usage in Texas and California to illustrate best practices of Analyzing Time Series Data. We found ourselves repeatedly changing how we visualized the data to reveal the underlying signals, rather than treating those signals as noise by following the standard practice of aggregating the hourly data to days, weeks, or months.Behind many of the best practices we recommended for time series analysis was a deeper theme: actually embracing the complexity of the data.\nAggregation is the standard best practice for analyzing time series data, but it can create problems by stripping away crucial context so that you’re not even aware of how much potential insight you’ve lost. In this article, I’ll start by discussing how aggregation can be problematic, before walking through three specific alternatives to aggregation with before/after examples that illustrate:"
  },
  {
    "objectID": "posts/stop-aggregating-signal.html#what-is-the-problem-with-aggregation",
    "href": "posts/stop-aggregating-signal.html#what-is-the-problem-with-aggregation",
    "title": "Stop aggregating away the signal in your data",
    "section": "What is the problem with aggregation?",
    "text": "What is the problem with aggregation?\nWe praise the importance of large, rich datasets when we talk about algorithms and teaching machines to learn from data. However, too often when we visualize data so that we as humans can make sense of it, especially time series data, we make the data smaller and smaller.\nAggregation is the default for a reason. It can feel overwhelming to handle the quantities of data we now have at our fingertips. It doesn’t have to be very “big data” to have more than 1M data points, more than the number of pixels on a basic laptop screen. There are many robust statistical approaches to effective aggregation and aggregation that can provide valuable context (comparing to median, for example). In other cases, we need to see more details while trying to find the key insight, but once we’ve finished the analysis and know which features of the data matter most, then aggregation can be a useful tool for focusing attention to communicate that insight.\nBut every time you aggregate, you make a decision about which features of your data matter and which ones you are willing to drop: which are the signal and which are the noise. When you smooth out a line chart, are you doing it because you’ve decided that the daily average is most important and that you don’t care about the distribution or seasonal variation in your peak usage hours? Or are you doing it because it’s the only way you know how to make the jagged lines in your chart go away?\nInformed aggregation simplifies and prioritizes. Uninformed aggregation means you’ll never know what insights you lost.\nIn our rush to aggregate, we sometimes forget that the numbers are tied to real things. To people’s actions. To the hourly, daily, weekly, monthly, and seasonal patterns that are so familiar that they’re almost forgettable. Or maybe it’s that we so rarely see disaggregated data presented effectively in practice that we don’t even realize it’s an option. By considering these seasonal patterns, these human factors, we could embrace complexity in more meaningful ways.\nConsider how much energy we use. If we take a moment to think about it, it’s obvious that we use a lot more energy in the late afternoon than the early morning, so we’d expect big dips and troughs every day. It also shouldn’t be a surprise that the daily energy usage patterns on a summer day and a winter day are different. These patterns aren’t noise, but rather are critical to making sense of this data. We especially need this context to tell what is expected and what is noteworthy.\nHowever, when our dataset has big, regular fluctuations from day to day or hour to hour, our line charts end up looking like an overwhelming mess of jagged lines. Consider this chart showing the 8,760 data points representing one year of data on hourly energy use in California.\n\nA standard way to deal with this overwhelming chart is to apply a moving average by day, week, or month (defined as a four-week period).\n\nYes, now we have a simple chart, and can easily see that the lowest energy usage is in April, and the peak in late August. But we could see that in the first chart. Moreover, we’ve smoothed out anything else of interest. We’ve thrown away so much information that we don’t even know what we’ve lost.\nIs this annual pattern, with a dip in April and a peak in August, consistent for all hours of the day? Do some hours of day or days of week change more than others through the seasons? Were there any hours, days, or weeks that were unusual for their time of year/time of day? What are the outliers? Is energy usage equally variable through all times of year, or are some weeks/seasons/hours more consistent than others?\nDespite starting with data that should contain the answers to these questions, we can’t answer them. Moreover, the smoothed line doesn’t even give us any hints about what questions to ask or what’s worth digging into more."
  },
  {
    "objectID": "posts/stop-aggregating-signal.html#solution-embrace-the-complexity-by-rearranging-augmenting-and-using-the-data-itself-to-provide-context.",
    "href": "posts/stop-aggregating-signal.html#solution-embrace-the-complexity-by-rearranging-augmenting-and-using-the-data-itself-to-provide-context.",
    "title": "Stop aggregating away the signal in your data",
    "section": "Solution: Embrace the complexity by rearranging, augmenting, and using the data itself to provide context.",
    "text": "Solution: Embrace the complexity by rearranging, augmenting, and using the data itself to provide context.\n\n#1: Don’t aggregate: Rearrange\nWhat if we considered what categories likely matter based on what we know of human behavior and environmental factors, especially temperature? Factors like time of day and time of year? In Discovering Data Patterns, we grouped the data into 96 small, aligned tick charts, one for each hour of the day for each season of the year and organized the visualization around the concepts most likely to matter. The x-axis for each mini chart is the amount of electricity used, and each tick mark represents a single hour on a specific day.\n\nThis way we can immediately see what’s typical or unusual for each hour and quarter. For example, generally more energy is used at midnight in winter than at 3am. Skimming down a column, we can see the shape of a day for each season. And, we can see how energy demand by hour changes across seasons by comparing each column to the next.\nNow the “noise” has become the signal. We can clearly answer the questions we posed above:\n\nIs this annual pattern consistent for all hours of the day?\n\nNo, the “shape” of energy used during the course of the day is different in winter vs. summer, with a double peak in Q1 and a single peak in Q3. Also, Q4 looks a lot like Q1, except for a few unusual days. And Q2 shows the most variability in “shape” of day.\n\nDo some hours of day or days of week change more than other hours through the seasons?\n\nYes, late afternoon and evening hours show much more of an increase in energy usage from Q1 to Q3 than early morning hours.\n\nWere there any hours, days, or weeks that were unusual for their time of year/time of day?\n\nYes. For example, in Q4 some very unusual days saw high energy usage in the evening.\nYes. In Q3 in the early morning hours (between 4am and 6am), there were some outlier days with much higher energy usage.\n\nIs energy usage equally variable through all times of year, or are some weeks/seasons/hours more consistent than others?\n\nNo! Q1 has very more consistent energy usage, with a very narrow range of energy used for any particular hour of the day. Meanwhile, Q2 shows very inconsistent energy usage, with a lot of variability especially in the higher-energy evening hours.\n\n\nNot only do we notice some patterns immediately, but this view of the data also gives us the chance to go deeper just by looking more closely (and doing some basic research into what was going on in California at that time).\nLet’s look closer at the early morning hours of Q3. There were some abnormally high values between 4pm and 6pm. Interactive tooltips reveal that these took place on August 19. A quick Google search for “California Aug 19th 2020” shows that the region was suffering from wildfires, so perhaps people kept windows closed and the AC on instead of opening their windows to the cooler nighttime air. September 6 also shows up among the highest values, and a search indicates a likely cause: a record-breaking heat wave in California that hit the national news while the fires continued to burn.\n\nOverall, our faceted tick heatmap chart has the same number of data points as the original jagged line, but now we can see the underlying daily and seasonal patterns (and how daily patterns vary by season) as well as the relative outliers. The more time we spend with the chart, the more we notice, as it invites us to ask new data-driven questions.\n\n\n#2: Augment first, then group or color.\n\nBring in common knowledge: augment with familiar classifications\n\nAt another point in our exploratory analysis, we looked at a chart showing 52 weeks of hourly energy usage in California (shown above) and noticed that the higher-energy weeks seemed to have a single bump each day in the evening while lower-energy weeks seemed to have more of a double bump (see above). This is actually the same pattern revealed in section #1 on rearranging.\nWe guessed that the single bump/double hump might be related to seasonal differences in temperatures. To test this hypothesis we added a column to the dataset to designate “summer” vs. “winter,” and then made two charts (faceted) by splitting data on that parameter. Suddenly it was obvious. No longer were we squinting to notice a pattern hidden in the squiggles.\n\nThe “faceting” itself was simple, a feature built into many charting APIs including Observable Plot (which we were using to visualize our data). In hindsight, this seems like an obvious way to split the data. But how often do we step back and actually augment our data with these human-relevant concepts? The key was having the summer/winter parameter to facet on. It doesn’t have to be perfect. Guessing at a date boundary for summer/winter is enough to see that a distinct pattern emerges. Once we have the double-bump/single-bump insight visible here, we can use that insight to go back and look at our data more closely. For example, it appears that there are some daily “double-bump” weeks in the “summer.” Are those boundary weeks that should be classified as winter (or fall or spring)? Or are they unusual weeks during the summer? Moreover, now that we know a defining signal, we could use that signal to classify the data and thereby use the data to identify when energy usage transitions from a “summer” pattern to a “winter” pattern.\n\n\nAugment with data-driven classifications\nThis line chart shows the daily energy use by a single household in Atlanta from March through July 2021. What do you notice? Lots of spikes? Higher energy use in the summer months?\n\nSwitching to a scatterplot makes it more obvious that there are normal-energy days and also high-energy days. Drawing in a line for the moving average plus a (5kwh) buffermakes this split between “normal” and “high-energy” days more clear and shows that the gap is maintained even as energy use overall increases in the summer months.\n\nNow that our exploratory data analysis has revealed two distinct categories (normal and high-energy), we can augment our data by using the moving average to define which points fall into each category. We can then color the points by those new classifications to make it easier to analyze.\n\nIn this way, we complete the circle: we use visualization to notice a key feature of the data and leverage this insight to further classify our data, making the visualization easier to read. And we can take it a step further, continuing to analyze our data based on this classification by creating a histogram showing the frequency of high-energy vs. normal usage days by month. In this view, we can see that in the summer the amount of energy used on normal days went up, and that there were more high-energy days in June and July than in March and April (even after taking into account that the baseline energy usage also went up). Therefore, we can now say with confidence that overall energy consumption increased for two reasons: (1) baseline energy usage increased and (2) a higher percent of days were high-energy days.\n\nThis pattern of looking, then augmenting, then looking again using the classification can also reveal any issues with our classification, like the high point that occurred on our sixth day of data which is mislabeled because the moving average was not defined until the seventh day (as a trailing moving average). This gives us a chance to improve our classification algorithm.\nWhile this example used a very simple algorithm of “moving average + 5kwh” to classify days as “normal” or “high-energy,” this cycle of “look, augment, look, refine classification” becomes more important for machine learning as our algorithms become more opaque.\n\n\n\n#3: Split your data into foreground and background\n\nSplit based on a time period of interest\nWe also dug into data on energy generated by fuel type in Texas in January and February of 2021, including a critical time period in February leading up to and during the rolling blackouts that were initiated to save the electricity grid from collapse following an unusual winter storm. In the analysis story, my colleague Ian faceted the data, creating a chart for each fuel type. This was quite effective: you can immediately see which fuels make up the bulk of energy in Texas, as well as some of the abnormal patterns in mid-February.\n\nKnowing that the critical time period was around February 7 to February 21, Ian further focused attention on those two weeks by making the weeks before and after partially transparent and adding vertical gridlines. He might have been tempted to delete the data outside the period. After all, why waste space on data outside the time period of interest?\n\nBut it’s that granular background data that helps us understand what is so unusual for each fuel type during the critical time period. For example, in coal we’d notice the dip after February 15 regardless, but we need the data from January to notice how unusual the nearly flat plateau between February 1 and February 15 is. Similarly, the January and late-February data for nuclear shows how steady that fuel source typically is, helping us to understand just how strange the dip that we see after February 15 is.\n\n\n\nSplit by comparing each category of interest to the full dataset\nWhen we want to know if there is a relationship between metric A and metric B, the first step is to create a scatterplot. For example, the scatterplot below shows the outdoor temperature and energy demand in Texas for each hour over the course of a year. It’s immediately clear that there is a strong relationship between temperature and energy use (even though that relationship is also obviously non-linear!).\n\nWhile there is clearly a correlation between temperature and electricity demand, it’s also clear that temperature doesn’t tell the whole story. For any given temperature, there is a roughly 10-15K MWh difference from minimum to maximum energy use. Knowing that in our own homes we crank the thermostat a lot higher on a cold afternoon than on a cold night, we guessed that the hour of day could play a key role in the relationship between temperature and energy use.\nThe standard approach to adding an additional category to a scatterplot is to apply a categorical color, thereby comparing everything to everything (comparing all hours, temperatures, and energy demand in one chart). If we do that, we do see that something is going on. More greens and blues in the top right, more pinks low. But to understand what the colors refer to, you have to look back and forth between the legend and the data a lot. Moreover, it’s impossible to answer a question like, “What’s the relationship between temperature and energy at 10am?” Or, “How does early morning compare to evening?\n\nInstead we can apply two techniques: grouping and splitting the data into foreground and background.\nIn the three charts below, the dots representing 5am, 10am, and 6pm are colored brightly. Meanwhile, the entire dataset is shown in grey in the background. This gives us the context to see the relationship between temp and energy for each hour, and see that in the context of the full dataset.\nBy specifically comparing “5am” to “all other times of day,” we can see that 5am is relatively low energy use regardless of temperature (and temperatures are never very high at 5am). Meanwhile, at 6pm energy use is generally higher at all temperatures.\n10am is in some ways the most interesting: at lower temperatures (in the left half of the graph) the yellow dots are relatively high compared to the grey dots, indicating high energy use relative to other hours of the day at the same temperature. Meanwhile, for high temperatures on the right half of the graph, the yellow dots hug the bottom of the grey area. At hot temperatures, relatively little energy is used at 10am compared to the rest of the day. This type of insight is made possible not just by grouping, but also by using the full “noisy” dataset as a consistent background providing context for all the faceted charts."
  },
  {
    "objectID": "posts/stop-aggregating-signal.html#summary-embrace-the-complexity-of-your-data",
    "href": "posts/stop-aggregating-signal.html#summary-embrace-the-complexity-of-your-data",
    "title": "Stop aggregating away the signal in your data",
    "section": "Summary: Embrace the complexity of your data",
    "text": "Summary: Embrace the complexity of your data\nIn the course of creating the Analyzing Time Series Data collection, Ian Johnson, Mike Freeman, and I employed a range of strategies to embrace the complexity of the data instead of relying on standard methods that aggregate it away. Those frustratingly jagged lines are the signal, not the noise.\nWe embraced complexity by:\n\nRearranging data to compare “like to like.”\nAugmenting our data based on the concepts that we know matter and on what we discovered in the data.\nUsing the larger dataset to provide background context for the data of interest (the foreground).\n\nThese approaches are especially powerful for time series data because the underlying daily, weekly, and seasonal patterns can feel so distracting. In particular, consider how these strategies might power real-time data analysis by putting incoming data in a richer historical context for quick visual pattern-matching to identify normal vs. worrisome patterns. At the same time, these foundational techniques also apply to any data that can feel overwhelming and noisy, like machine learning classifications or data resulting from high-throughput scientific experiments.\nAfter seeing each of these techniques in action, perhaps the next time you are about to aggregate your data in order to simplify it, you might instead try to rearrange, augment, or split your data into foreground/background. See the data in its full context to reveal unexpected patterns and prompt new data-driven questions. Embrace complexity by (literally) changing how you look at your data."
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Example notebooks and explainers",
    "section": "",
    "text": "Predicting diabetes with Pima Indians\n\n\n\n\n\n\nDaniel Kapitan\n\n\nNov 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting house prices in Ames, Iowa\n\n\n\n\n\n\nDaniel Kapitan\n\n\nOct 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemonstration of overfitting and underfitting\n\n\nThis notebook illustrates why we require a train/test split (or cross-validation) to balance overfitting vs. underfitting.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeconfounding explained\n\n\nA demonstration howcorrelations ‘magically’ disappear if confounders are added to your model.\n\n\n\n\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#objectives",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#objectives",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Objectives",
    "text": "Objectives\n\nExample end-to-end supervised learning workflow with Pima Indians\nFocus on conceptual understanding of machine learning\nDemonstrate use of Predictive Power Score (PPS)\nDemonstrate capabilities of low-code tools\nDemonstrate use of average_precision_score (link)\nDemonstrate SHAP values"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#attribution",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#attribution",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Attribution",
    "text": "Attribution\n\nDataset\n\nPima Indians paper (original paper)\nKaggle datacard (link)\n\n\n\nPython libraries\n\nAltair (docs)\nydata-profiling (docs)\nPredictive Power Score (PPS, GitHub, blog)\nPyCaret: open-source, low-code machine learning library in Python that automates machine learning workflows (link)\n\n\nimport altair as alt\nimport pandas as pd\nimport ppscore as pps\nfrom pycaret.classification import *\nimport shap\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import train_test_split\nfrom ydata_profiling import ProfileReport\n\n\n# customize Altair\ndef y_axis():\n    return {\n        \"config\": {\n            \"axisX\": {\"grid\": False},\n            \"axisY\": {\n                \"domain\": False,\n                \"gridDash\": [2, 4],\n                \"tickSize\": 0,\n                \"titleAlign\": \"right\",\n                \"titleAngle\": 0,\n                \"titleX\": -5,\n                \"titleY\": -10,\n            },\n            \"view\": {\n                \"stroke\": \"transparent\",\n                # To keep the same height and width as the default theme:\n                \"continuousHeight\": 300,\n                \"continuousWidth\": 400,\n            },\n        }\n    }\n\n\nalt.themes.register(\"y_axis\", y_axis)\nalt.themes.enable(\"y_axis\");"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#read-and-explore-the-data",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#read-and-explore-the-data",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Read and explore the data",
    "text": "Read and explore the data\n\n%%time\ndf = pd.read_csv(\"diabetes.csv\").astype({\"Outcome\": bool})\ntrain, test = train_test_split(df, test_size=0.3)\nprofile = ProfileReport(train, minimal=True, title=\"Pima Indians Profiling Report\")\nprofile.to_file(\"pima-indians-profiling-report-minimal.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 5.54 s, sys: 184 ms, total: 5.72 s\nWall time: 1.7 s\n\n\n\nprofile.to_notebook_iframe()"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Investigate features with largest predictive power",
    "text": "Investigate features with largest predictive power\nWe use the Predictive Power Score to evaluate which features have the highest predictive power with respect to Outcome.\n\npredictors = (\n    pps.predictors(train, \"Outcome\")\n    .round(3)\n    .iloc[:, :-1]\n)\nbase = (\n    alt.Chart(predictors)\n    .encode(\n        y=alt.Y(\"x:N\").sort(\"-x\"),\n        x=\"ppscore\",\n        tooltip=[\"x\", \"ppscore\"],\n    )\n)\nbase.mark_bar() + base.mark_text(align=\"center\", dy=-5)"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-colinearity",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-colinearity",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Investigate colinearity",
    "text": "Investigate colinearity\n\npps.matrix(train)\n\n\n\n\n\n\n\n\nx\ny\nppscore\ncase\nis_valid_score\nmetric\nbaseline_score\nmodel_score\nmodel\n\n\n\n\n0\nPregnancies\nPregnancies\n1.000000\npredict_itself\nTrue\nNone\n0.000000\n1.000000\nNone\n\n\n1\nPregnancies\nGlucose\n0.000000\nregression\nTrue\nmean absolute error\n23.651769\n24.024994\nDecisionTreeRegressor()\n\n\n2\nPregnancies\nBloodPressure\n0.000000\nregression\nTrue\nmean absolute error\n12.748603\n12.961633\nDecisionTreeRegressor()\n\n\n3\nPregnancies\nSkinThickness\n0.000000\nregression\nTrue\nmean absolute error\n13.467412\n13.630828\nDecisionTreeRegressor()\n\n\n4\nPregnancies\nInsulin\n0.000000\nregression\nTrue\nmean absolute error\n79.240223\n85.015086\nDecisionTreeRegressor()\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n76\nOutcome\nInsulin\n0.000000\nregression\nTrue\nmean absolute error\n79.240223\n84.802415\nDecisionTreeRegressor()\n\n\n77\nOutcome\nBMI\n0.035133\nregression\nTrue\nmean absolute error\n5.672812\n5.473511\nDecisionTreeRegressor()\n\n\n78\nOutcome\nDiabetesPedigreeFunction\n0.000000\nregression\nTrue\nmean absolute error\n0.226384\n0.234531\nDecisionTreeRegressor()\n\n\n79\nOutcome\nAge\n0.000000\nregression\nTrue\nmean absolute error\n8.888268\n8.997094\nDecisionTreeRegressor()\n\n\n80\nOutcome\nOutcome\n1.000000\npredict_itself\nTrue\nNone\n0.000000\n1.000000\nNone\n\n\n\n\n81 rows × 9 columns\n\n\n\n\npps_matrix = (\n    pps.matrix(\n        train.loc[:, predictors.query(\"ppscore &gt; 0\")[\"x\"].tolist()],\n    )\n    .loc[:, [\"x\", \"y\", \"ppscore\"]]\n    .round(3)\n)\n(\n    alt.Chart(pps_matrix)\n    .mark_rect()\n    .encode(\n        x=\"x:O\",\n        y=\"y:O\",\n        color=\"ppscore:Q\",\n        tooltip=[\"x\", \"y\", \"ppscore\"])\n).properties(width=500, height=500)"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#build-models",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#build-models",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Build models",
    "text": "Build models\n\ncls = setup(data = train, \n             target = 'Outcome',\n             numeric_imputation = 'mean',\n             feature_selection = False,\n             pca=False,\n             remove_multicollinearity=True,\n             remove_outliers = False,\n             normalize = True,\n             )\nadd_metric('apc', 'APC', average_precision_score, target = 'pred_proba');\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n5752\n\n\n1\nTarget\nOutcome\n\n\n2\nTarget type\nBinary\n\n\n3\nOriginal data shape\n(537, 9)\n\n\n4\nTransformed data shape\n(537, 9)\n\n\n5\nTransformed train set shape\n(375, 9)\n\n\n6\nTransformed test set shape\n(162, 9)\n\n\n7\nNumeric features\n8\n\n\n8\nPreprocess\nTrue\n\n\n9\nImputation type\nsimple\n\n\n10\nNumeric imputation\nmean\n\n\n11\nCategorical imputation\nmode\n\n\n12\nRemove multicollinearity\nTrue\n\n\n13\nMulticollinearity threshold\n0.900000\n\n\n14\nNormalize\nTrue\n\n\n15\nNormalize method\nzscore\n\n\n16\nFold Generator\nStratifiedKFold\n\n\n17\nFold Number\n10\n\n\n18\nCPU Jobs\n-1\n\n\n19\nUse GPU\nFalse\n\n\n20\nLog Experiment\nFalse\n\n\n21\nExperiment Name\nclf-default-name\n\n\n22\nUSI\n10e4\n\n\n\n\n\n\n%%time\nbest_model = compare_models(include=[\"et\", \"lightgbm\", \"rf\", \"dt\"], sort=\"APC\")\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nAPC\nTT (Sec)\n\n\n\n\net\nExtra Trees Classifier\n0.7569\n0.8039\n0.5090\n0.6832\n0.5719\n0.4106\n0.4261\n0.6795\n0.1740\n\n\nrf\nRandom Forest Classifier\n0.7412\n0.7893\n0.5013\n0.6420\n0.5538\n0.3784\n0.3896\n0.6661\n0.0330\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.7252\n0.7776\n0.5090\n0.5987\n0.5452\n0.3519\n0.3574\n0.6378\n0.1310\n\n\ndt\nDecision Tree Classifier\n0.6691\n0.6275\n0.5019\n0.5002\n0.4904\n0.2503\n0.2559\n0.4269\n0.0050\n\n\n\n\n\n\n\n\nCPU times: user 537 ms, sys: 135 ms, total: 672 ms\nWall time: 3.96 s"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#evaluation",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#evaluation",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Evaluation",
    "text": "Evaluation\n\npredictions = (\n    predict_model(best_model, data=test.iloc[:, :-1])\n)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nprediction_label\nprediction_score\n\n\n\n\n752\n3\n108\n62\n24\n0\n26.000000\n0.223\n25\n0\n0.76\n\n\n295\n6\n151\n62\n31\n120\n35.500000\n0.692\n28\n0\n0.57\n\n\n532\n1\n86\n66\n52\n65\n41.299999\n0.917\n29\n0\n0.88\n\n\n426\n0\n94\n0\n0\n0\n0.000000\n0.256\n25\n0\n0.89\n\n\n68\n1\n95\n66\n13\n38\n19.600000\n0.334\n25\n0\n0.97\n\n\n\n\n\n\n\n\nevaluate_model(best_model)"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#shap",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#shap",
    "title": "Predicting diabetes with Pima Indians",
    "section": "SHAP",
    "text": "SHAP\n\ninterpret_model(best_model)\n\n\n\n\n\ninterpret_model(best_model, plot=\"reason\", observation=1)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\ninterpret_model(best_model, plot=\"reason\")\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written."
  }
]