[
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#objectives",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#objectives",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Objectives",
    "text": "Objectives\n\nExample end-to-end supervised learning workflow with Pima Indians\nFocus on conceptual understanding of machine learning\nDemonstrate use of Predictive Power Score (PPS)\nDemonstrate capabilities of low-code tools\nDemonstrate use of average_precision_score (link)\nDemonstrate SHAP values"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#attribution",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#attribution",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Attribution",
    "text": "Attribution\n\nDataset\n\nPima Indians paper (original paper)\nKaggle datacard (link)\n\n\n\nPython libraries\n\nAltair (docs)\nydata-profiling (docs)\nPredictive Power Score (PPS, GitHub, blog)\nPyCaret: open-source, low-code machine learning library in Python that automates machine learning workflows (link)\n\n\nimport altair as alt\nimport pandas as pd\nimport ppscore as pps\nfrom pycaret.classification import *\nimport shap\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import train_test_split\nfrom ydata_profiling import ProfileReport\n\n\n# customize Altair\ndef y_axis():\n    return {\n        \"config\": {\n            \"axisX\": {\"grid\": False},\n            \"axisY\": {\n                \"domain\": False,\n                \"gridDash\": [2, 4],\n                \"tickSize\": 0,\n                \"titleAlign\": \"right\",\n                \"titleAngle\": 0,\n                \"titleX\": -5,\n                \"titleY\": -10,\n            },\n            \"view\": {\n                \"stroke\": \"transparent\",\n                # To keep the same height and width as the default theme:\n                \"continuousHeight\": 300,\n                \"continuousWidth\": 400,\n            },\n        }\n    }\n\n\nalt.themes.register(\"y_axis\", y_axis)\nalt.themes.enable(\"y_axis\");"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#read-and-explore-the-data",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#read-and-explore-the-data",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Read and explore the data",
    "text": "Read and explore the data\n\n%%time\ndf = pd.read_csv(\"diabetes.csv\").astype({\"Outcome\": bool})\ntrain, test = train_test_split(df, test_size=0.3)\nprofile = ProfileReport(train, minimal=True, title=\"Pima Indians Profiling Report\")\nprofile.to_file(\"pima-indians-profiling-report-minimal.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 5.54 s, sys: 184 ms, total: 5.72 s\nWall time: 1.7 s\n\n\n\nprofile.to_notebook_iframe()"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Investigate features with largest predictive power",
    "text": "Investigate features with largest predictive power\nWe use the Predictive Power Score to evaluate which features have the highest predictive power with respect to Outcome.\n\npredictors = (\n    pps.predictors(train, \"Outcome\")\n    .round(3)\n    .iloc[:, :-1]\n)\nbase = (\n    alt.Chart(predictors)\n    .encode(\n        y=alt.Y(\"x:N\").sort(\"-x\"),\n        x=\"ppscore\",\n        tooltip=[\"x\", \"ppscore\"],\n    )\n)\nbase.mark_bar() + base.mark_text(align=\"center\", dy=-5)"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-colinearity",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-colinearity",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Investigate colinearity",
    "text": "Investigate colinearity\n\npps.matrix(train)\n\n\n\n\n\n\n\n\nx\ny\nppscore\ncase\nis_valid_score\nmetric\nbaseline_score\nmodel_score\nmodel\n\n\n\n\n0\nPregnancies\nPregnancies\n1.000000\npredict_itself\nTrue\nNone\n0.000000\n1.000000\nNone\n\n\n1\nPregnancies\nGlucose\n0.000000\nregression\nTrue\nmean absolute error\n23.651769\n24.024994\nDecisionTreeRegressor()\n\n\n2\nPregnancies\nBloodPressure\n0.000000\nregression\nTrue\nmean absolute error\n12.748603\n12.961633\nDecisionTreeRegressor()\n\n\n3\nPregnancies\nSkinThickness\n0.000000\nregression\nTrue\nmean absolute error\n13.467412\n13.630828\nDecisionTreeRegressor()\n\n\n4\nPregnancies\nInsulin\n0.000000\nregression\nTrue\nmean absolute error\n79.240223\n85.015086\nDecisionTreeRegressor()\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n76\nOutcome\nInsulin\n0.000000\nregression\nTrue\nmean absolute error\n79.240223\n84.802415\nDecisionTreeRegressor()\n\n\n77\nOutcome\nBMI\n0.035133\nregression\nTrue\nmean absolute error\n5.672812\n5.473511\nDecisionTreeRegressor()\n\n\n78\nOutcome\nDiabetesPedigreeFunction\n0.000000\nregression\nTrue\nmean absolute error\n0.226384\n0.234531\nDecisionTreeRegressor()\n\n\n79\nOutcome\nAge\n0.000000\nregression\nTrue\nmean absolute error\n8.888268\n8.997094\nDecisionTreeRegressor()\n\n\n80\nOutcome\nOutcome\n1.000000\npredict_itself\nTrue\nNone\n0.000000\n1.000000\nNone\n\n\n\n\n81 rows × 9 columns\n\n\n\n\npps_matrix = (\n    pps.matrix(\n        train.loc[:, predictors.query(\"ppscore &gt; 0\")[\"x\"].tolist()],\n    )\n    .loc[:, [\"x\", \"y\", \"ppscore\"]]\n    .round(3)\n)\n(\n    alt.Chart(pps_matrix)\n    .mark_rect()\n    .encode(\n        x=\"x:O\",\n        y=\"y:O\",\n        color=\"ppscore:Q\",\n        tooltip=[\"x\", \"y\", \"ppscore\"])\n).properties(width=500, height=500)"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#build-models",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#build-models",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Build models",
    "text": "Build models\n\ncls = setup(data = train, \n             target = 'Outcome',\n             numeric_imputation = 'mean',\n             feature_selection = False,\n             pca=False,\n             remove_multicollinearity=True,\n             remove_outliers = False,\n             normalize = True,\n             )\nadd_metric('apc', 'APC', average_precision_score, target = 'pred_proba');\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n5752\n\n\n1\nTarget\nOutcome\n\n\n2\nTarget type\nBinary\n\n\n3\nOriginal data shape\n(537, 9)\n\n\n4\nTransformed data shape\n(537, 9)\n\n\n5\nTransformed train set shape\n(375, 9)\n\n\n6\nTransformed test set shape\n(162, 9)\n\n\n7\nNumeric features\n8\n\n\n8\nPreprocess\nTrue\n\n\n9\nImputation type\nsimple\n\n\n10\nNumeric imputation\nmean\n\n\n11\nCategorical imputation\nmode\n\n\n12\nRemove multicollinearity\nTrue\n\n\n13\nMulticollinearity threshold\n0.900000\n\n\n14\nNormalize\nTrue\n\n\n15\nNormalize method\nzscore\n\n\n16\nFold Generator\nStratifiedKFold\n\n\n17\nFold Number\n10\n\n\n18\nCPU Jobs\n-1\n\n\n19\nUse GPU\nFalse\n\n\n20\nLog Experiment\nFalse\n\n\n21\nExperiment Name\nclf-default-name\n\n\n22\nUSI\n10e4\n\n\n\n\n\n\n%%time\nbest_model = compare_models(include=[\"et\", \"lightgbm\", \"rf\", \"dt\"], sort=\"APC\")\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nAPC\nTT (Sec)\n\n\n\n\net\nExtra Trees Classifier\n0.7569\n0.8039\n0.5090\n0.6832\n0.5719\n0.4106\n0.4261\n0.6795\n0.1740\n\n\nrf\nRandom Forest Classifier\n0.7412\n0.7893\n0.5013\n0.6420\n0.5538\n0.3784\n0.3896\n0.6661\n0.0330\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.7252\n0.7776\n0.5090\n0.5987\n0.5452\n0.3519\n0.3574\n0.6378\n0.1310\n\n\ndt\nDecision Tree Classifier\n0.6691\n0.6275\n0.5019\n0.5002\n0.4904\n0.2503\n0.2559\n0.4269\n0.0050\n\n\n\n\n\n\n\n\nCPU times: user 537 ms, sys: 135 ms, total: 672 ms\nWall time: 3.96 s"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#evaluation",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#evaluation",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Evaluation",
    "text": "Evaluation\n\npredictions = (\n    predict_model(best_model, data=test.iloc[:, :-1])\n)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nprediction_label\nprediction_score\n\n\n\n\n752\n3\n108\n62\n24\n0\n26.000000\n0.223\n25\n0\n0.76\n\n\n295\n6\n151\n62\n31\n120\n35.500000\n0.692\n28\n0\n0.57\n\n\n532\n1\n86\n66\n52\n65\n41.299999\n0.917\n29\n0\n0.88\n\n\n426\n0\n94\n0\n0\n0\n0.000000\n0.256\n25\n0\n0.89\n\n\n68\n1\n95\n66\n13\n38\n19.600000\n0.334\n25\n0\n0.97\n\n\n\n\n\n\n\n\nevaluate_model(best_model)"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#shap",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#shap",
    "title": "Predicting diabetes with Pima Indians",
    "section": "SHAP",
    "text": "SHAP\n\ninterpret_model(best_model)\n\n\n\n\n\ninterpret_model(best_model, plot=\"reason\", observation=1)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\ninterpret_model(best_model, plot=\"reason\")\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written."
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#objectives",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#objectives",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Objectives",
    "text": "Objectives\n\nExample end-to-end supervised learning workflow with Ames Housing dataset\nFocus on conceptual understanding of machine learning\nDemonstrate use of Predictive Power Score (PPS)\nDemonstrate capabilities of low-code tools"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#attribution",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#attribution",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Attribution",
    "text": "Attribution\n\nDataset\n\nAmes Housing dataset paper (original paper)\nKaggle competition advanced regression techniques (link)\n\n\n\nPython libraries\n\nAltair (docs)\nydata-profiling (docs)\nPredictive Power Score (PPS, GitHub, blog)\nPyCaret: open-source, low-code machine learning library in Python that automates machine learning workflows (link)\n\n\nimport altair as alt\nimport pandas as pd\nimport ppscore as pps\nfrom pycaret.regression import *\nfrom ydata_profiling import ProfileReport\n\n\n# customize Altair\ndef y_axis():\n    return {\n        \"config\": {\n            \"axisX\": {\"grid\": False},\n            \"axisY\": {\n                \"domain\": False,\n                \"gridDash\": [2, 4],\n                \"tickSize\": 0,\n                \"titleAlign\": \"right\",\n                \"titleAngle\": 0,\n                \"titleX\": -5,\n                \"titleY\": -10,\n            },\n            \"view\": {\n                \"stroke\": \"transparent\",\n                # To keep the same height and width as the default theme:\n                \"continuousHeight\": 300,\n                \"continuousWidth\": 400,\n            },\n        }\n    }\n\n\nalt.themes.register(\"y_axis\", y_axis)\nalt.themes.enable(\"y_axis\")\n\n\ndef get_descriptions():\n    \"Parse descriptions of columns of Ames Housing dataset\"\n    with open(\"data_description.txt\") as reader:\n        descriptions = {}\n        for line in reader.readlines():\n            if \":\" in line and \"2nd level\" not in line:\n                descriptions[line.split(\": \")[0].strip()] = line.split(\": \")[1].strip()\n    return pd.Series(descriptions).rename(\"descriptions\")\n\n\ndescriptions = get_descriptions()"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#read-and-explore-the-data",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#read-and-explore-the-data",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Read and explore the data",
    "text": "Read and explore the data\n\n%%time\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\nprofile = ProfileReport(train, minimal=True, title=\"Ames Housing Profiling Report\")\nprofile.to_file(\"ames-housing-profiling-report-minimal.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 45 s, sys: 1.78 s, total: 46.7 s\nWall time: 16.4 s\n\n\n\nprofile.to_notebook_iframe()"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Investigate features with largest predictive power",
    "text": "Investigate features with largest predictive power\nWe use the Predictive Power Score to evaluate which features have the highest predictive power with respect to SalePrice.\n\npredictors = (\n    pps.predictors(train, \"SalePrice\")\n    .round(3)\n    .iloc[:, :-1]\n    .merge(descriptions, how=\"left\", left_on=\"x\", right_index=True)\n)\nbase = (\n    alt.Chart(predictors)\n    .encode(\n        x=alt.Y(\"x:N\").sort(\"-y\"),\n        y=\"ppscore\",\n        tooltip=[\"x\", \"ppscore\", \"descriptions\"],\n    )\n    .transform_filter(\"datum.ppscore &gt; 0\")\n)\nbase.mark_bar() + base.mark_text(align=\"center\", dy=-5)"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-colinearity",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-colinearity",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Investigate colinearity",
    "text": "Investigate colinearity\n\npps_matrix = (\n    pps.matrix(\n        train.loc[:, predictors.query(\"ppscore &gt; 0\")[\"x\"].tolist()],\n    )\n    .loc[:, [\"x\", \"y\", \"ppscore\"]]\n    .round(3)\n)\n(\n    alt.Chart(pps_matrix)\n    .mark_rect()\n    .encode(\n        x=\"x:O\",\n        y=\"y:O\",\n        color=\"ppscore:Q\",\n        tooltip=[\"x\", \"y\", \"ppscore\"])\n)"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#build-models",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#build-models",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Build models",
    "text": "Build models\nWe select the 30 features that have the highest predictive power score\n\nselected_predictors = (\n    predictors.sort_values(\"ppscore\", ascending=False).head(30)[\"x\"].to_list()\n)\nreg = setup(data = train.loc[:, selected_predictors + [\"SalePrice\"]], \n             target = 'SalePrice',\n             numeric_imputation = 'mean',\n             categorical_features =  list(train.loc[:, selected_predictors].select_dtypes(\"object\").columns), \n             feature_selection = False,\n             pca=False,\n             remove_multicollinearity=True,\n             remove_outliers = False,\n             normalize = True,\n             )\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n8378\n\n\n1\nTarget\nSalePrice\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(1460, 31)\n\n\n4\nTransformed data shape\n(1460, 116)\n\n\n5\nTransformed train set shape\n(1021, 116)\n\n\n6\nTransformed test set shape\n(439, 116)\n\n\n7\nOrdinal features\n1\n\n\n8\nNumeric features\n16\n\n\n9\nCategorical features\n14\n\n\n10\nRows with missing values\n94.7%\n\n\n11\nPreprocess\nTrue\n\n\n12\nImputation type\nsimple\n\n\n13\nNumeric imputation\nmean\n\n\n14\nCategorical imputation\nmode\n\n\n15\nMaximum one-hot encoding\n25\n\n\n16\nEncoding method\nNone\n\n\n17\nRemove multicollinearity\nTrue\n\n\n18\nMulticollinearity threshold\n0.900000\n\n\n19\nNormalize\nTrue\n\n\n20\nNormalize method\nzscore\n\n\n21\nFold Generator\nKFold\n\n\n22\nFold Number\n10\n\n\n23\nCPU Jobs\n-1\n\n\n24\nUse GPU\nFalse\n\n\n25\nLog Experiment\nFalse\n\n\n26\nExperiment Name\nreg-default-name\n\n\n27\nUSI\n81f6\n\n\n\n\n\n\n%%time\nselected_models = [model for model in models().index if model not in [\"lar\", \"lr\", \"ransac\"]]\nbest_model = compare_models(sort='RMSLE', include=selected_models)\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\nlightgbm\nLight Gradient Boosting Machine\n18267.8967\n969345616.0929\n30245.0381\n0.8400\n0.1474\n0.1051\n0.3780\n\n\ngbr\nGradient Boosting Regressor\n18349.4461\n1064907228.1139\n31464.4286\n0.8221\n0.1497\n0.1059\n0.0810\n\n\nrf\nRandom Forest Regressor\n18834.6022\n1052157810.7295\n31669.8884\n0.8263\n0.1530\n0.1091\n0.1370\n\n\npar\nPassive Aggressive Regressor\n18695.3332\n1145943934.1128\n32527.7429\n0.8093\n0.1535\n0.1061\n0.0560\n\n\nen\nElastic Net\n19941.1185\n1212771199.2709\n33679.9238\n0.8018\n0.1536\n0.1131\n0.0370\n\n\net\nExtra Trees Regressor\n19749.8604\n1158574471.9795\n33510.6172\n0.8073\n0.1591\n0.1138\n0.1370\n\n\nhuber\nHuber Regressor\n18580.7407\n1172797296.1965\n32571.8573\n0.8024\n0.1602\n0.1069\n0.0420\n\n\nbr\nBayesian Ridge\n20557.3468\n1251454965.3245\n34036.3809\n0.7934\n0.1715\n0.1191\n0.0380\n\n\nard\nAutomatic Relevance Determination\n20446.5401\n1229331466.4696\n33711.3986\n0.7969\n0.1747\n0.1193\n0.2740\n\n\nomp\nOrthogonal Matching Pursuit\n21882.7966\n1294135379.9217\n34955.8947\n0.7847\n0.1849\n0.1296\n0.0340\n\n\nada\nAdaBoost Regressor\n24866.3282\n1379609584.9159\n36498.7175\n0.7707\n0.2036\n0.1621\n0.0580\n\n\nknn\nK Neighbors Regressor\n26571.2016\n1730405638.7521\n40931.3774\n0.7200\n0.2050\n0.1518\n0.0360\n\n\ndt\nDecision Tree Regressor\n27747.5148\n2157234242.4490\n45330.0191\n0.6512\n0.2169\n0.1564\n0.0350\n\n\nllar\nLasso Least Angle Regression\n21458.2025\n1320695830.3446\n35006.4301\n0.7809\n0.2187\n0.1268\n0.0380\n\n\nlasso\nLasso Regression\n21455.6793\n1320742178.3808\n35006.1951\n0.7809\n0.2189\n0.1268\n0.2100\n\n\nridge\nRidge Regression\n21439.2241\n1318937040.3720\n34981.0548\n0.7812\n0.2196\n0.1266\n0.0360\n\n\nsvm\nSupport Vector Regression\n55543.3805\n6417749387.4994\n79739.3850\n-0.0524\n0.3979\n0.3195\n0.0450\n\n\ndummy\nDummy Regressor\n57352.4774\n6133919031.8184\n78021.7431\n-0.0086\n0.4061\n0.3635\n0.0340\n\n\ntr\nTheilSen Regressor\n29178.3219\n2564758742.0908\n49572.3895\n0.5667\n0.4258\n0.1978\n4.0290\n\n\nkr\nKernel Ridge\n182040.0692\n34133507087.4154\n184731.2672\n-4.7500\n1.7994\n1.1623\n0.0380\n\n\nmlp\nMLP Regressor\n166456.5847\n32851392125.7179\n181040.0031\n-4.4796\n2.7703\n0.9182\n0.2890\n\n\n\n\n\n\n\n\nCPU times: user 4.09 s, sys: 446 ms, total: 4.53 s\nWall time: 1min 3s"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#evaluation",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#evaluation",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Evaluation",
    "text": "Evaluation\n\nWith a standard, AutoML-like workflow, we achive RMSLE of 0.13 - 0.14 (over different runs), which is already in the top 25% of the 4,200 submissions on the leaderboard\nWe can now make predictions on the test set\n\n\npredictions = (\n    predict_model(best_model, data=test)\n    .rename(columns={\"prediction_label\": \"SalePrice\"})\n    .loc[:, [\"Id\", \"SalePrice\"]]\n)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nId\nSalePrice\n\n\n\n\n0\n1461\n126951.931078\n\n\n1\n1462\n142402.002648\n\n\n2\n1463\n185086.014955\n\n\n3\n1464\n191718.590497\n\n\n4\n1465\n186412.972060\n\n\n\n\n\n\n\n\nPipeline\n\nplot_model(best_model, 'pipeline')\n\n\n\n\n\nplot_model(best_model, 'feature')\n\n\n\n\n\n\n\n\nplot_model(best_model, 'residuals')"
  },
  {
    "objectID": "posts/predictive-power-score.html",
    "href": "posts/predictive-power-score.html",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "",
    "text": "It is Friday afternoon and your boss tells you that the data delivery surprisingly arrived early — after only 4 weeks of back and forth. This was the missing piece for your predictive model. You’re excited but also a little bit anxious because you know what’s next: exploring the data. All. 45. Columns. This will take many hours but you know it’s worth it because without data understanding you are walking blind. One obvious step is to have a look at all the univariate column distributions. But this won’t be enough.\n\nYou ask yourself: what relationships exist between columns?\n\nTo answer this question, you just repeat the typical drill: calculate a correlation matrix and check for some surprising relationships. Whenever you are surprised, you take a moment to plot a scatterplot of the two columns at hand and see if you can make any sense of it. Hopefully you can but too often you cannot because you don’t even know what the columns mean in the first place. But this is a story for another day.\n\nAfter inspecting the correlation matrix you move on and you don’t even know what you don’t know (scary).\n\nLet’s take a moment to review the correlation. The score ranges from -1 to 1 and indicates if there is a strong linear relationship — either in a positive or negative direction. So far so good. However, there are many non-linear relationships that the score simply won’t detect. For example, a sinus wave, a quadratic curve or a mysterious step function. The score will just be 0, saying: “Nothing interesting here”. Also, correlation is only defined for numeric columns. So, let’s drop all the categoric columns. In my last project more than 60% of the columns were categoric, but hey. Never mind. And no, I won’t convert the columns because they are not ordinal and OneHotEncoding will create a matrix that has more values than there are atoms in the universe.\n\nIf you are a little bit too well educated you know that the correlation matrix is symmetric. So you basically can throw away one half of it. Great, we saved ourselves some work there! Or did we? Symmetry means that the correlation is the same whether you calculate the correlation of A and B or the correlation of B and A. However, relationships in the real world are rarely symmetric. More often, relationships are asymmetric. Here is an example: The last time I checked, my zip code of 60327 tells strangers quite reliably that I am living in Frankfurt, Germany. But when I only tell them my city, somehow they are never able to deduce the correct zip code. Pff … amateurs. Another example is this: a column with 3 unique values will never be able to perfectly predict another column with 100 unique values. But the opposite might be true. Clearly, asymmetry is important because it is so common in the real world.\n\nThinking about those shortcomings of correlation, I started to wonder: can we do better?\n\nThe requirements: One day last year, I was dreaming about a score that would tell me if there is any relationship between two columns — no matter if the relationship is linear, non-linear, gaussian or only known by aliens. Of course, the score should be asymmetric because I want to detect all the weird relationships between cities and zip codes. The score should be 0 if there is no relationship and the score should be 1 if there is a perfect relationship. And as the icing on the cake, the score should be able to handle categoric and numeric columns out of the box. Summing it up for all my academic friends: an asymmetric, data-type-agnostic score for predictive relationships between two columns that ranges from 0 to 1."
  },
  {
    "objectID": "posts/predictive-power-score.html#too-many-problems-with-the-correlation",
    "href": "posts/predictive-power-score.html#too-many-problems-with-the-correlation",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "",
    "text": "It is Friday afternoon and your boss tells you that the data delivery surprisingly arrived early — after only 4 weeks of back and forth. This was the missing piece for your predictive model. You’re excited but also a little bit anxious because you know what’s next: exploring the data. All. 45. Columns. This will take many hours but you know it’s worth it because without data understanding you are walking blind. One obvious step is to have a look at all the univariate column distributions. But this won’t be enough.\n\nYou ask yourself: what relationships exist between columns?\n\nTo answer this question, you just repeat the typical drill: calculate a correlation matrix and check for some surprising relationships. Whenever you are surprised, you take a moment to plot a scatterplot of the two columns at hand and see if you can make any sense of it. Hopefully you can but too often you cannot because you don’t even know what the columns mean in the first place. But this is a story for another day.\n\nAfter inspecting the correlation matrix you move on and you don’t even know what you don’t know (scary).\n\nLet’s take a moment to review the correlation. The score ranges from -1 to 1 and indicates if there is a strong linear relationship — either in a positive or negative direction. So far so good. However, there are many non-linear relationships that the score simply won’t detect. For example, a sinus wave, a quadratic curve or a mysterious step function. The score will just be 0, saying: “Nothing interesting here”. Also, correlation is only defined for numeric columns. So, let’s drop all the categoric columns. In my last project more than 60% of the columns were categoric, but hey. Never mind. And no, I won’t convert the columns because they are not ordinal and OneHotEncoding will create a matrix that has more values than there are atoms in the universe.\n\nIf you are a little bit too well educated you know that the correlation matrix is symmetric. So you basically can throw away one half of it. Great, we saved ourselves some work there! Or did we? Symmetry means that the correlation is the same whether you calculate the correlation of A and B or the correlation of B and A. However, relationships in the real world are rarely symmetric. More often, relationships are asymmetric. Here is an example: The last time I checked, my zip code of 60327 tells strangers quite reliably that I am living in Frankfurt, Germany. But when I only tell them my city, somehow they are never able to deduce the correct zip code. Pff … amateurs. Another example is this: a column with 3 unique values will never be able to perfectly predict another column with 100 unique values. But the opposite might be true. Clearly, asymmetry is important because it is so common in the real world.\n\nThinking about those shortcomings of correlation, I started to wonder: can we do better?\n\nThe requirements: One day last year, I was dreaming about a score that would tell me if there is any relationship between two columns — no matter if the relationship is linear, non-linear, gaussian or only known by aliens. Of course, the score should be asymmetric because I want to detect all the weird relationships between cities and zip codes. The score should be 0 if there is no relationship and the score should be 1 if there is a perfect relationship. And as the icing on the cake, the score should be able to handle categoric and numeric columns out of the box. Summing it up for all my academic friends: an asymmetric, data-type-agnostic score for predictive relationships between two columns that ranges from 0 to 1."
  },
  {
    "objectID": "posts/predictive-power-score.html#calculating-the-predictive-power-score-pps",
    "href": "posts/predictive-power-score.html#calculating-the-predictive-power-score-pps",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Calculating the Predictive Power Score (PPS)",
    "text": "Calculating the Predictive Power Score (PPS)\n\nFirst of all, there is not the one and only way to calculate the predictive power score. In fact, there are many possible ways to calculate a score that satisfies the requirements mentioned before. So, let’s rather think of the predictive power score as a framework for a family of scores.\n\nLet’s say we have two columns and want to calculate the predictive power score of A predicting B. In this case, we treat B as our target variable and A as our (only) feature. We can now calculate a cross-validated Decision Tree and calculate a suitable evaluation metric. When the target is numeric we can use a Decision Tree Regressor and calculate the Mean Absolute Error (MAE). When the target is categoric, we can use a Decision Tree Classifier and calculate the weighted F1. You might also use other scores like the ROC etc but let’s put those doubts aside for a second because we have another problem:\n\nMost evaluation metrics are meaningless if you don’t compare them to a baseline\n\nI guess you all know the situation: you tell your grandma that your new model has a F1 score of 0.9 and somehow she is not as excited as you are. In fact, this is very smart of her because she does not know if anyone can score 0.9 or if you are the first human being who ever scored higher than 0.5 after millions of awesome KAGGLErs tried. So, we need to “normalize” our evaluation score. And how do you normalize a score? You define a lower and an upper limit and put the score into perspective. So what should the lower and upper limit be? Let’s start with the upper limit because this is usually easier: a perfect F1 is 1. A perfect MAE is 0. Boom! Done. But what about the lower limit? Actually, we cannot answer this in absolute terms.\n\nThe lower limit depends on the evaluation metric and your data set. It is the value that a naive predictor achieves.\n\nIf you achieve a F1 score of 0.9 this might be super bad or really good. If your super fancy cancer detection model always predicts “benign” and it still scores 0.9 on that highly skewed dataset then 0.9 is obviously not so good. So, we need to calculate a score for a very naive model. But what is a naive model? For a classification problem, always predicting the most common class is pretty naive. For a regression problem, always predicting the median value is pretty naive.\n\nLet’s have a look at a detailed, fictional example:\nGetting back to the example of the zip codes and the city name. Imagine both columns are categoric. First, we want to calculate the PPS of zip code to city. We use the weighted F1 score because city is categoric. Our cross-validated Decision Tree Classifier achieves a score of 0.95 F1. We calculate a baseline score via always predicting the most common city and achieve a score of 0.1 F1. If you normalize the score, you will get a final PPS of 0.94 after applying the following normalization formula: (0.95–0.1) / (1–0.1). As we can see, a PPS score of 0.94 is rather high, so the zip code seems to have a good predictive power towards the city. However, if we calculate the PPS in the opposite direction, we might achieve a PPS of close to 0 because the Decision Tree Classifier is not substantially better than just always predicting the most common zip code.\n\nPlease note: the normalization formula for the MAE is different from the F1. For MAE lower is better and the best value is 0."
  },
  {
    "objectID": "posts/predictive-power-score.html#comparing-the-pps-to-correlation",
    "href": "posts/predictive-power-score.html#comparing-the-pps-to-correlation",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Comparing the PPS to correlation",
    "text": "Comparing the PPS to correlation\nIn order to get a better feeling for the PPS and its differences to the correlation, let’s have a look at the following two examples:\n\npps2Example 1: Non-linear effects and asymmetry\n\nLet’s use a typical quadratic relationship: the feature x is a uniform variable ranging from -2 to 2 and the target y is the square of x plus some error. In this case, x can predict y very well because there is a clear non-linear, quadratic relationship — after all that’s how we generated the data. However, this is not true in the other direction from y to x. For example, if y is 4, it is impossible to predict whether x was roughly 2 or -2. Thus, the predictive relationship is asymmetric and the scores should reflect this.\nWhat are the values of the scores in this example? If you don’t already know what you are looking for, the correlation will leave you hanging because the correlation is 0. Both from x to y and from y to x because the correlation is symmetric. However, the PPS from x to y is 0.67, detecting the non-linear relationship and saving the day. Nevertheless, the PPS is not 1 because there exists some error in the relationship. In the other direction, the PPS from y to x is 0 because your prediction cannot be better than the naive baseline and thus the score is 0.\n\n\nExample 2: Categorical columns and hidden patterns\nLet’s compare the correlation matrix to the PPS matrix on the Titanic dataset. “The Titanic dataset? Again??” I know, you probably think you already have seen everything about the Titanic dataset but maybe the PPS will give you some new insights.\n\n\nTwo findings about the correlation matrix:\n\nThe correlation matrix is smaller and leaves out many interesting relationships. Of course, that makes sense because columns like Sex, TicketID or Port are categoric and the correlation cannot be computed for them.\nThe correlation matrix shows a negative correlation between TicketPrice and Class of medium strength (-0.55). We can double-check this relationship if we have a look at the PPS. We will see that the TicketPrice is a strong predictor for the Class (0.9 PPS) but not vice versa. The Class only predicts the TicketPrice with a PPS of 0.2. This makes sense because whether your ticket did cost 5.000$ or 10.000$ you were most likely in the highest class. In contrast, if you know that someone was in the highest class you cannot say whether they paid 5.000$ or 10.000$ for their ticket. In this scenario, the asymmetry of the PPS shines again.\n\n\n\nFour findings about the PPS matrix:\n\nThe first row of the matrix tells you that the best univariate predictor of the column Survived is the column Sex. This makes sense because women were prioritized during the rescue. (We could not find this information in the correlation matrix because the column Sex was dropped.)\nIf you have a look at the column for TicketID, you can see that TicketID is a fairly good predictor for a range of columns. If you further dig into this pattern, you will find out that multiple persons had the same TicketID. Thus, the TicketID is actually referencing a latent group of passengers who bought the ticket together, for example the big Italian Rossi family that turns any evening into a spectacle. Thus, the PPS helped me to detect a hidden pattern.\nWhat’s even more surprising than the strong predictive power of TicketID is the strong predictive power of TicketPrice across a wide range of columns. Especially, the fact that the TicketPrice is fairly good at predicting the TicketID (0.67) and vice versa (0.64). Upon further research you will find out that tickets often had unique prices. For example, only the Italian Rossi family paid a price of 72,50$. This is a critical insight! It means that the TicketPrice contains information about the TicketID and thus about our Italian family. An information that you need to have when considering potential information leakage.\nLooking at the PPS matrix, we can see effects that might be explained by causal chains. (Did he just say causal? — Of course, those causal hypotheses have to be treated carefully but this is beyond the scope of this article.) For example, you might be surprised why the TicketPrice has predictive power on the survival rate (PPS 0.39). But if you know that the Class influences your survival rate (PPS 0.36) and that the TicketPrice is a good predictor for your Class (PPS 0.9), then you might have found an explanation."
  },
  {
    "objectID": "posts/predictive-power-score.html#applications-of-the-pps-and-the-pps-matrix",
    "href": "posts/predictive-power-score.html#applications-of-the-pps-and-the-pps-matrix",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Applications of the PPS and the PPS matrix",
    "text": "Applications of the PPS and the PPS matrix\nAfter we learned about the advantages of the PPS, let’s see where we can use the PPS in the real life.\n\nDisclaimer: There are use cases for both the PPS and the correlation. The PPS clearly has some advantages over correlation for finding predictive patterns in the data. However, once the patterns are found, the correlation is still a great way of communicating found linear relationships.\n\n\nFind patterns in the data: The PPS finds every relationship that the correlation finds — and more. Thus, you can use the PPS matrix as an alternative to the correlation matrix to detect and understand linear or nonlinear patterns in your data. This is possible across data types using a single score that always ranges from 0 to 1.\nFeature selection: In addition to your usual feature selection mechanism, you can use the predictive power score to find good predictors for your target column. Also, you can eliminate features that just add random noise. Those features sometimes still score high in feature importance metrics. In addition, you can eliminate features that can be predicted by other features because they don’t add new information. Besides, you can identify pairs of mutually predictive features in the PPS matrix — this includes strongly correlated features but will also detect non-linear relationships.\nDetect information leakage: Use the PPS matrix to detect information leakage between variables — even if the information leakage is mediated via other variables.\nData Normalization: Find entity structures in the data via interpreting the PPS matrix as a directed graph. This might be surprising when the data contains latent structures that were previously unknown. For example: the TicketID in the Titanic dataset is often an indicator for a family."
  },
  {
    "objectID": "posts/predictive-power-score.html#how-to-use-the-pps-in-your-own-python-project",
    "href": "posts/predictive-power-score.html#how-to-use-the-pps-in-your-own-python-project",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "How to use the PPS in your own (Python) project",
    "text": "How to use the PPS in your own (Python) project\nIf you are still following along you are one of the rare human beings who still have an attention span — you crazy beast! If you can’t wait to see what the PPS will reveal on your own data, we have some good news for you: we open-sourced an implementation of the PPS as a Python library named ppscore.\n\nBefore using the Python library, please take a moment to read through the calculation details\n\nInstalling the package is as simple as\n\npip install ppscore\n\nCalculating the PPS for a given pandas dataframe:\n\nimport ppscore as pps\npps.score(df, \"feature_column\", \"target_column\")\n\nYou can also calculate the whole PPS matrix:\n\npps.matrix(df)\n\n\nHow fast is the PPS in comparison to the correlation?\nAlthough the PPS has many advantages over the correlation, there is some drawback: it takes longer to calculate. But how bad is it? Does it take multiple weeks or are we done in a couple of minutes or even seconds? When calculating a single PPS using the Python library, the time should be no problem because it usually takes around 10–500ms. The calculation time mostly depends on the data types, the number of rows and the used implementation. However, when calculating the whole PPS matrix for 40 columns this results in 40*40=1600 individual calculations which might take 1–10 minutes. So you might want to start the calculation of the PPS matrix in the background and go on that summer vacation you always dreamed of! 🏖 ️For our projects and datasets the computational performance was always good enough but of course there is room for improvement. Fortunately, we see many ways how the calculation of the PPS can be improved to achieve speed gains of a factor of 10–100. For example, using intelligent sampling, heuristics or different implementations of the PPS. If you like the PPS and are in need of a faster calculation, please reach out to us."
  },
  {
    "objectID": "posts/predictive-power-score.html#limitations",
    "href": "posts/predictive-power-score.html#limitations",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Limitations",
    "text": "Limitations\nWe made it — you are excited and want to show the PPS to your colleagues. However, you know they are always so critical about new methods. That’s why you better be prepared to know the limitations of the PPS:\n\nThe calculation is slower than the correlation (matrix).\nThe score cannot be interpreted as easily as the correlation because it does not tell you anything about the type of relationship that was found. Thus, the PPS is better for finding patterns but the correlation is better to communicate found linear relationships.\nYou cannot compare the scores for different target variables in a strict mathematical way because they are calculated using different evaluation metrics. The scores are still valuable in the real world, but you need to keep this in mind.\nThere are limitations of the components used underneath the hood. Please remember: you might exchange the components e.g. using a GLM instead of a Decision Tree or using ROC instead of F1 for binary classifications.\nIf you use the PPS for feature selection you still want to perform forward and backward selection in addition. Also, the PPS cannot detect interaction effects between features towards your target."
  },
  {
    "objectID": "posts/predictive-power-score.html#conclusion",
    "href": "posts/predictive-power-score.html#conclusion",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Conclusion",
    "text": "Conclusion\nAfter years of using the correlation we were so bold (or crazy?) to suggest an alternative that can detect linear and non-linear relationships. The PPS can be applied to numeric and categoric columns and it is asymmetric. We proposed an implementation and open-sourced a Python package. In addition, we showed the differences to the correlation on some examples and discussed some new insights that we can derive from the PPS matrix. Now it is up to you to decide what you think about the PPS and if you want to use it on your own projects.\nGithub: https://github.com/8080labs/ppscore."
  },
  {
    "objectID": "posts/precision-recall.html",
    "href": "posts/precision-recall.html",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "",
    "text": "A receiver operating characteristic (ROC) curve displays how well a model can classify binary outcomes. An ROC curve is generated by plotting the false positive rate of a model against its true positive rate, for each possible cutoff value. Often, the area under the curve (AUC) is calculated and used as a metric showing how well a model can classify data points.\nIf you’re interest in learning more about ROC and AUC, I recommend this short Medium blog, which contains this neat graphic.\n\nDariya Sydykova, graduate student at the Wilke lab at the University of Texas at Austin, shared some great visual animations of how model accuracy and model cutoffs alter the ROC curve and the AUC metric. The quotes and animations are from the associated github repository."
  },
  {
    "objectID": "posts/precision-recall.html#roc-auc-precision-and-recall-visually-explained",
    "href": "posts/precision-recall.html#roc-auc-precision-and-recall-visually-explained",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "",
    "text": "A receiver operating characteristic (ROC) curve displays how well a model can classify binary outcomes. An ROC curve is generated by plotting the false positive rate of a model against its true positive rate, for each possible cutoff value. Often, the area under the curve (AUC) is calculated and used as a metric showing how well a model can classify data points.\nIf you’re interest in learning more about ROC and AUC, I recommend this short Medium blog, which contains this neat graphic.\n\nDariya Sydykova, graduate student at the Wilke lab at the University of Texas at Austin, shared some great visual animations of how model accuracy and model cutoffs alter the ROC curve and the AUC metric. The quotes and animations are from the associated github repository."
  },
  {
    "objectID": "posts/precision-recall.html#roc-auc",
    "href": "posts/precision-recall.html#roc-auc",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "ROC & AUC",
    "text": "ROC & AUC\nThe plot on the left shows the distributions of predictors for the two outcomes, and the plot on the right shows the ROC curve for these distributions. The vertical line that travels left-to-right is the cutoff value. The red dot that travels along the ROC curve corresponds to the false positive rate and the true positive rate for the cutoff value given in the plot on the left.\nThe traveling cutoff demonstrates the trade-off between trying to classify one outcome correctly and trying to classify the other outcome correcly. When we try to increase the true positive rate, we also increase the false positive rate. When we try to decrease the false positive rate, we decrease the true positive rate.\n\nThe shape of an ROC curve changes when a model changes the way it classifies the two outcomes.\nThe animation [below] starts with a model that cannot tell one outcome from the other, and the two distributions completely overlap (essentially a random classifier). As the two distributions separate, the ROC curve approaches the left-top corner, and the AUC value of the curve increases. When the model can perfectly separate the two outcomes, the ROC curve forms a right angle and the AUC becomes 1."
  },
  {
    "objectID": "posts/precision-recall.html#precision-recall",
    "href": "posts/precision-recall.html#precision-recall",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Precision-recall",
    "text": "Precision-recall\nTwo other metrics that are often used to quantify model performance are precision and recall.\nPrecision (also called positive predictive value) is defined as the number of true positives divided by the total number of positive predictions. Hence, precision quantifies what percentage of the positive predictions were correct: How correct your model’s positive predictions were.\nRecall (also called sensitivity) is defined as the number of true positives divided by the total number of true postives and false negatives (i.e. all actual positives). Hence, recall quantifies what percentage of the actual positives you were able to identify: How sensitive your model was in identifying positives.\nDariya also made some visualizations of precision-recall curves: precision-recall curves also displays how well a model can classify binary outcomes. However, it does it differently from the way an ROC curve does. Precision-recall curve plots true positive rate (recall or sensitivity) against the positive predictive value (precision).\nIn the middle, here below, the ROC curve with AUC. On the right, the associated precision-recall curve. Similarly to the ROC curve, when the two outcomes separate, precision-recall curves will approach the top-right corner. Typically, a model that produces a precision-recall curve that is closer to the top-right corner is better than a model that produces a precision-recall curve that is skewed towards the bottom of the plot."
  },
  {
    "objectID": "posts/precision-recall.html#class-imbalance",
    "href": "posts/precision-recall.html#class-imbalance",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Class imbalance",
    "text": "Class imbalance\nClass imbalance happens when the number of outputs in one class is different from the number of outputs in another class. For example, one of the distributions has 1000 observations and the other has 10. An ROC curve tends to be more robust to class imbalanace that a precision-recall curve.\nIn this animation [below], both distributions start with 1000 outcomes. The blue one is then reduced to 50. The precision-recall curve changes shape more drastically than the ROC curve, and the AUC value mostly stays the same. We also observe this behaviour when the other disribution is reduced to 50.\n\nHere’s the same, but now with the red distribution shrinking to just 50 samples."
  },
  {
    "objectID": "posts/precision-recall.html#further-reading",
    "href": "posts/precision-recall.html#further-reading",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Further reading",
    "text": "Further reading\n\nSaito & Rehmsmeier (2015), The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets"
  },
  {
    "objectID": "python/realpython.html",
    "href": "python/realpython.html",
    "title": "Getting into Python with RealPython.com",
    "section": "",
    "text": "The table below lists the topics that you should have covered _as a bare minimum. You often have a choice to suit your prefered learning style: courses are videos, tutorials are text. Note that the tutorials are free of charge, while the videos are only accessible with a paid subscription.\n\n\n\nchapter\ntopic\ncourse\ntutorial\n\n\n\n\n2. Setting up Python\nInstalling Python 3\nlink\nlink\n\n\n3. Your First Python Program\nCode Your First Python Program\nlink\n\n\n\n\nBasic Data Types in Python\nlink\nlink\n\n\n\nVariables in Python\nlink\nlink\n\n\n4. Strings and String Methods\nStrings and Character Data in Python\nlink\nlink\n\n\n\nReading Input and Writing Output in Python\nlink\nlink\n\n\n5. Numbers and Math\nOperators and Expressions in Python\nlink\nlink\n\n\n\nNumbers in Python\n\nlink\n\n\n6. Functions and Loops\n“for” loops (Definite Iteration)\nlink\nlink\n\n\n\n“while” loops (Indefinite Iteration)\nlink\nlink\n\n\n\nDefining and Calling Python Functions\nlink\nlink\n\n\n8. Conditional Logic and Control Flow\nConditional Statements in Python (if/elis/else)\nlink\nlink\n\n\n9. Tuples, Lists and Dictinairies\nLists and Tuples in Python\nlink\nlink\n\n\n\nDictionairies in Python\nlink\nlink\n\n\n11. Modules and Packages\nPython Modules and Packages: An Introduction\nlink\nlink\n\n\n12. File Input and Output\nReading and Writing Files in Python\nlink"
  },
  {
    "objectID": "python/whirlwind.html",
    "href": "python/whirlwind.html",
    "title": "A Whirlwind Tour of Python",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "altair/comet-chart.html",
    "href": "altair/comet-chart.html",
    "title": "Comet charts in Python",
    "section": "",
    "text": "Zan Armstrong’s comet chart has been on my list of hobby projects for a while now. I think it is an elegant solution to visualize statistical mix effects and address Simpson’s paradox, and particularly useful when working with longitudinal data involving different sub-populations. Recently I found a good excuse to spend some time to actually use it as part of a exploratory data analysis on a project.\nSince I mostly work in Python and have recently fallen in love with Altair — for the same reasons as Fernando explains here — I wondered how the comet chart could be implemented using the grammar of interactive graphics. It took me a while to figure out how to actually plot the comets. In a previous version, I had drawn glyphs using Bokeh. While Altair allows you to plot any SVG path in a graph, this felt a bit hacky and not quite in line with the philosophy of using a grammar of graphics.\nThankfully Mattijn was quick to suggest using trail-marks, after which it was almost as easy as pie. So here’s an example using a dataset of 20,000 flights for 59 destination airports.\n\nimport altair as alt\nimport pandas as pd\nimport vega_datasets\n\n\n# Use airline data to assess statistical mix effects of delays\nflights = vega_datasets.data.flights_20k()\naggregation = dict(\n    number_of_flights=(\"destination\", \"count\"),\n    mean_delay=(\"delay\", \"mean\"),\n    mean_distance=(\"distance\", \"mean\"),\n)\n\n# Compare delays by destination between month 1 and 3\ngrouped = flights.groupby(by=[flights.destination, flights.date.dt.month])\ndf = (\n    grouped.agg(**aggregation)\n    .loc[(slice(None), [1, 3]), :]\n    .assign(\n        change_mean_delay=lambda df:\n            df.groupby(\"destination\")[\"mean_delay\"].diff(),\n    )\n    .fillna(method=\"bfill\")\n    .reset_index()\n    .round(2)\n)\n\n# Calculate weigthed average of delays for month 1 and 3\ntotal = (\n    flights.groupby(flights.date.dt.month)\n    .agg(**aggregation)\n    .loc[[1, 3], :]\n    .assign(\n        change_mean_delay=lambda df: df.mean_delay.diff(),\n        destination='TOTAL'\n    )\n    .fillna(method=\"bfill\")\n    .round(2)\n    .reset_index()\n    .loc[:, df.columns]\n)\n\n\ndef comet_chart(df, stroke=\"white\"):\n    return (\n    alt.Chart(df, width=600, height=450)\n    .mark_trail(stroke=stroke)\n    .encode(\n        x=alt.X(\"number_of_flights\", scale=alt.Scale(type=\"log\")),\n        y=alt.Y(\"mean_delay\"),\n        detail=\"destination\",\n        size=alt.Size(\"date\", scale=alt.Scale(range=[0, 10]), legend=None),\n        tooltip=[\n            \"destination\",\n            \"number_of_flights\",\n            \"mean_delay\",\n            \"change_mean_delay\",\n            \"mean_distance\",\n        ],\n        # trails don't support continuous color\n        # see https://github.com/vega/vega/issues/1187\n        # hence use bins\n        color=alt.Color(\n            \"change_mean_delay:Q\",\n            bin=alt.Bin(step=2),\n            scale=alt.Scale(scheme=\"blueorange\"),\n            legend=alt.Legend(orient=\"top\"),\n        ),\n    )\n)\n\n\ncomet_chart(df) + comet_chart(total, stroke=\"black\")\n\n\n\n\n\n\n\nIn the example shown here, each comet represents one destination airport. The head of the comet corresponds to the most recent observation of the number of flight arrivals (x-axis, shown as logarithmic scale to accommodate the wide range of observations) against the mean delay of those flights (y-axis). The tail of the comet represents a similar (x,y) datum, but from an earlier point in time. Finally, the colour of the comet is encoded to show the change in the mean delay for each airport. A tooltip with a summary of the data is shown when hovering over the head of the comet.\nSo-called mix effects can often lead to misinterpretation of aggregate numbers. In the example of flight delays, the fact that only a small change is observed in the mean delay across all airports — visualized with the right-most comet outlined in black — hides the underlying variance between airports. Note that in this example the size of each sub-population (number of flights per airport) remains relatively constant, hence the comets here only go up and down. As explained in the original article, mix effects become harder to interpret when the relative size of the sub-populations change as well as their relative values. In the most extreme case this may lead to Simpson’s paradox.\nWith this base implementation of comet charts in Altair, you can really go to town and combine it with other interactive graphs. Using the overview-detail pattern, you could plot an accompanying density plot of all the flights for a given airport. That way you can quickly zoom in to the lowest level of detail and get a better understanding of the underlying mix effects."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Anthology of Data Science",
    "section": "",
    "text": "An Introduction to Statistical Learning\n\n\nStatistical learning has become a critical toolkit for anyone who wishes to understand data. This book provides a broad and less technical treatment of key topics in…\n\n\n\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Jonathan Taylor\n\n\nJul 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Principles & Practice\n\n\nThis textbook is provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to be able to use them sensibly.\n\n\n\nRob J Hyndman, George Athanasopoulos\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretable Machine Learning\n\n\nThis book is about making machine learning models and their decisions interpretable. It will enable you to select and correctly apply the interpretation method that is most…\n\n\n\nChristoph Molnar\n\n\nMar 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython of Data Analysis, Third Edition\n\n\nThis book is concerned with the nuts and bolts of manipulating, processing, cleaning, and crunching data in Python.\n\n\n\nWes McKinney\n\n\nApr 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR for Data Science, Second Edition\n\n\nThis book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it and visualize.\n\n\n\nHadley Wickham, Mine Çetinkaya-Rundel, Garrett Grolemenund\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Turing Way\n\n\nAn open source, open collaboration, community-driven handbook to reproducible, ethical and collaborative data science.\n\n\n\nThe Turing Way Community\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\nThis book explains the ideas that underlie deep learning, distinguishing it from volumes that cover coding and other practical aspects.\n\n\n\nSimon Prince\n\n\nOct 23, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "books/udl.html",
    "href": "books/udl.html",
    "title": "Understanding Deep Learning",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "books/islp.html",
    "href": "books/islp.html",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An anthology of open access data science materials",
    "section": "",
    "text": "Machine learning is permeating many fields of work. As a new ‘system technology’1, its expected impact on organizations and society is expected to be of the same magnitude as the steam engine or electricity. As such, more and more professionals are seeking to acquire the necessary understanding and skills to apply machine learning in their day-to-day work. In fact, more people without a background in either computer science or statistics - let alone both - have a need for high-quality, open access content to explore and learn data science by themselves.\nNow there is a lot of machine learning learning materials out there, so why this anthology? Based on my experience in teaching professional education course on data & AI, I am continouly challenged to:\n\ncurate content for different professional learning paths, combining various existing open access materials;\nbalance between too technical vs too vague, handwaving or even downright wrong;\nproblem-based approach: rather than, say, explaining the principles underlying regularization, we choose to demonstrate these principles using the simplest algorithms. With a little math, everyone should be able to understand how LASSO performs regularisation for regression models. With this intuitive understanding, you can move on to more complex algorithms and applications, and intuitively reason where and how to use regularisation.\n\nThis anthology was born out of a need to scratch my own itch, namely have a central repository where I can continously collate and update interesting resources. I hope it is also helpful those seeking a stepping stone into the wonderful field of data science. Feel free to drop me a note with suggestions or feedback by reporting an issue."
  },
  {
    "objectID": "index.html#why-this-anthology",
    "href": "index.html#why-this-anthology",
    "title": "An anthology of open access data science materials",
    "section": "",
    "text": "Machine learning is permeating many fields of work. As a new ‘system technology’1, its expected impact on organizations and society is expected to be of the same magnitude as the steam engine or electricity. As such, more and more professionals are seeking to acquire the necessary understanding and skills to apply machine learning in their day-to-day work. In fact, more people without a background in either computer science or statistics - let alone both - have a need for high-quality, open access content to explore and learn data science by themselves.\nNow there is a lot of machine learning learning materials out there, so why this anthology? Based on my experience in teaching professional education course on data & AI, I am continouly challenged to:\n\ncurate content for different professional learning paths, combining various existing open access materials;\nbalance between too technical vs too vague, handwaving or even downright wrong;\nproblem-based approach: rather than, say, explaining the principles underlying regularization, we choose to demonstrate these principles using the simplest algorithms. With a little math, everyone should be able to understand how LASSO performs regularisation for regression models. With this intuitive understanding, you can move on to more complex algorithms and applications, and intuitively reason where and how to use regularisation.\n\nThis anthology was born out of a need to scratch my own itch, namely have a central repository where I can continously collate and update interesting resources. I hope it is also helpful those seeking a stepping stone into the wonderful field of data science. Feel free to drop me a note with suggestions or feedback by reporting an issue."
  },
  {
    "objectID": "index.html#how-is-this-anthology-set-up",
    "href": "index.html#how-is-this-anthology-set-up",
    "title": "An anthology of open access data science materials",
    "section": "How is this anthology set up?",
    "text": "How is this anthology set up?\n\nA selection of the best books\nA selection of what I think are the best textbooks is provided as a digital bookshelf. All the books included are open access, thanks to their respective authors! They should keep you busy for a while.\n\n\nVisualization with Altair\nI am a big fan of the Vega-Altair ecosystem for data visualization, because it not only helps me in creating appealing, interactive visualizations, but it also helps me to reason about the data when I am doing exploratory data analysis. A selection of tutorials, blogpost is provided in the section Visualization with Altair.\n\n\nSelected papers & blogposts\nVarious topics and perspectives data science. Clearly this is an opinionated and very personal selection.\n\n\nNotebooks\nNotebooks and explainers for those seeking hands-on examples."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "An anthology of open access data science materials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSheikh et al. (2023), Mission AI: the New System Technology, https://doi.org/10.1007/978-3-031-21448-6↩︎"
  },
  {
    "objectID": "altair/index.html",
    "href": "altair/index.html",
    "title": "Anthology of Data Science",
    "section": "",
    "text": "Visualization curriculum with Altair\n\n\nA data visualization curriculum of interactive notebooks, using Vega-Lite and Altair.\n\n\n\nJeffrey Heer, Dominik Moritz, Jake VanderPlas, Brock Craft\n\n\nMar 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComet charts in Python\n\n\nVisualizing statistical mix effects and Simpson’s paradox with Altair\n\n\n\nDaniel Kapitan\n\n\nJan 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAltair’s data types\n\n\nUnderstanding your data is critical in creating visualizations. This video outlines Altair’s data types and explains how they can influence the visualization process.\n\n\n\nEitan Lees\n\n\nAug 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAltair’s visualization grammar\n\n\nThis video outlines the visualization grammar Altair is built on. Understanding the ways in which the elements of the visualization grammar interact is important when using…\n\n\n\nEitan Lees\n\n\nAug 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Altair?\n\n\nThis video describes the Python package Altair and the software stack it is built on.\n\n\n\nEitan Lees\n\n\nAug 4, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Anthology of Data Science",
    "section": "",
    "text": "Jake VanderPlas in the Python Data Science Handbook\n\n\n\nPython has emerged over the last couple decades as a first-class tool for scientific computing tasks, including the analysis and visualization of large datasets. This may have come as a surprise to early proponents of the Python language: the language itself was not specifically designed with data analysis or scientific computing in mind. The usefulness of Python for data science stems primarily from the large and active ecosystem of third-party packages, particularly:\n\nNumPy for manipulation of homogeneous array-based data\nPandas for manipulation of heterogeneous and labeled data\nSciPy for common scientific computing tasks, Matplotlib for publication-quality visualizations\nIPython for interactive execution and sharing of code\nScikit-Learn for machine learning"
  },
  {
    "objectID": "python/index.html#sec-how-much-python",
    "href": "python/index.html#sec-how-much-python",
    "title": "Anthology of Data Science",
    "section": "How much Python should I know?",
    "text": "How much Python should I know?\nAs with any other (programming) language, it takes years to master it fluently which is beyond the scope this course. Instead, our objective is to have a working knowledge of Python to be able to learn and apply machine learning. To make this explicit we take the following book and online resources as our point of reference. Prior to starting the Data Science Foundation programme, participants are expected to have mastered the following topics:\n\nA Whirlwind Tour of Python (pages number from the pdf version):\n\nKnow how to install and use Python on your own computer (pages 1 to 13)\nKnow basic semantics of variables, objects and operators (pages 13 to 24)\nKnow built-in simple values and data structures (pages 24 to 37)\nKnow how to use control flow and functions (pages 37 to 45)\nKnow how to iterate and use list comprehensions (pages 52 to 61)\n\nPython Data Science handbook\n\nKnow how to manipulate data with pandas (chapter 3)\n\n\n\n\n\n\n\n\nPCEP™ – Certified Entry-Level Python Programmer\n\n\n\nThe learning path proposed here is similar to the PCEP™ – Certified Entry-Level Python Programmer certification. The PCEP™ certification is a good way to assess your current Python knowledge and to prepare for the Machine Learning Foundation course. The certification is offered by the Python Institute. You may opt to obtain this certificate."
  },
  {
    "objectID": "python/index.html#how-should-i-learn-python",
    "href": "python/index.html#how-should-i-learn-python",
    "title": "Anthology of Data Science",
    "section": "How should I learn Python?",
    "text": "How should I learn Python?\nThis foundational course aims to cater for participants both with and without programming experience. To ensure everyone is able to acquire the level described above, participants are given access to Real Python and are required to obtain a working knowledge of Python through self-study prior to starting the lectures.\n\nFirst-time programmersExperienced programmers\n\n\nIf you have never done any scientific programming before, you can prepare as follows:\n\nComplete the Python Basics with Real Python notebook which includes basic exercises.\nBrowse the sections from A Whirlwind Tour of Python and Python Data Science Handbook as listed in Section 1 to verify you have understood te basics.\n\n\n\nIf you have some experience in scientific programming, for example in R or Matlab, you jump into Python as follows:\n\nread A Whirlwind Tour of Python and chapter 3 of the Python Data Science Handbook\nbrush up skills on Real Python on specific topics."
  },
  {
    "objectID": "python/index.html#which-python-environment-should-i-use",
    "href": "python/index.html#which-python-environment-should-i-use",
    "title": "Anthology of Data Science",
    "section": "Which Python environment should I use?",
    "text": "Which Python environment should I use?\nOptions how to start using Python are listed below. At the very least, make sure you know how to use Deepnote (option 1 listed below). This is the default platform which will be used in class.\n\nOnline environment\nFor those new to Python, it is probably easiest to start with one of these online tools:\n\nDeepnote: there is a generous free-tier. If you decide to upgrade, you can collaborate and share notebooks privately.\nGoogle Colab:\n\nActivate a Google account if you haven’t got one yet. This how-to guide explains how you can link an existing (work) email address to a Google account.\nWork your way through the Colab introduction notebook\n\n\nOnce you have gained some traction, you can move on to install Python on your local machine.\n\n\nLocal environment\nYou can setup your local machine/laptop for data science and machine learning as a follows\n\nInstall Anaconda (recommended package manager for data science).\nInstall Visual Studio Code including the Python extension (recommended integrated development environment (IDE))."
  },
  {
    "objectID": "python/index.html#guidelines-for-using-python-for-data-science",
    "href": "python/index.html#guidelines-for-using-python-for-data-science",
    "title": "Anthology of Data Science",
    "section": "Guidelines for using Python for data science",
    "text": "Guidelines for using Python for data science\nUsing Python for data science is inherently different than using it for, say, building a website. To provide you with some guidance to the many different ways c.q. styles of using Python, please consider the following:\n\nFocus on using existing data science libraries, instead of writing your own basic functions. If you find yourself spending a lot of time reading documentation, you are on the right track.\nTake a functional approach to programming instead of an object-oriented approach. The former is more fitting for data science, where it is common to structure your work in terms of pipelines and think about each processing step as a function. The latter is more suitable for application development.\n\nFor those wanting to further develop their Python skills for data science, the following books are recommended:\n\nPython for Data Analysis 3rd Edition by Wes McKinney, the creator of pandas.\nData Science With Python Core Skills on Real Python provides an extensive learning path.\nHands-On Machine Learning with Scikit-Learn, Keras and Tensorflow (2nd edition) by Aurélien Géron. You will need to purchase the book, but the notebooks with example code are freely available.\nEffective Python: 90 Specific Ways to Write Better Python (second edition)."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Selected papers & blogpost",
    "section": "",
    "text": "Stop aggregating away the signal in your data\n\n\nBy aggregating our data in an effort to simplify it, we lose the signal and the context we need to make sense of what we’re seeing. Originally published on &lt;a…\n\n\n\nZan Armstrong\n\n\nMar 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRIP correlation. Introducing the Predictive Power Score.\n\n\nWe define the Predictive Power Score (PPS), an alternative to the correlation that finds more patterns in your data. Originally published on &lt;a…\n\n\n\nFlorian Wetschoreck\n\n\nApr 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nThe Precision-Recall Plot Is More Informative than the ROC Plot\n\n\nAn introduction to performance metrics for binary classification. Originally published on &lt;a…\n\n\n\nPaul van der Laken, Daniel Kapitan\n\n\nAug 16, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/stop-aggregating-signal.html",
    "href": "posts/stop-aggregating-signal.html",
    "title": "Stop aggregating away the signal in your data",
    "section": "",
    "text": "For five years as a data analyst, I forecasted and analyzed Google’s revenue. For six years as a data visualization specialist, I’ve helped clients and colleagues discover new features of the data they know best. Time and time again, I’ve found that by being more specific about what’s important to us and embracing the complexity in our data, we can discover new features in that data. These features can lead us to ask better data-driven questions that change how we analyze our data, the parameters we choose for our models, our scientific processes, or our business strategies.\nMy colleagues Ian Johnson, Mike Freeman, and I recently collaborated on a series of data-driven stories about electricity usage in Texas and California to illustrate best practices of Analyzing Time Series Data. We found ourselves repeatedly changing how we visualized the data to reveal the underlying signals, rather than treating those signals as noise by following the standard practice of aggregating the hourly data to days, weeks, or months.Behind many of the best practices we recommended for time series analysis was a deeper theme: actually embracing the complexity of the data.\nAggregation is the standard best practice for analyzing time series data, but it can create problems by stripping away crucial context so that you’re not even aware of how much potential insight you’ve lost. In this article, I’ll start by discussing how aggregation can be problematic, before walking through three specific alternatives to aggregation with before/after examples that illustrate:"
  },
  {
    "objectID": "posts/stop-aggregating-signal.html#what-is-the-problem-with-aggregation",
    "href": "posts/stop-aggregating-signal.html#what-is-the-problem-with-aggregation",
    "title": "Stop aggregating away the signal in your data",
    "section": "What is the problem with aggregation?",
    "text": "What is the problem with aggregation?\nWe praise the importance of large, rich datasets when we talk about algorithms and teaching machines to learn from data. However, too often when we visualize data so that we as humans can make sense of it, especially time series data, we make the data smaller and smaller.\nAggregation is the default for a reason. It can feel overwhelming to handle the quantities of data we now have at our fingertips. It doesn’t have to be very “big data” to have more than 1M data points, more than the number of pixels on a basic laptop screen. There are many robust statistical approaches to effective aggregation and aggregation that can provide valuable context (comparing to median, for example). In other cases, we need to see more details while trying to find the key insight, but once we’ve finished the analysis and know which features of the data matter most, then aggregation can be a useful tool for focusing attention to communicate that insight.\nBut every time you aggregate, you make a decision about which features of your data matter and which ones you are willing to drop: which are the signal and which are the noise. When you smooth out a line chart, are you doing it because you’ve decided that the daily average is most important and that you don’t care about the distribution or seasonal variation in your peak usage hours? Or are you doing it because it’s the only way you know how to make the jagged lines in your chart go away?\nInformed aggregation simplifies and prioritizes. Uninformed aggregation means you’ll never know what insights you lost.\nIn our rush to aggregate, we sometimes forget that the numbers are tied to real things. To people’s actions. To the hourly, daily, weekly, monthly, and seasonal patterns that are so familiar that they’re almost forgettable. Or maybe it’s that we so rarely see disaggregated data presented effectively in practice that we don’t even realize it’s an option. By considering these seasonal patterns, these human factors, we could embrace complexity in more meaningful ways.\nConsider how much energy we use. If we take a moment to think about it, it’s obvious that we use a lot more energy in the late afternoon than the early morning, so we’d expect big dips and troughs every day. It also shouldn’t be a surprise that the daily energy usage patterns on a summer day and a winter day are different. These patterns aren’t noise, but rather are critical to making sense of this data. We especially need this context to tell what is expected and what is noteworthy.\nHowever, when our dataset has big, regular fluctuations from day to day or hour to hour, our line charts end up looking like an overwhelming mess of jagged lines. Consider this chart showing the 8,760 data points representing one year of data on hourly energy use in California.\n\nA standard way to deal with this overwhelming chart is to apply a moving average by day, week, or month (defined as a four-week period).\n\nYes, now we have a simple chart, and can easily see that the lowest energy usage is in April, and the peak in late August. But we could see that in the first chart. Moreover, we’ve smoothed out anything else of interest. We’ve thrown away so much information that we don’t even know what we’ve lost.\nIs this annual pattern, with a dip in April and a peak in August, consistent for all hours of the day? Do some hours of day or days of week change more than others through the seasons? Were there any hours, days, or weeks that were unusual for their time of year/time of day? What are the outliers? Is energy usage equally variable through all times of year, or are some weeks/seasons/hours more consistent than others?\nDespite starting with data that should contain the answers to these questions, we can’t answer them. Moreover, the smoothed line doesn’t even give us any hints about what questions to ask or what’s worth digging into more."
  },
  {
    "objectID": "posts/stop-aggregating-signal.html#solution-embrace-the-complexity-by-rearranging-augmenting-and-using-the-data-itself-to-provide-context.",
    "href": "posts/stop-aggregating-signal.html#solution-embrace-the-complexity-by-rearranging-augmenting-and-using-the-data-itself-to-provide-context.",
    "title": "Stop aggregating away the signal in your data",
    "section": "Solution: Embrace the complexity by rearranging, augmenting, and using the data itself to provide context.",
    "text": "Solution: Embrace the complexity by rearranging, augmenting, and using the data itself to provide context.\n\n#1: Don’t aggregate: Rearrange\nWhat if we considered what categories likely matter based on what we know of human behavior and environmental factors, especially temperature? Factors like time of day and time of year? In Discovering Data Patterns, we grouped the data into 96 small, aligned tick charts, one for each hour of the day for each season of the year and organized the visualization around the concepts most likely to matter. The x-axis for each mini chart is the amount of electricity used, and each tick mark represents a single hour on a specific day.\n\nThis way we can immediately see what’s typical or unusual for each hour and quarter. For example, generally more energy is used at midnight in winter than at 3am. Skimming down a column, we can see the shape of a day for each season. And, we can see how energy demand by hour changes across seasons by comparing each column to the next.\nNow the “noise” has become the signal. We can clearly answer the questions we posed above:\n\nIs this annual pattern consistent for all hours of the day?\n\nNo, the “shape” of energy used during the course of the day is different in winter vs. summer, with a double peak in Q1 and a single peak in Q3. Also, Q4 looks a lot like Q1, except for a few unusual days. And Q2 shows the most variability in “shape” of day.\n\nDo some hours of day or days of week change more than other hours through the seasons?\n\nYes, late afternoon and evening hours show much more of an increase in energy usage from Q1 to Q3 than early morning hours.\n\nWere there any hours, days, or weeks that were unusual for their time of year/time of day?\n\nYes. For example, in Q4 some very unusual days saw high energy usage in the evening.\nYes. In Q3 in the early morning hours (between 4am and 6am), there were some outlier days with much higher energy usage.\n\nIs energy usage equally variable through all times of year, or are some weeks/seasons/hours more consistent than others?\n\nNo! Q1 has very more consistent energy usage, with a very narrow range of energy used for any particular hour of the day. Meanwhile, Q2 shows very inconsistent energy usage, with a lot of variability especially in the higher-energy evening hours.\n\n\nNot only do we notice some patterns immediately, but this view of the data also gives us the chance to go deeper just by looking more closely (and doing some basic research into what was going on in California at that time).\nLet’s look closer at the early morning hours of Q3. There were some abnormally high values between 4pm and 6pm. Interactive tooltips reveal that these took place on August 19. A quick Google search for “California Aug 19th 2020” shows that the region was suffering from wildfires, so perhaps people kept windows closed and the AC on instead of opening their windows to the cooler nighttime air. September 6 also shows up among the highest values, and a search indicates a likely cause: a record-breaking heat wave in California that hit the national news while the fires continued to burn.\n\nOverall, our faceted tick heatmap chart has the same number of data points as the original jagged line, but now we can see the underlying daily and seasonal patterns (and how daily patterns vary by season) as well as the relative outliers. The more time we spend with the chart, the more we notice, as it invites us to ask new data-driven questions.\n\n\n#2: Augment first, then group or color.\n\nBring in common knowledge: augment with familiar classifications\n\nAt another point in our exploratory analysis, we looked at a chart showing 52 weeks of hourly energy usage in California (shown above) and noticed that the higher-energy weeks seemed to have a single bump each day in the evening while lower-energy weeks seemed to have more of a double bump (see above). This is actually the same pattern revealed in section #1 on rearranging.\nWe guessed that the single bump/double hump might be related to seasonal differences in temperatures. To test this hypothesis we added a column to the dataset to designate “summer” vs. “winter,” and then made two charts (faceted) by splitting data on that parameter. Suddenly it was obvious. No longer were we squinting to notice a pattern hidden in the squiggles.\n\nThe “faceting” itself was simple, a feature built into many charting APIs including Observable Plot (which we were using to visualize our data). In hindsight, this seems like an obvious way to split the data. But how often do we step back and actually augment our data with these human-relevant concepts? The key was having the summer/winter parameter to facet on. It doesn’t have to be perfect. Guessing at a date boundary for summer/winter is enough to see that a distinct pattern emerges. Once we have the double-bump/single-bump insight visible here, we can use that insight to go back and look at our data more closely. For example, it appears that there are some daily “double-bump” weeks in the “summer.” Are those boundary weeks that should be classified as winter (or fall or spring)? Or are they unusual weeks during the summer? Moreover, now that we know a defining signal, we could use that signal to classify the data and thereby use the data to identify when energy usage transitions from a “summer” pattern to a “winter” pattern.\n\n\nAugment with data-driven classifications\nThis line chart shows the daily energy use by a single household in Atlanta from March through July 2021. What do you notice? Lots of spikes? Higher energy use in the summer months?\n\nSwitching to a scatterplot makes it more obvious that there are normal-energy days and also high-energy days. Drawing in a line for the moving average plus a (5kwh) buffermakes this split between “normal” and “high-energy” days more clear and shows that the gap is maintained even as energy use overall increases in the summer months.\n\nNow that our exploratory data analysis has revealed two distinct categories (normal and high-energy), we can augment our data by using the moving average to define which points fall into each category. We can then color the points by those new classifications to make it easier to analyze.\n\nIn this way, we complete the circle: we use visualization to notice a key feature of the data and leverage this insight to further classify our data, making the visualization easier to read. And we can take it a step further, continuing to analyze our data based on this classification by creating a histogram showing the frequency of high-energy vs. normal usage days by month. In this view, we can see that in the summer the amount of energy used on normal days went up, and that there were more high-energy days in June and July than in March and April (even after taking into account that the baseline energy usage also went up). Therefore, we can now say with confidence that overall energy consumption increased for two reasons: (1) baseline energy usage increased and (2) a higher percent of days were high-energy days.\n\nThis pattern of looking, then augmenting, then looking again using the classification can also reveal any issues with our classification, like the high point that occurred on our sixth day of data which is mislabeled because the moving average was not defined until the seventh day (as a trailing moving average). This gives us a chance to improve our classification algorithm.\nWhile this example used a very simple algorithm of “moving average + 5kwh” to classify days as “normal” or “high-energy,” this cycle of “look, augment, look, refine classification” becomes more important for machine learning as our algorithms become more opaque.\n\n\n\n#3: Split your data into foreground and background\n\nSplit based on a time period of interest\nWe also dug into data on energy generated by fuel type in Texas in January and February of 2021, including a critical time period in February leading up to and during the rolling blackouts that were initiated to save the electricity grid from collapse following an unusual winter storm. In the analysis story, my colleague Ian faceted the data, creating a chart for each fuel type. This was quite effective: you can immediately see which fuels make up the bulk of energy in Texas, as well as some of the abnormal patterns in mid-February.\n\nKnowing that the critical time period was around February 7 to February 21, Ian further focused attention on those two weeks by making the weeks before and after partially transparent and adding vertical gridlines. He might have been tempted to delete the data outside the period. After all, why waste space on data outside the time period of interest?\n\nBut it’s that granular background data that helps us understand what is so unusual for each fuel type during the critical time period. For example, in coal we’d notice the dip after February 15 regardless, but we need the data from January to notice how unusual the nearly flat plateau between February 1 and February 15 is. Similarly, the January and late-February data for nuclear shows how steady that fuel source typically is, helping us to understand just how strange the dip that we see after February 15 is.\n\n\n\nSplit by comparing each category of interest to the full dataset\nWhen we want to know if there is a relationship between metric A and metric B, the first step is to create a scatterplot. For example, the scatterplot below shows the outdoor temperature and energy demand in Texas for each hour over the course of a year. It’s immediately clear that there is a strong relationship between temperature and energy use (even though that relationship is also obviously non-linear!).\n\nWhile there is clearly a correlation between temperature and electricity demand, it’s also clear that temperature doesn’t tell the whole story. For any given temperature, there is a roughly 10-15K MWh difference from minimum to maximum energy use. Knowing that in our own homes we crank the thermostat a lot higher on a cold afternoon than on a cold night, we guessed that the hour of day could play a key role in the relationship between temperature and energy use.\nThe standard approach to adding an additional category to a scatterplot is to apply a categorical color, thereby comparing everything to everything (comparing all hours, temperatures, and energy demand in one chart). If we do that, we do see that something is going on. More greens and blues in the top right, more pinks low. But to understand what the colors refer to, you have to look back and forth between the legend and the data a lot. Moreover, it’s impossible to answer a question like, “What’s the relationship between temperature and energy at 10am?” Or, “How does early morning compare to evening?\n\nInstead we can apply two techniques: grouping and splitting the data into foreground and background.\nIn the three charts below, the dots representing 5am, 10am, and 6pm are colored brightly. Meanwhile, the entire dataset is shown in grey in the background. This gives us the context to see the relationship between temp and energy for each hour, and see that in the context of the full dataset.\nBy specifically comparing “5am” to “all other times of day,” we can see that 5am is relatively low energy use regardless of temperature (and temperatures are never very high at 5am). Meanwhile, at 6pm energy use is generally higher at all temperatures.\n10am is in some ways the most interesting: at lower temperatures (in the left half of the graph) the yellow dots are relatively high compared to the grey dots, indicating high energy use relative to other hours of the day at the same temperature. Meanwhile, for high temperatures on the right half of the graph, the yellow dots hug the bottom of the grey area. At hot temperatures, relatively little energy is used at 10am compared to the rest of the day. This type of insight is made possible not just by grouping, but also by using the full “noisy” dataset as a consistent background providing context for all the faceted charts."
  },
  {
    "objectID": "posts/stop-aggregating-signal.html#summary-embrace-the-complexity-of-your-data",
    "href": "posts/stop-aggregating-signal.html#summary-embrace-the-complexity-of-your-data",
    "title": "Stop aggregating away the signal in your data",
    "section": "Summary: Embrace the complexity of your data",
    "text": "Summary: Embrace the complexity of your data\nIn the course of creating the Analyzing Time Series Data collection, Ian Johnson, Mike Freeman, and I employed a range of strategies to embrace the complexity of the data instead of relying on standard methods that aggregate it away. Those frustratingly jagged lines are the signal, not the noise.\nWe embraced complexity by:\n\nRearranging data to compare “like to like.”\nAugmenting our data based on the concepts that we know matter and on what we discovered in the data.\nUsing the larger dataset to provide background context for the data of interest (the foreground).\n\nThese approaches are especially powerful for time series data because the underlying daily, weekly, and seasonal patterns can feel so distracting. In particular, consider how these strategies might power real-time data analysis by putting incoming data in a richer historical context for quick visual pattern-matching to identify normal vs. worrisome patterns. At the same time, these foundational techniques also apply to any data that can feel overwhelming and noisy, like machine learning classifications or data resulting from high-throughput scientific experiments.\nAfter seeing each of these techniques in action, perhaps the next time you are about to aggregate your data in order to simplify it, you might instead try to rearrange, augment, or split your data into foreground/background. See the data in its full context to reveal unexpected patterns and prompt new data-driven questions. Embrace complexity by (literally) changing how you look at your data."
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Example notebooks",
    "section": "",
    "text": "Predicting diabetes with Pima Indians\n\n\n\n\n\n\nDaniel Kapitan\n\n\nNov 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting house prices in Ames, Iowa\n\n\n\n\n\n\nDaniel Kapitan\n\n\nOct 21, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]