[
  {
    "objectID": "lectures/composable-data-stack.html",
    "href": "lectures/composable-data-stack.html",
    "title": "Modern, composable and downward-scaleable data platforms",
    "section": "",
    "text": "(view full screen)",
    "crumbs": [
      "Lectures",
      "Modern, composable and downward-scaleable data platforms"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An anthology of open access data science materials",
    "section": "",
    "text": "Machine learning is permeating many fields of work. As a new ‘system technology’1, its impact on organizations and society is expected to be of the same magnitude as that of the steam engine or electricity. As such, more and more professionals are seeking to acquire the necessary understanding and skills to apply machine learning in their day-to-day work. Hence more people without a background in either computer science or statistics - let alone both - have a need for high-quality, open access content to explore and learn data science by themselves.\nNow there is a lot of machine learning learning materials out there, so why this anthology? Based on my experience in teaching professional education course on data & AI, I am continouly challenged to:\n\ncurate content for different professional learning paths, combining various existing open access materials that can be readily shared and thus contribute to the democratization of know-how in this field of work;\nfinding a balance between too technical vs. too vague, handwaving or even downright wrong;\ntake a hands-on, problem-based approach. Rather than, say, explaining the principles that underlie regularization, we choose to demonstrate these principles using the simplest algorithms. With a little math, everyone should be able to understand how LASSO performs regularization for regression models. With this intuitive understanding, you can move on to more complex algorithms and applications, and reason where and how to use regularization."
  },
  {
    "objectID": "index.html#why-this-anthology",
    "href": "index.html#why-this-anthology",
    "title": "An anthology of open access data science materials",
    "section": "",
    "text": "Machine learning is permeating many fields of work. As a new ‘system technology’1, its impact on organizations and society is expected to be of the same magnitude as that of the steam engine or electricity. As such, more and more professionals are seeking to acquire the necessary understanding and skills to apply machine learning in their day-to-day work. Hence more people without a background in either computer science or statistics - let alone both - have a need for high-quality, open access content to explore and learn data science by themselves.\nNow there is a lot of machine learning learning materials out there, so why this anthology? Based on my experience in teaching professional education course on data & AI, I am continouly challenged to:\n\ncurate content for different professional learning paths, combining various existing open access materials that can be readily shared and thus contribute to the democratization of know-how in this field of work;\nfinding a balance between too technical vs. too vague, handwaving or even downright wrong;\ntake a hands-on, problem-based approach. Rather than, say, explaining the principles that underlie regularization, we choose to demonstrate these principles using the simplest algorithms. With a little math, everyone should be able to understand how LASSO performs regularization for regression models. With this intuitive understanding, you can move on to more complex algorithms and applications, and reason where and how to use regularization."
  },
  {
    "objectID": "index.html#what-is-the-best-online-course-to-get-into-data-science-ai",
    "href": "index.html#what-is-the-best-online-course-to-get-into-data-science-ai",
    "title": "An anthology of open access data science materials",
    "section": "What is the best online course to get into data science & AI?",
    "text": "What is the best online course to get into data science & AI?\nFor now, my recommendation is to start with one of the courses offered by Elements of AI.\n\nIntroduction to AIBuilding AI\n\n\n\nAn Introduction to AI is a free online course for everyone interested in learning what AI is, what is possible (and not possible) with AI, and how it affects our lives – with no complicated math or programming required.\n\n\n\nBuilding AI is a free online course where you’ll learn about the actual algorithms that make creating AI methods possible. Some basic Python programming skills are recommended to get the most out of the course."
  },
  {
    "objectID": "index.html#how-is-this-anthology-set-up",
    "href": "index.html#how-is-this-anthology-set-up",
    "title": "An anthology of open access data science materials",
    "section": "How is this anthology set up?",
    "text": "How is this anthology set up?\n\nBooks\nA selection of what I think are the best open access textbooks is provided as a digital bookshelf. All the books included are open access, thanks to their respective authors! They should keep you busy for a while.\n\n\nLectures\nI am gradually rewriting my lectures as Revealjs presentations. More will be added over time.\n\n\nPerspectives\nVarious topics and perspectives on how data science is done. Clearly this is an opinionated and very personal selection. It includes a section on getting into Python, where I have curated a selection of books and online tutorials to get you going. Also, a selection of tutorials and blogpost is provided on creating visualizations with Altair. I am a big fan of the Vega-Altair ecosystem for data visualization, because it not only helps me in creating appealing, interactive visualizations, but it also helps me to reason about the data when I am doing exploratory data analysis.\n\n\nUse cases\nA selection of inspiring use cases across various industries, which I think are a demonstration of good practices.\n\n\nNotebooks\nNotebooks for those seeking hands-on examples and explainers."
  },
  {
    "objectID": "index.html#report-an-issue",
    "href": "index.html#report-an-issue",
    "title": "An anthology of open access data science materials",
    "section": "Report an issue",
    "text": "Report an issue\nThis anthology was born out of a need to scratch my own itch, namely to have a central repository where I can continously collate interesting resources and keep the content up-to-date as the best practices evolve. I hope it is also helpful for those seeking a stepping stone into the wonderful field of data science. Feel free to drop me a note with suggestions or feedback by reporting an issue."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "An anthology of open access data science materials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSheikh et al. (2023), Mission AI: the New System Technology, https://doi.org/10.1007/978-3-031-21448-6↩︎"
  },
  {
    "objectID": "notebooks/pima-indians/index.html#objectives",
    "href": "notebooks/pima-indians/index.html#objectives",
    "title": "Predicting diabetes with pycaret",
    "section": "Objectives",
    "text": "Objectives\n\nExample end-to-end supervised learning workflow with Pima Indians\nFocus on conceptual understanding of machine learning\nDemonstrate use of Predictive Power Score (PPS)\nDemonstrate capabilities of low-code tools\nDemonstrate use of average_precision_score (link)\nDemonstrate SHAP values",
    "crumbs": [
      "Notebooks",
      "Predicting diabetes with pycaret"
    ]
  },
  {
    "objectID": "notebooks/pima-indians/index.html#attribution",
    "href": "notebooks/pima-indians/index.html#attribution",
    "title": "Predicting diabetes with pycaret",
    "section": "Attribution",
    "text": "Attribution\n\nDataset\n\nPima Indians paper (original paper)\nKaggle datacard (link)\n\n\n\nPython libraries\n\nAltair (docs)\nydata-profiling (docs)\nPredictive Power Score (PPS, GitHub, blog)\nPyCaret: open-source, low-code machine learning library in Python that automates machine learning workflows (link)\n\n\nimport altair as alt\nimport pandas as pd\nimport ppscore as pps\nfrom pycaret.classification import *\nimport shap\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import train_test_split\nfrom ydata_profiling import ProfileReport\n\n\n# customize Altair\ndef y_axis():\n    return {\n        \"config\": {\n            \"axisX\": {\"grid\": False},\n            \"axisY\": {\n                \"domain\": False,\n                \"gridDash\": [2, 4],\n                \"tickSize\": 0,\n                \"titleAlign\": \"right\",\n                \"titleAngle\": 0,\n                \"titleX\": -5,\n                \"titleY\": -10,\n            },\n            \"view\": {\n                \"stroke\": \"transparent\",\n                # To keep the same height and width as the default theme:\n                \"continuousHeight\": 300,\n                \"continuousWidth\": 400,\n            },\n        }\n    }\n\n\nalt.themes.register(\"y_axis\", y_axis)\nalt.themes.enable(\"y_axis\");",
    "crumbs": [
      "Notebooks",
      "Predicting diabetes with pycaret"
    ]
  },
  {
    "objectID": "notebooks/pima-indians/index.html#read-and-explore-the-data",
    "href": "notebooks/pima-indians/index.html#read-and-explore-the-data",
    "title": "Predicting diabetes with pycaret",
    "section": "Read and explore the data",
    "text": "Read and explore the data\n\n%%time\ndf = pd.read_csv(\"diabetes.csv\").astype({\"Outcome\": bool})\ntrain, test = train_test_split(df, test_size=0.3)\nprofile = ProfileReport(train, minimal=True, title=\"Pima Indians Profiling Report\")\nprofile.to_file(\"pima-indians-profiling-report-minimal.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 5.54 s, sys: 184 ms, total: 5.72 s\nWall time: 1.7 s\n\n\n\nprofile.to_notebook_iframe()",
    "crumbs": [
      "Notebooks",
      "Predicting diabetes with pycaret"
    ]
  },
  {
    "objectID": "notebooks/pima-indians/index.html#investigate-features-with-largest-predictive-power",
    "href": "notebooks/pima-indians/index.html#investigate-features-with-largest-predictive-power",
    "title": "Predicting diabetes with pycaret",
    "section": "Investigate features with largest predictive power",
    "text": "Investigate features with largest predictive power\nWe use the Predictive Power Score to evaluate which features have the highest predictive power with respect to Outcome.\n\npredictors = (\n    pps.predictors(train, \"Outcome\")\n    .round(3)\n    .iloc[:, :-1]\n)\nbase = (\n    alt.Chart(predictors)\n    .encode(\n        y=alt.Y(\"x:N\").sort(\"-x\"),\n        x=\"ppscore\",\n        tooltip=[\"x\", \"ppscore\"],\n    )\n)\nbase.mark_bar() + base.mark_text(align=\"center\", dy=-5)",
    "crumbs": [
      "Notebooks",
      "Predicting diabetes with pycaret"
    ]
  },
  {
    "objectID": "notebooks/pima-indians/index.html#investigate-colinearity",
    "href": "notebooks/pima-indians/index.html#investigate-colinearity",
    "title": "Predicting diabetes with pycaret",
    "section": "Investigate colinearity",
    "text": "Investigate colinearity\n\npps.matrix(train)\n\n\n\n\n\n\n\n\nx\ny\nppscore\ncase\nis_valid_score\nmetric\nbaseline_score\nmodel_score\nmodel\n\n\n\n\n0\nPregnancies\nPregnancies\n1.000000\npredict_itself\nTrue\nNone\n0.000000\n1.000000\nNone\n\n\n1\nPregnancies\nGlucose\n0.000000\nregression\nTrue\nmean absolute error\n23.651769\n24.024994\nDecisionTreeRegressor()\n\n\n2\nPregnancies\nBloodPressure\n0.000000\nregression\nTrue\nmean absolute error\n12.748603\n12.961633\nDecisionTreeRegressor()\n\n\n3\nPregnancies\nSkinThickness\n0.000000\nregression\nTrue\nmean absolute error\n13.467412\n13.630828\nDecisionTreeRegressor()\n\n\n4\nPregnancies\nInsulin\n0.000000\nregression\nTrue\nmean absolute error\n79.240223\n85.015086\nDecisionTreeRegressor()\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n76\nOutcome\nInsulin\n0.000000\nregression\nTrue\nmean absolute error\n79.240223\n84.802415\nDecisionTreeRegressor()\n\n\n77\nOutcome\nBMI\n0.035133\nregression\nTrue\nmean absolute error\n5.672812\n5.473511\nDecisionTreeRegressor()\n\n\n78\nOutcome\nDiabetesPedigreeFunction\n0.000000\nregression\nTrue\nmean absolute error\n0.226384\n0.234531\nDecisionTreeRegressor()\n\n\n79\nOutcome\nAge\n0.000000\nregression\nTrue\nmean absolute error\n8.888268\n8.997094\nDecisionTreeRegressor()\n\n\n80\nOutcome\nOutcome\n1.000000\npredict_itself\nTrue\nNone\n0.000000\n1.000000\nNone\n\n\n\n\n81 rows × 9 columns\n\n\n\n\npps_matrix = (\n    pps.matrix(\n        train.loc[:, predictors.query(\"ppscore &gt; 0\")[\"x\"].tolist()],\n    )\n    .loc[:, [\"x\", \"y\", \"ppscore\"]]\n    .round(3)\n)\n(\n    alt.Chart(pps_matrix)\n    .mark_rect()\n    .encode(\n        x=\"x:O\",\n        y=\"y:O\",\n        color=\"ppscore:Q\",\n        tooltip=[\"x\", \"y\", \"ppscore\"])\n).properties(width=500, height=500)",
    "crumbs": [
      "Notebooks",
      "Predicting diabetes with pycaret"
    ]
  },
  {
    "objectID": "notebooks/pima-indians/index.html#build-models",
    "href": "notebooks/pima-indians/index.html#build-models",
    "title": "Predicting diabetes with pycaret",
    "section": "Build models",
    "text": "Build models\n\ncls = setup(data = train, \n             target = 'Outcome',\n             numeric_imputation = 'mean',\n             feature_selection = False,\n             pca=False,\n             remove_multicollinearity=True,\n             remove_outliers = False,\n             normalize = True,\n             )\nadd_metric('apc', 'APC', average_precision_score, target = 'pred_proba');\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n5752\n\n\n1\nTarget\nOutcome\n\n\n2\nTarget type\nBinary\n\n\n3\nOriginal data shape\n(537, 9)\n\n\n4\nTransformed data shape\n(537, 9)\n\n\n5\nTransformed train set shape\n(375, 9)\n\n\n6\nTransformed test set shape\n(162, 9)\n\n\n7\nNumeric features\n8\n\n\n8\nPreprocess\nTrue\n\n\n9\nImputation type\nsimple\n\n\n10\nNumeric imputation\nmean\n\n\n11\nCategorical imputation\nmode\n\n\n12\nRemove multicollinearity\nTrue\n\n\n13\nMulticollinearity threshold\n0.900000\n\n\n14\nNormalize\nTrue\n\n\n15\nNormalize method\nzscore\n\n\n16\nFold Generator\nStratifiedKFold\n\n\n17\nFold Number\n10\n\n\n18\nCPU Jobs\n-1\n\n\n19\nUse GPU\nFalse\n\n\n20\nLog Experiment\nFalse\n\n\n21\nExperiment Name\nclf-default-name\n\n\n22\nUSI\n10e4\n\n\n\n\n\n\n%%time\nbest_model = compare_models(include=[\"et\", \"lightgbm\", \"rf\", \"dt\"], sort=\"APC\")\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nAPC\nTT (Sec)\n\n\n\n\net\nExtra Trees Classifier\n0.7569\n0.8039\n0.5090\n0.6832\n0.5719\n0.4106\n0.4261\n0.6795\n0.1740\n\n\nrf\nRandom Forest Classifier\n0.7412\n0.7893\n0.5013\n0.6420\n0.5538\n0.3784\n0.3896\n0.6661\n0.0330\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.7252\n0.7776\n0.5090\n0.5987\n0.5452\n0.3519\n0.3574\n0.6378\n0.1310\n\n\ndt\nDecision Tree Classifier\n0.6691\n0.6275\n0.5019\n0.5002\n0.4904\n0.2503\n0.2559\n0.4269\n0.0050\n\n\n\n\n\n\n\n\nCPU times: user 537 ms, sys: 135 ms, total: 672 ms\nWall time: 3.96 s",
    "crumbs": [
      "Notebooks",
      "Predicting diabetes with pycaret"
    ]
  },
  {
    "objectID": "notebooks/pima-indians/index.html#evaluation",
    "href": "notebooks/pima-indians/index.html#evaluation",
    "title": "Predicting diabetes with pycaret",
    "section": "Evaluation",
    "text": "Evaluation\n\npredictions = (\n    predict_model(best_model, data=test.iloc[:, :-1])\n)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nprediction_label\nprediction_score\n\n\n\n\n752\n3\n108\n62\n24\n0\n26.000000\n0.223\n25\n0\n0.76\n\n\n295\n6\n151\n62\n31\n120\n35.500000\n0.692\n28\n0\n0.57\n\n\n532\n1\n86\n66\n52\n65\n41.299999\n0.917\n29\n0\n0.88\n\n\n426\n0\n94\n0\n0\n0\n0.000000\n0.256\n25\n0\n0.89\n\n\n68\n1\n95\n66\n13\n38\n19.600000\n0.334\n25\n0\n0.97\n\n\n\n\n\n\n\n\nevaluate_model(best_model)",
    "crumbs": [
      "Notebooks",
      "Predicting diabetes with pycaret"
    ]
  },
  {
    "objectID": "notebooks/pima-indians/index.html#shap",
    "href": "notebooks/pima-indians/index.html#shap",
    "title": "Predicting diabetes with pycaret",
    "section": "SHAP",
    "text": "SHAP\n\ninterpret_model(best_model)\n\n\n\n\n\n\n\n\n\ninterpret_model(best_model, plot=\"reason\", observation=1)\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\ninterpret_model(best_model, plot=\"reason\")\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.",
    "crumbs": [
      "Notebooks",
      "Predicting diabetes with pycaret"
    ]
  },
  {
    "objectID": "notebooks/deconfounding/index.html",
    "href": "notebooks/deconfounding/index.html",
    "title": "Deconfounding explained",
    "section": "",
    "text": "The original material for this demonstration was written in R by Jeroen de Mast. His original code was ported to Python by Daniel Kapitan.",
    "crumbs": [
      "Notebooks",
      "Deconfounding explained"
    ]
  },
  {
    "objectID": "notebooks/deconfounding/index.html#credits",
    "href": "notebooks/deconfounding/index.html#credits",
    "title": "Deconfounding explained",
    "section": "",
    "text": "The original material for this demonstration was written in R by Jeroen de Mast. His original code was ported to Python by Daniel Kapitan.",
    "crumbs": [
      "Notebooks",
      "Deconfounding explained"
    ]
  },
  {
    "objectID": "notebooks/deconfounding/index.html#setting-the-scene",
    "href": "notebooks/deconfounding/index.html#setting-the-scene",
    "title": "Deconfounding explained",
    "section": "Setting the scene",
    "text": "Setting the scene\nSuppose that we want to test whether \\(X\\) has a causal effect on \\(Y\\):\n\\[X \\longrightarrow Y\\]\nAnd also we have 1000 \\((X, Y)\\) tuples as our data and that we want to build a regressions model.\n\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n\n# setting up our experiment\nnp.random.sample(1973)\nN = 1000\nC = np.random.normal(loc=0.0, scale=1.0, size=N)\nerror_x = np.random.normal(loc=0.0, scale=1.0, size=N)\nerror_y = np.random.normal(loc=0.0, scale=0.01, size=N)\nX = 10 + 5*C + error_x\nY = 1 + 0.5*C + error_y\ndf = pd.DataFrame({'X': X, 'Y': Y, 'C': C})\n\n\nconfounded = smf.ols(\"Y ~ X\", data=df).fit()\nconfounded.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nY\nR-squared:\n0.960\n\n\nModel:\nOLS\nAdj. R-squared:\n0.960\n\n\nMethod:\nLeast Squares\nF-statistic:\n2.385e+04\n\n\nDate:\nThu, 21 Dec 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n23:47:40\nLog-Likelihood:\n854.10\n\n\nNo. Observations:\n1000\nAIC:\n-1704.\n\n\nDf Residuals:\n998\nBIC:\n-1694.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0446\n0.007\n6.389\n0.000\n0.031\n0.058\n\n\nX\n0.0958\n0.001\n154.420\n0.000\n0.095\n0.097\n\n\n\n\n\n\n\n\nOmnibus:\n1.331\nDurbin-Watson:\n1.855\n\n\nProb(Omnibus):\n0.514\nJarque-Bera (JB):\n1.406\n\n\nSkew:\n0.080\nProb(JB):\n0.495\n\n\nKurtosis:\n2.911\nCond. No.\n24.2\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nSo our first (confounded) model yields a result that \\(Y = 0.03 + 0.1X\\). Note there can be small differences each time you re-run this notebook. But most importantly the fitted model has a high \\(R^2 = 0.95\\) and high significance \\(p = 0.0\\)!\nHowever, if you look closely at the Python code, you see that the real model has a confounder \\(C\\):\n\\[C \\longrightarrow X\\] \\[C \\longrightarrow Y\\]\nIn other words, X and Y are both causally affected by C. As a consequence, X and Y are correlated, but they do not causally affect each other. So, the regression analysis above is actually wrong, and the correlation between X and Y is called spurious. C is called a confounder.\nNow here is the great deconfounding trick: suppose that we include both X and C in the regression analysis and fit the following modelL\n\\[ Y = \\beta_0 + \\beta_1 X + \\beta_2 C + ϵ\\]\n\ndeconfounded = smf.ols(\"Y ~ X + C\", data=df).fit()\ndeconfounded.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nY\nR-squared:\n1.000\n\n\nModel:\nOLS\nAdj. R-squared:\n1.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n1.261e+06\n\n\nDate:\nThu, 21 Dec 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n23:47:40\nLog-Likelihood:\n3164.9\n\n\nNo. Observations:\n1000\nAIC:\n-6324.\n\n\nDf Residuals:\n997\nBIC:\n-6309.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.0055\n0.003\n323.164\n0.000\n0.999\n1.012\n\n\nX\n-0.0005\n0.000\n-1.671\n0.095\n-0.001\n9.05e-05\n\n\nC\n0.5023\n0.002\n316.780\n0.000\n0.499\n0.505\n\n\n\n\n\n\n\n\nOmnibus:\n2.983\nDurbin-Watson:\n2.042\n\n\nProb(Omnibus):\n0.225\nJarque-Bera (JB):\n3.073\n\n\nSkew:\n-0.063\nProb(JB):\n0.215\n\n\nKurtosis:\n3.240\nCond. No.\n122.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNote that, by including \\(C\\) as an independent variable in the regression analysis, suddenly X has stopped being significant (p=0.36)!\nThis holds in general: if the true causal relationships are as given in the second diagram, then including the confounder C in the regression analysis gives the direct effect of X onto Y (if any such direct effect exists), and the part of the correlation that is induced by the confounder C is now entirely attributed to C and not to X. This approach is called “deconfounding”.",
    "crumbs": [
      "Notebooks",
      "Deconfounding explained"
    ]
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Anthology of Data Science",
    "section": "",
    "text": "Modern, composable and downward-scaleable data platforms\n\n\nSlides from a 3-hour guest lecture given at Strathmore University.\n\n\n\nDaniel Kapitan\n\n\nJul 2, 2024\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Lectures"
    ]
  },
  {
    "objectID": "posts/python/whirlwind.html",
    "href": "posts/python/whirlwind.html",
    "title": "A Whirlwind Tour of Python",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python",
      "A Whirlwind Tour of Python"
    ]
  },
  {
    "objectID": "posts/python/effective-python-and-idiomatic-pandas.html",
    "href": "posts/python/effective-python-and-idiomatic-pandas.html",
    "title": "Effective Python and idiomatic pandas",
    "section": "",
    "text": "After you have gained basic working knowledge of Python, it is essential that you continue to develop your skills to write effective Python and use pandas in an idiomatic way. This notebook provides guidelines how to do this. It is very opinionated, and based on a functional programming approach to Python. Functional programming defines a computation using expressions and evaluation—often encapsulated in function definitions. It de-emphasizes or avoids the complexity of state change and mutable objects. This tends to create programs that are more succinct and expressive. Since you can’t easily create purely functional programs in Python, we take a hybrid approach, using functional features where we can add clariy or simplify the code. There are several key features of functional programming that are available in Python which we will use.\nFunctional programming is often a very good fit for data analysis.\n\nUse functions as first-class objects, meaning you can assign functions to variables and pass functions as arguments in other functions allowing for for composability of complex functions for data pipelines using smaller, more simple ones.\nUse of higher-order functions that operate on lists (iterables), like sorted(), min() and max(). NumPy and pandas follow this same principle with so-called universal functions, being functions that operate on ndarrays in an element-by-element fashion, supporting array broadcasting, type casting, and several other standard features.\nFunctional programmes often exploits immutable data structures. Python offers tuples, namedtuples and frozen dataclass as complex but immutable objects. Although NumPy arrays and pandas DataFrames are mutable, we will adhere to the principle of immutable in data pipelines as much as possible.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python",
      "Effective Python and idiomatic pandas"
    ]
  },
  {
    "objectID": "posts/python/effective-python-and-idiomatic-pandas.html#a-functional-approach-to-python",
    "href": "posts/python/effective-python-and-idiomatic-pandas.html#a-functional-approach-to-python",
    "title": "Effective Python and idiomatic pandas",
    "section": "",
    "text": "After you have gained basic working knowledge of Python, it is essential that you continue to develop your skills to write effective Python and use pandas in an idiomatic way. This notebook provides guidelines how to do this. It is very opinionated, and based on a functional programming approach to Python. Functional programming defines a computation using expressions and evaluation—often encapsulated in function definitions. It de-emphasizes or avoids the complexity of state change and mutable objects. This tends to create programs that are more succinct and expressive. Since you can’t easily create purely functional programs in Python, we take a hybrid approach, using functional features where we can add clariy or simplify the code. There are several key features of functional programming that are available in Python which we will use.\nFunctional programming is often a very good fit for data analysis.\n\nUse functions as first-class objects, meaning you can assign functions to variables and pass functions as arguments in other functions allowing for for composability of complex functions for data pipelines using smaller, more simple ones.\nUse of higher-order functions that operate on lists (iterables), like sorted(), min() and max(). NumPy and pandas follow this same principle with so-called universal functions, being functions that operate on ndarrays in an element-by-element fashion, supporting array broadcasting, type casting, and several other standard features.\nFunctional programmes often exploits immutable data structures. Python offers tuples, namedtuples and frozen dataclass as complex but immutable objects. Although NumPy arrays and pandas DataFrames are mutable, we will adhere to the principle of immutable in data pipelines as much as possible.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python",
      "Effective Python and idiomatic pandas"
    ]
  },
  {
    "objectID": "posts/python/effective-python-and-idiomatic-pandas.html#structure-of-notebook",
    "href": "posts/python/effective-python-and-idiomatic-pandas.html#structure-of-notebook",
    "title": "Effective Python and idiomatic pandas",
    "section": "Structure of notebook",
    "text": "Structure of notebook\nThis notebook is structred in a couple of chapters with a number of related items. Feel free to jump between the items. Each item contains concise and specific guidance explaining how you can write effective, functional Python. We have used the following books and online resources, which are recommended for further reading:\n\nEffective Python, 2nd edition, by Brett Slatkin (eBook: EUR 30). Referenced as EP for items that have been taken from this book (which is warmly recommended).\nBlog posts on Modern pandas by Tom Augspurger.\nFunctional Python Programming by Steven F. Lott (eBook: EUR 5).\nIntermediate tutorials for data science on Real Python.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python",
      "Effective Python and idiomatic pandas"
    ]
  },
  {
    "objectID": "posts/python/effective-python-and-idiomatic-pandas.html#pythonic-thinking",
    "href": "posts/python/effective-python-and-idiomatic-pandas.html#pythonic-thinking",
    "title": "Effective Python and idiomatic pandas",
    "section": "Pythonic thinking",
    "text": "Pythonic thinking\n\n#1. Follow the PEP-8 style guide (EP #2)\nPython Enhancement Proposal #8, otherwise known as PEP-8, is the style guide for how to format Python code. You are welcome to write Python code any way you want, as long as it has valid syntax. However, using a consistent style makes your code more approachable and easier to read. Sharing a common style with other Python programmers in the larger community facilitates collaboration on projects. But even if you are the only one who will ever read your code, following the style guide will make it easier for you to change things later, and can help you avoid many common errors.\nPEP-8 provides a wealth of details about how to write clear Python code. It continues to be updated as the Python language evolves. It’s worth reading the whole guide online. Here are a few rules you should be sure to follow.\n\nWhitespace\n\nUse spaces instead of tabs for indentation.\nUse four spaces for each level of syntactically significant indenting.\nLines should be 79 characters in length or less.\nContinuations of long expressions onto additional lines should be indented by four extra spaces from their normal indentation level.\nIn a file, functions and classes should be separated by two blank lines.\nIn a class, methods should be separated by one blank line.\nIn a dictionary, put no whitespace between each key and colon, and put a single space before the corresponding value if it fits on the same line.\nPut one—and only one—space before and after the = operator in a variable assignment.\nFor type annotations, ensure that there is no separation between the variable name and the colon, and use a space before the type information.\n\n\n\nNaming\nPEP 8 suggests unique styles of naming for different parts in the language. These conventions make it easy to distinguish which type corresponds to each name when reading code. Follow these guidelines related to naming:\n\nFunctions, variables, and attributes should be in lowercase_underscore format.\nProtected instance attributes should be in _leading_underscore format.\nPrivate instance attributes should be in __double_leading_underscore format.\nClasses (including exceptions) should be in CapitalizedWord format.\nModule-level constants should be in ALL_CAPS format.\nInstance methods in classes should use self, which refers to the object, as the name of the first parameter.\nClass methods should use cls, which refers to the class, as the name of the first parameter.\n\n\n\nExpressions and Statements\nThe Zen of Python states: “There should be one—and preferably only one—obvious way to do it.” PEP 8 attempts to codify this style in its guidance for expressions and statements:\n\nUse inline negation (if a is not b) instead of negation of positive expressions (if not a is b).\nDon’t check for empty containers or sequences (like [] or '') by comparing the length to zero (if len(somelist) == 0). Use if not somelist and assume that empty values will implicitly evaluate to False.\nThe same thing goes for non-empty containers or sequences (like [1] or 'hi'). The statement if somelist is implicitly True for non-empty values.\nAvoid single-line if statements, for and while loops, and except compound statements. Spread these over multiple lines for clarity.\nIf you can’t fit an expression on one line, surround it with parentheses and add line breaks and indentation to make it easier to read.\nPrefer surrounding multiline expressions with parentheses over using the \\ line continuation character.\n\n\n\nImports\nPEP 8 suggests some guidelines for how to import modules and use them in your code:\n\nAlways put import statements (including from x import y) at the top of a file.\nAlways use absolute names for modules when importing them, not names relative to the current module’s own path. For example, to import the foo module from within the bar package, you should use from bar import foo, not just import foo.\nIf you must do relative imports, use the explicit syntax from . import foo.\nImports should be in sections in the following order:\n\nstandard library modules\nthird-party modules\nyour own modules.\n\nEach subsection should have imports in alphabetical order.\n\n\n\nUse code formatters\nIt is recommended to use a code formatter like pylint or Black to handle these conventions automatically. There is also Jupyter Black for doing this in notebooks.\n\n\n\n#2. Prefer interpolated f-strings over C-style formatting (EP #4)\nBefore f-strings were introduced in Python 3.6, you had to do this to format your strings:\n\nfirst_name = \"Eric\"\nlast_name = \"Idle\"\nage = 74\nprofession = \"comedian\"\naffiliation = \"Monty Python\"\nprint(\n    \"Hello, %s %s. You are %s. You are a %s. You were a member of %s.\"\n    % (first_name, last_name, age, profession, affiliation)\n)\n\nHello, Eric Idle. You are 74. You are a comedian. You were a member of Monty Python.\n\n\nAlthough the str.format() syntax is definitely better, it is still verbose:\n\nprint(\n    \"Hello {} {}, You are a {}. You are a{}. You were a member of {}\".format(\n        first_name, last_name, age, profession, affiliation\n    )\n)\n\nHello Eric Idle, You are a 74. You are acomedian. You were a member of Monty Python\n\n\nWith f-strings this becomes a lot more readable.\n\nprint(\n    f\"Hello {first_name} {last_name}. You are {age}. You are a {profession}. You were a member of {affiliation}\"\n)\n\nHello Eric Idle. You are 74. You are a comedian. You were a member of Monty Python\n\n\nYou can read more about using f-strings in this Real Python tutorial.\n\n\n#3. Write helper functions instead of complex expressions (EP #5)\nPython’s pithy syntax makes it easy to write single-line expressions that implement a lot of logic. For example, say that I want to decode the query string from a URL. Here, each query string parameter represents an integer value:\n\nfrom urllib.parse import parse_qs\n\nmy_values = parse_qs(\"red=5&blue=0&green=\", keep_blank_values=True)\nprint(repr(my_values))\n\n{'red': ['5'], 'blue': ['0'], 'green': ['']}\n\n\nSome query string parameters may have multiple values, some may have single values, some may be present but have blank values, and some may be missing entirely. Using the get (more on that in item 11) method on the result dictionary will return different values in each circumstance:\n\nprint(\"Red:     \", my_values.get(\"red\"))\nprint(\"Green:   \", my_values.get(\"green\"))\nprint(\"Opacity: \", my_values.get(\"opacity\"))\n\nRed:      ['5']\nGreen:    ['']\nOpacity:  None\n\n\nIt’d be nice if a default value of 0 were assigned when a parameter isn’t supplied or is blank. I might choose to do this with Boolean expressions because it feels like this logic doesn’t merit a whole if statement or helper function quite yet.\nPython’s syntax makes this choice all too easy. The trick here is that the empty string, the empty list, and zero all evaluate to False implicitly. Thus, the expressions below will evaluate to the subexpression after the or operator when the first subexpression is False:\n\nred = my_values.get(\"red\", [\"\"])[0] or 0\ngreen = my_values.get(\"green\", [\"\"])[0] or 0\nopacity = my_values.get(\"opacity\", [\"\"])[0] or 0\nprint(f\"Red:     {red!r}\")\nprint(f\"Green:   {green!r}\")\nprint(f\"Opacity: {opacity!r}\")\n\nRed:     '5'\nGreen:   0\nOpacity: 0\n\n\nThe red case works because the key is present in the my_values dictionary. The value is a list with one member: the string '5'. This string implicitly evaluates to True, so red is assigned to the first part of the or expression.\nThe green case works because the value in the my_values dictionary is a list with one member: an empty string. The empty string implicitly evaluates to False, causing the or expression to evaluate to 0.\nThe opacity case works because the value in the my_values dictionary is missing altogether. The behavior of the get method is to return its second argument if the key doesn’t exist in the dictionary (see item 11: “Prefer get over in and KeyError to handle missing dictionary keys”). The default value in this case is a list with one member: an empty string. When opacity isn’t found in the dictionary, this code does exactly the same thing as the green case.\nHowever, this expression is difficult to read, and it still doesn’t do everything I need. I’d also want to ensure that all the parameter values are converted to integers so I can immediately use them in mathematical expressions. To do that, I’d wrap each expression with the int built-in function to parse the string as an integer:\n\nred = int(my_values.get(\"red\", [\"\"])[0] or 0)\ngreen = int(my_values.get(\"green\", [\"\"])[0] or 0)\nopacity = int(my_values.get(\"opacity\", [\"\"])[0] or 0)\nprint(f\"Red:     {red}\")\nprint(f\"Green:   {green}\")\nprint(f\"Opacity: {opacity}\")\n\nRed:     5\nGreen:   0\nOpacity: 0\n\n\nYou see this get unwieldly quite quickly. Furthermore, if you need to reuse this logic repeatedly - even just to or three times, as in this example, - then writing a helper function is the way to go:\n\ndef get_first_int(values, key, default=0):\n    found = values.get(key, [\"\"])\n    if found[0]:\n        return int(found[0])\n    return default\n\n\ngreen = get_first_int(my_values, \"green\")\nprint(f\"Green:   {green!r}\")\n\nGreen:   0\n\n\nAs soon as expressions get complicated, it’s time to consider splitting them into smaller pieces and moving logic into helper functions. What you gain in readability always outweighs what brevity may have afforded you. Avoid letting Python’s pithy syntax for complex expressions from getting you into a mess like this. Follow the DRY principle: Don’t repeat yourself.\n\n\n#4. Prefer multiple assignment unpacking over indexing (EP #6)\nPython also has syntax for unpacking, which allows for assigning multiple values in a single statement. The patterns that you specify in unpacking assignments look a lot like trying to mutate tuples—which isn’t allowed—but they actually work quite differently. For example, if you know that a tuple is a pair, instead of using indexes to access its values, you can assign it to a tuple of two variable names:\n\nitem = (\"Peanut butter\", \"Jelly\")\nfirst, second = item  # Unpacking\nprint(first, \"and\", second)\n\nPeanut butter and Jelly\n\n\nUnpacking has less visual noise than accessing the tuple’s indexes, and it often requires fewer lines. The same pattern matching syntax of unpacking works when assigning to lists, sequences, and multiple levels of arbitrary iterables within iterables. Newcomers to Python may be surprised to learn that unpacking can even be used to swap values in place without the need to create temporary variables. Here, I use typical syntax with indexes to swap the values between two positions in a list as part of an ascending order sorting algorithm (using bubble sort):\n\ndef bubble_sort(a):\n    for _ in range(len(a)):\n        for i in range(1, len(a)):\n            if a[i] &lt; a[i - 1]:\n                temp = a[i]\n                a[i] = a[i - 1]\n                a[i - 1] = temp\n\n\nnames = [\"pretzels\", \"carrots\", \"arugula\", \"bacon\"]\nbubble_sort(names)\nprint(names)\n\n['arugula', 'bacon', 'carrots', 'pretzels']\n\n\nHowever, with unpacking syntax, it’s possible to swap indexes in a single line:\n\ndef bubble_sort(a):\n    for _ in range(len(a)):\n        for i in range(1, len(a)):\n            if a[i] &lt; a[i - 1]:\n                a[i - 1], a[i] = a[i], a[i - 1]  # Swap\n\n\nnames = [\"pretzels\", \"carrots\", \"arugula\", \"bacon\"]\nbubble_sort(names)\nprint(names)\n\n['arugula', 'bacon', 'carrots', 'pretzels']\n\n\nThe way this swap works is that the right side of the assignment (a[i], a[i-1]) is evaluated first, and its values are put into a new temporary, unnamed tuple (such as ('carrots', 'pretzels') on the first iteration of the loops). Then, the unpacking pattern from the left side of the assignment (a[i-1], a[i]) is used to receive that tuple value and assign it to the variable names a[i-1] and a[i], respectively. This replaces 'pretzels' with 'carrots' at index 0 and 'carrots' with 'pretzels' at index 1. Finally, the temporary unnamed tuple silently goes away.\nAnother valuable application of unpacking is in the target list of for loops and similar constructs, such as comprehensions and generator expressions (see item 13: “Use comprehensions instead of map and filter” for those). As an example for contrast, here I iterate over a list of snacks without using unpacking:\n\nsnacks = [(\"bacon\", 350), (\"donut\", 240), (\"muffin\", 190)]\nfor i in range(len(snacks)):\n    item = snacks[i]\n    name = item[0]\n    calories = item[1]\n    print(f\"#{i+1}: {name} has {calories} calories\")\n\n#1: bacon has 350 calories\n#2: donut has 240 calories\n#3: muffin has 190 calories\n\n\nThis works, but it’s noisy. There are a lot of extra characters required in order to index into the various levels of the snacks structure. Here, I achieve the same output by using unpacking along with the enumerate built-in function (see item 5: “Prefer enumerate Over range”):\n\nfor rank, (name, calories) in enumerate(snacks, 1):\n    print(f\"#{rank}: {name} has {calories} calories\")\n\n#1: bacon has 350 calories\n#2: donut has 240 calories\n#3: muffin has 190 calories\n\n\nUsing unpacking wisely will enable you to avoid indexing when possible, resulting in clearer and more Pythonic code. So remember:\n\nPython has special syntax called unpacking for assigning multiple values in a single statement.\nUnpacking is generalized in Python and can be applied to any iterable, including many levels of iterables within iterables.\nReduce visual noise and increase code clarity by using unpacking to avoid explicitly indexing into sequences.\n\n ### #5. Prefer enumerate over range (EP #7)\nWhen you have a data structure to iterate over, like a list of strings, you can loop directly over the sequence:\n\nflavor_list = [\"vanilla\", \"chocolate\", \"pecan\", \"strawberry\"]\nfor flavor in flavor_list:\n    print(f\"{flavor} is delicious\")\n\nvanilla is delicious\nchocolate is delicious\npecan is delicious\nstrawberry is delicious\n\n\nOften, you’ll want to iterate over a list and also know the index of the current item in the list. For example, say that I want to print the ranking of my favorite ice cream flavors. One way to do it is by using range:\n\nfor i in range(len(flavor_list)):\n    flavor = flavor_list[i]\n    print(f\"{i + 1}: {flavor}\")\n\n1: vanilla\n2: chocolate\n3: pecan\n4: strawberry\n\n\nThis looks clumsy compared with the other example of iterating over flavor_list. I have to get the length of the list. I have to index into the array. The multiple steps make it harder to read.\nPython provides the enumerate built-in function to address this situation. enumerate wraps any iterator with a lazy generator (read more on generators in this Real Python article). enumerate yields pairs of the loop index and the next value from the given iterator. Here, I manually advance the returned iterator with the next built-in function to demonstrate what it does:\n\nfor i, flavor in enumerate(flavor_list):\n    print(f\"{i + 1}: {flavor}\")\n\n1: vanilla\n2: chocolate\n3: pecan\n4: strawberry\n\n\nI can make this even shorter by specifying the number from which enumerate should begin counting (1 in this case) as the second parameter:\n\nfor i, flavor in enumerate(flavor_list, 1):\n    print(f\"{i}: {flavor}\")\n\n1: vanilla\n2: chocolate\n3: pecan\n4: strawberry\n\n\n\n\n#6. Use zip to process iterators in parallel (EP #8)\nOften in Python you find yourself with many lists of related objects. List comprehensions make it easy to take a source list and get a derived list by applying an expression (see item 13: “Use comprehensions instead of map and filter”):\n\nnames = [\"Cecilia\", \"Lise\", \"Marie\"]\ncounts = [len(n) for n in names]\nprint(counts)\n\n[7, 4, 5]\n\n\nThe items in the derived list are related to the items in the source list by their indexes. To iterate over both lists in parallel, I can iterate over the length of the names source list:\n\nlongest_name = None\nmax_count = 0\n\nfor i in range(len(names)):\n    count = counts[i]\n    if count &gt; max_count:\n        longest_name = names[i]\n        max_count = count\n\nprint(longest_name)\n\nCecilia\n\n\nThe problem is that this whole loop statement is visually noisy. The indexes into names and counts make the code hard to read. Indexing into the arrays by the loop index i happens twice. Using enumerate (see item 5: “Prefer enumerate Over range”) improves this slightly, but it’s still not ideal:\n\nlongest_name = None\nmax_count = 0\nfor i, name in enumerate(names):\n    count = counts[i]\n    if count &gt; max_count:\n        longest_name = name\n        max_count = count\n\nprint(longest_name)\n\nCecilia\n\n\nTo make this code clearer, Python provides the zip built-in function. zip wraps two or more iterators with a lazy generator. The zip generator yields tuples containing the next value from each iterator. These tuples can be unpacked directly within a for statement (see item 4: “Prefer multiple assignment unpacking over indexing”). The resulting code is much cleaner than the code for indexing into multiple lists:\n\nlongest_name = None\nmax_count = 0\nfor name, count in zip(names, counts):\n    if count &gt; max_count:\n        longest_name = name\n        max_count = count\n\nprint(longest_name)\n\nCecilia\n\n\nzip consumes the iterators it wraps one item at a time, which means it can be used with infinitely long inputs without risk of a program using too much memory and crashing.\nHowever, beware of zip’s behavior when the input iterators are of different lengths. For example, say that I add another item to names above but forget to update counts. Running zip on the two input lists will have an unexpected result:\n\nnames.append(\"Rosalind\")\nfor name, count in zip(names, counts):\n    print(name)\n\nCecilia\nLise\nMarie\n\n\nThe new item for 'Rosalind' isn’t there. Why not? This is just how zip works. It keeps yielding tuples until any one of the wrapped iterators is exhausted. Its output is as long as its shortest input. This approach works fine when you know that the iterators are of the same length, which is often the case for derived lists created by list comprehensions. But in many other cases, the truncating behavior of zip is surprising and bad. If you don’t expect the lengths of the lists passed to zip to be equal, consider using the zip_longest function from the itertools built-in module instead:\n\nimport itertools\n\nfor name, count in itertools.zip_longest(names, counts):\n    print(f\"{name}: {count}\")\n\nCecilia: 7\nLise: 4\nMarie: 5\nRosalind: None\n\n\nzip_longest replaces missing values—the length of the string 'Rosalind' in this case—with whatever fillvalue is passed to it, which defaults to None.\nNote that the itertools implements more iterator building block that come from functional programming. Read more in this Real Python article.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python",
      "Effective Python and idiomatic pandas"
    ]
  },
  {
    "objectID": "posts/python/effective-python-and-idiomatic-pandas.html#lists-dictionairies-and-dataclasses",
    "href": "posts/python/effective-python-and-idiomatic-pandas.html#lists-dictionairies-and-dataclasses",
    "title": "Effective Python and idiomatic pandas",
    "section": "Lists, dictionairies and dataclasses",
    "text": "Lists, dictionairies and dataclasses\n\n#7. Use dataclass if you need to create a data container\nThe dataclass class was introduced in Python 3.7. Given our functional approach, you should first ask yourself whether you really need a new container object for your data. In the words of Alan J. Perlis:\n\nIt is better to have 100 functions operate on one data structure than to have 10 functions operate on 10 data structures.\n\nIf you have consciously decided you do want a new data object, then dataclass is your friend. It is created using the new @dataclass decorator as follows:\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass DataClassCard:\n    rank: str\n    suit: str\n\nA dataclass comes with basic functionality already implemented. For instance, you can instantiate, print, and compare dataclass instances straight out of the box:\n\nqueen_of_hearts = DataClassCard(\"Q\", \"Hearts\")\nprint(queen_of_hearts.rank)\n\nQ\n\n\n\nqueen_of_hearts\n\nDataClassCard(rank='Q', suit='Hearts')\n\n\n\nqueen_of_hearts == DataClassCard(\"Q\", \"Hearts\")\n\nTrue\n\n\nRead more on dataclass and how to use them in this Real Python tutorial.\n\n\n#8. Prefer catch-all unpacking over slicing (EP #13)\nOne limitation of basic unpacking (see item 4) is that you must know the length of the sequences you’re unpacking in advance. For example, here I have a list of the ages of cars that are being traded in at a dealership. When I try to take the first two items of the list with basic unpacking, an exception is raised at runtime:\n\ncar_ages = [0, 9, 4, 8, 7, 20, 19, 1, 6, 15]\ncar_ages_descending = sorted(car_ages, reverse=True)\noldest, second_oldest = car_ages_descending\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-28-99634ca0e408&gt; in &lt;module&gt;\n      1 car_ages = [0, 9, 4, 8, 7, 20, 19, 1, 6, 15]\n      2 car_ages_descending = sorted(car_ages, reverse=True)\n----&gt; 3 oldest, second_oldest = car_ages_descending\n\nValueError: too many values to unpack (expected 2)\n\n\n\nNewcomers to Python often rely on indexing and slicing for this situation. For example, here I extract the oldest, second oldest, and other car ages from a list of at least two items:\n\noldest = car_ages_descending[0]\nsecond_oldest = car_ages_descending[1]\nothers = car_ages_descending[2:]\nprint(oldest, second_oldest, others)\n\n20 19 [15, 9, 8, 7, 6, 4, 1, 0]\n\n\nHowever, to unpack assignments that contain a starred expression, you must have at least one required part, or else you’ll get a SyntaxError. You can’t use a catch-all expression on its own:\n\n*others = car_ages_descending\n\n\n  File \"&lt;ipython-input-30-77c6f344fe32&gt;\", line 1\n    *others = car_ages_descending\n    ^\nSyntaxError: starred assignment target must be in a list or tuple\n\n\n\n\nYou also can’t use multiple catch-all expressions in a single-level unpacking pattern:\n\nfirst, *middle, *second_middle, last = [1, 2, 3, 4]\n\n\n  File \"&lt;ipython-input-31-77dccc131ad1&gt;\", line 1\n    first, *middle, *second_middle, last = [1, 2, 3, 4]\n    ^\nSyntaxError: multiple starred expressions in assignment\n\n\n\n\nBut it is possible to use multiple starred expressions in an unpacking assignment statement, as long as they’re catch-alls for different parts of the multilevel structure being unpacked. I don’t recommend doing the following (see item 11: “Never unpack more than three variables when functions return multiple values” for related guidance), but understanding it should help you develop an intuition for how starred expressions can be used in unpacking assignments:\n\ncar_inventory = {\n    \"Downtown\": (\"Silver Shadow\", \"Pinto\", \"DMC\"),\n    \"Airport\": (\"Skyline\", \"Viper\", \"Gremlin\", \"Nova\"),\n}\n\n((loc1, (best1, *rest1)), (loc2, (best2, *rest2))) = car_inventory.items()\nprint(f\"Best at {loc1} is {best1}, {len(rest1)} others\")\nprint(f\"Best at {loc2} is {best2}, {len(rest2)} others\")\n\nBest at Downtown is Silver Shadow, 2 others\nBest at Airport is Skyline, 3 others\n\n\nStarred expressions become list instances in all cases. If there are no leftover items from the sequence being unpacked, the catch-all part will be an empty list. This is especially useful when you’re processing a sequence that you know in advance has at least \\(N\\) elements:\n\nshort_list = [1, 2]\nfirst, second, *rest = short_list\nprint(first, second, rest)\n\n1 2 []\n\n\nBut with the addition of starred expressions, the value of unpacking iterators becomes clear. For example, here I have a generator that yields the rows of a CSV file containing all car orders from the dealership this week:\n\ndef generate_csv():\n    yield (\"Date\", \"Make\", \"Model\", \"Year\", \"Price\")\n    for i in range(100):\n        yield (\"2019-03-25\", \"Honda\", \"Fit\", \"2010\", \"$3400\")\n        yield (\"2019-03-26\", \"Ford\", \"F150\", \"2008\", \"$2400\")\n\nProcessing the results of this generator using indexes and slices is fine, but it requires multiple lines and is visually noisy:\n\nall_csv_rows = list(generate_csv())\nheader = all_csv_rows[0]\nrows = all_csv_rows[1:]\nprint(\"CSV Header:\", header)\nprint(\"Row count: \", len(rows))\n\nCSV Header: ('Date', 'Make', 'Model', 'Year', 'Price')\nRow count:  200\n\n\nUnpacking with a starred expression makes it easy to process the first row—the header—separately from the rest of the iterator’s contents. This is much clearer:\n\nit = generate_csv()\nheader, *rows = it\nprint(\"CSV Header:\", header)\nprint(\"Row count: \", len(rows))\n\nCSV Header: ('Date', 'Make', 'Model', 'Year', 'Price')\nRow count:  200\n\n\nKeep in mind, however, that because a starred expression is always turned into a list, unpacking an iterator also risks the potential of using up all of the memory on your computer and causing your program to crash. So you should only use catch-all unpacking on iterators when you have good reason to believe that the result data will all fit in memory.\nFinally, note that the unpacking operators * for lists and ** for dicts is often used in functions. Read this tutorial on Real Python that demystifies *args and **kwargs. Note that args and kwargs are just names that are used by convention to refer to positional arguments and keyword arguments, respectively. You could use any other name if you wanted to. The magic lies in the * and ** unpacking operators, as is shown with this trick from Real Python’s homepage:\n\nx = {\"a\": 1, \"b\": 2}\ny = {\"c\": 3, \"d\": 4}\nz = {**x, **y}\n\nWhen unpacking dicts with overlapping keys, the last value is kept:\n\ny = {\"b\": 3, \"c\": 4}\n{**x, **y}\n\n{'a': 1, 'b': 3, 'c': 4}\n\n\nUsing **kwargs you can easily re-use parameters are the same for different calls to the same function:\n\nimport pandas as pd\n\ncsv_kwargs = {\"sep\": \";\", \"encoding\": \"utf-8\"}\names = pd.read_csv(\n    \"https://github.com/eaisi/discover-projects/blob/main/ames-housing/AmesHousing.csv?raw=true\",\n    **csv_kwargs\n)\npima = pd.read_csv(\n    \"https://github.com/eaisi/discover-projects/blob/main/pima-indians-diabetes/diabetes.csv?raw=true\",\n    **csv_kwargs\n)\n\n\n\n#9. Sort by complex criteria using the key parameter (EP #14)\nThe list built-in type provides a sort method for ordering the items in a list instance based on a variety of criteria. By default, sort will order a list’s contents by the natural ascending order of the items. For example, here I sort a list of integers from smallest to largest:\n\nnumbers = [93, 86, 11, 68, 70]\nnumbers.sort()\nprint(numbers)\n\n[11, 68, 70, 86, 93]\n\n\nThe sort method works for nearly all built-in types (strings, floats, etc.) that have a natural ordering to them. What does sort do with objects? Often there’s an attribute on the object that you’d like to use for sorting. To support this use case, the sort method accepts a key parameter that’s expected to be a function. The key function is passed a single argument, which is an item from the list that is being sorted. The return value of the key function should be a comparable value (i.e., with a natural ordering) to use in place of an item for sorting purposes.\nFor example, here I define a dataclass to represent various tools. To sort a the list of Tool object alphabetically by name, I use the lambda keyword to define a function for the key parameter:\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Tool:\n    name: str\n    weight: float\n\n\ntools = [\n    Tool(\"level\", 3.5),\n    Tool(\"hammer\", 1.25),\n    Tool(\"screwdriver\", 0.5),\n    Tool(\"chisel\", 0.25),\n]\n\n\nprint(\"\\nUnsorted:\\n\", tools)\ntools.sort(key=lambda x: x.name)\nprint(\"\\nSorted:\\n\", tools)\n\n\nUnsorted:\n [Tool(name='level', weight=3.5), Tool(name='hammer', weight=1.25), Tool(name='screwdriver', weight=0.5), Tool(name='chisel', weight=0.25)]\n\nSorted:\n [Tool(name='chisel', weight=0.25), Tool(name='hammer', weight=1.25), Tool(name='level', weight=3.5), Tool(name='screwdriver', weight=0.5)]\n\n\nI can just as easily define another lambda function to sort by weight and pass it as the key parameter to the sort method:\n\ntools.sort(key=lambda x: x.weight)\nprint(\"\\nBy weight:\\n\", tools)\n\n\nBy weight:\n [Tool(name='chisel', weight=0.25), Tool(name='screwdriver', weight=0.5), Tool(name='hammer', weight=1.25), Tool(name='level', weight=3.5)]\n\n\nWithin the lambda function passed as the key parameter you can access attributes of items as I’ve done here, index into items (for sequences, tuples, and dictionaries), or use any other valid expression.\nFor basic types like strings, you may even want to use the key function to do transformations on the values before sorting. For example, here I apply the lower method to each item in a list of place names to ensure that they’re in alphabetical order, ignoring any capitalization (since in the natural lexical ordering of strings, capital letters come before lowercase letters):\n\nplaces = [\"home\", \"work\", \"New York\", \"Paris\"]\nplaces.sort()\nprint(\"Case sensitive: \", places)\nplaces.sort(key=lambda x: x.lower())\nprint(\"Case insensitive:\", places)\n\nCase sensitive:  ['New York', 'Paris', 'home', 'work']\nCase insensitive: ['home', 'New York', 'Paris', 'work']\n\n\nSometimes you may need to use multiple criteria for sorting. For example, say that I have a list of power tools and I want to sort them first by weight and then by name. How can I accomplish this?\n\npower_tools = [\n    Tool(\"drill\", 4),\n    Tool(\"circular saw\", 5),\n    Tool(\"jackhammer\", 40),\n    Tool(\"sander\", 4),\n]\n\nThe simplest solution in Python is to use the tuple type. Tuples are immutable sequences of arbitrary Python values. Tuples are comparable by default and have a natural ordering, meaning that they implement all of the special methods, such as __lt__, that are required by the sort method. Tuples implement these special method comparators by iterating over each position in the tuple and comparing the corresponding values one index at a time. Here, I show how this works when one tool is heavier than another:\n\nsaw = (5, \"circular saw\")\njackhammer = (40, \"jackhammer\")\n(jackhammer &lt; saw)  # False\n\nFalse\n\n\nIf the first position in the tuples being compared are equal—weight in this case—then the tuple comparison will move on to the second position, and so on. You can take advantage of this tuple comparison behavior in order to sort the list of power tools first by weight and then by name. Here, I define a key function that returns a tuple containing the two attributes that I want to sort on in order of priority:\n\npower_tools.sort(key=lambda x: (x.weight, x.name))\nprint(power_tools)\n\n[Tool(name='drill', weight=4), Tool(name='sander', weight=4), Tool(name='circular saw', weight=5), Tool(name='jackhammer', weight=40)]\n\n\nOne limitation of having the key function return a tuple is that the direction of sorting for all criteria must be the same (either all in ascending order, or all in descending order). If I provide the reverse parameter to the sort method, it will affect both criteria in the tuple the same way (note how ’sander' now comes before 'drill' instead of after):\n\npower_tools.sort(\n    key=lambda x: (x.weight, x.name), reverse=True\n)  # Makes all criteria descending\nprint(power_tools)\n\n[Tool(name='jackhammer', weight=40), Tool(name='circular saw', weight=5), Tool(name='sander', weight=4), Tool(name='drill', weight=4)]\n\n\nFor situations where you do want to have different sorting orders, Python provides a stable sorting algorithm. The sort method of the list type will preserve the order of the input list when the key function returns values that are equal to each other. This means that I can call sort multiple times on the same list to combine different criteria together. Here, I produce the same sort ordering of weight descending and nameb ascending as I did above but by using two separate calls to sort:\n\npower_tools.sort(key=lambda x: x.name)  # Name ascending\npower_tools.sort(key=lambda x: x.weight, reverse=True)  # Weight descending\nprint(power_tools)\n\n[Tool(name='jackhammer', weight=40), Tool(name='circular saw', weight=5), Tool(name='drill', weight=4), Tool(name='sander', weight=4)]\n\n\nThis same approach can be used to combine as many different types of sorting criteria as you’d like in any direction, respectively. You just need to make sure that you execute the sorts in the opposite sequence of what you want the final list to contain. In this example, I wanted the sort order to be by weight descending and then by name ascending, so I had to do the name sort first, followed by the weight sort.\nThat said, the approach of having the key function return a tuple, and using unary negation to mix sort orders for numbers, is simpler to read and requires less code. I recommend only using multiple calls to sort if it’s absolutely necessary.\n ### #10. Prefer get over in and KeyError to handle missing dictionairy keys (EP 16)\nThe three fundamental operations for interacting with dictionaries are accessing, assigning, and deleting keys and their associated values. The contents of dictionaries are dynamic, and thus it’s entirely possible—even likely—that when you try to access or delete a key, it won’t already be present.\nFor example, say that I’m trying to determine people’s favorite type of bread to devise the menu for a sandwich shop. Here, I define a dictionary of counters with the current votes for each style:\n\ncounters = {\n    \"pumpernickel\": 2,\n    \"sourdough\": 1,\n}\n\nTo increment the counter for a new vote, I need to see if the key exists, insert the key with a default counter value of zero if it’s missing, and then increment the counter’s value. This requires accessing the key two times and assigning it once. Here, I accomplish this task using an if statement with an in expression that returns True when the key is present:\n\nkey = \"wheat\"\n\nif key in counters:\n    count = counters[key]\nelse:\n    count = 0\n\ncounters[key] = count + 1\n\nAnother way to accomplish the same behavior is by relying on how dictionaries raise a KeyError exception when you try to get the value for a key that doesn’t exist. This approach is more efficient because it requires only one access and one assignment:\n\ntry:\n    count = counters[key]\nexcept KeyError:\n    count = 0\n\ncounters[key] = count + 1\n\nThis flow of fetching a key that exists or returning a default value is so common that the dict built-in type provides the get method to accomplish this task. The second parameter to get is the default value to return in the case that the key—the first parameter—isn’t present. This also requires only one access and one assignment, but it’s much shorter than the KeyError example:\n\ncount = counters.get(key, 0)\ncounters[key] = count + 1\n\nThus, for a dictionary with simple types, using the get method is the shortest and clearest option.\n\nNote: ff you’re maintaining dictionaries of counters like this, it’s worth considering the Counter class from the collections built-in module, which provides most of the facilities you are likely to need.\n\nWhat if the values of the dictionary are a more complex type, like a list? For example, say that instead of only counting votes, I also want to know who voted for each type of bread. Here, I do this by associating a list of names with each key:\n\nvotes = {\n    \"baguette\": [\"Bob\", \"Alice\"],\n    \"ciabatta\": [\"Coco\", \"Deb\"],\n}\nkey = \"brioche\"\nwho = \"Elmer\"\n\nSimilarly, you can use the get method to fetch a list value when the key is present, or do one fetch and one assignment if the key isn’t present:\n\nnames = votes.get(key)\nif names is None:\n    votes[key] = names = []\nnames.append(who)\nprint(votes)\n\n{'baguette': ['Bob', 'Alice'], 'ciabatta': ['Coco', 'Deb'], 'brioche': ['Elmer']}\n\n\nThe approach that involves using get to fetch list values can further be shortened by one line if you use an assignment expression with the walrus operator which was introduced in Python 3.8. Read more about that on Real Python. For now, we will just leave you with the example:\n\n# note: this will throw error on Google Colab, which is Python 3.6\nif (names := votes.get(key)) is None:\n    votes[key] = names = []\nnames.append(who)\nprint(votes)\n\n{'baguette': ['Bob', 'Alice'], 'ciabatta': ['Coco', 'Deb'], 'brioche': ['Elmer', 'Elmer']}",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python",
      "Effective Python and idiomatic pandas"
    ]
  },
  {
    "objectID": "posts/python/effective-python-and-idiomatic-pandas.html#functions",
    "href": "posts/python/effective-python-and-idiomatic-pandas.html#functions",
    "title": "Effective Python and idiomatic pandas",
    "section": "Functions",
    "text": "Functions\n ### #11. Never unpack more than three variables when functions return multiple values (EP #19)\nOne effect of the unpacking syntax (see item 4) is that it allows Python functions to seemingly return more than one value. For example, say that I’m trying to determine various statistics for a population of alligators. Given a list of lengths, I need to calculate the minimum and maximum lengths in the population. Here, I do this in a single function that appears to return two values:\n\ndef get_stats(numbers):\n    minimum = min(numbers)\n    maximum = max(numbers)\n    return minimum, maximum\n\n\nlengths = [63, 73, 72, 60, 67, 66, 71, 61, 72, 70]\nminimum, maximum = get_stats(lengths)  # Two return values\nprint(f\"Min: {minimum}, Max: {maximum}\")\n\nMin: 60, Max: 73\n\n\nThe way this works is that multiple values are returned together in a two-item tuple. The calling code then unpacks the returned tuple by assigning two variables. Here, I use an even simpler example to show how an unpacking statement and multiple-return function work the same way:\n\nfirst, second = 1, 2\nassert first == 1\nassert second == 2\n\n\ndef my_function():\n    return [1, 2]\n\n\nfirst, second = my_function()\nassert first == 1\nassert second == 2\n\n\n_, second = my_function()\n\nMultiple return values can also be received by starred expressions for catch-all unpacking. For example, say I need another function that calculates how big each alligator is relative to the population average. This function returns a list of ratios, but I can receive the longest and shortest items individually by using a starred expression for the middle portion of the list:\n\ndef get_avg_ratio(numbers):\n    average = sum(numbers) / len(numbers)\n    scaled = [x / average for x in numbers]\n    scaled.sort(reverse=True)\n    return scaled\n\n\nlongest, *middle, shortest = get_avg_ratio(lengths)\nprint(f\"Longest: {longest:&gt;4.0%}\")\nprint(f\"Shortest: {shortest:&gt;4.0%}\")\n\nLongest: 108%\nShortest:  89%\n\n\nNow, imagine that the program’s requirements change, and I need to also determine the average length, median length, and total population size of the alligators. I can do this by expanding the get_stats function to also calculate these statistics and return them in the result tuple that is unpacked by the caller:\n\ndef get_stats(numbers):\n    minimum = min(numbers)\n    maximum = max(numbers)\n    count = len(numbers)\n    average = sum(numbers) / count\n\n    sorted_numbers = sorted(numbers)\n    middle = count // 2\n    if count % 2 == 0:\n        lower = sorted_numbers[middle - 1]\n        upper = sorted_numbers[middle]\n        median = (lower + upper) / 2\n    else:\n        median = sorted_numbers[middle]\n\n    return minimum, maximum, average, median, count\n\n\nminimum, maximum, average, median, count = get_stats(lengths)\n\nprint(f\"Min: {minimum}, Max: {maximum}\")\nprint(f\"Average: {average}, Median: {median}, Count {count}\")\n\nassert minimum == 60\nassert maximum == 73\nassert average == 67.5\nassert median == 68.5\nassert count == 10\n\nMin: 60, Max: 73\nAverage: 67.5, Median: 68.5, Count 10\n\n\nThere are two problems with this code. First, all the return values are numeric, so it is all too easy to reorder them accidentally (e.g., swapping average and median), which can cause bugs that are hard to spot later. Using a large number of return values is extremely error prone.\nSecond, the line that calls the function and unpacks the values is long, and it likely will need to be wrapped in one of a variety of ways, which hurts readability.\nTo avoid these problems, you should never use more than three variables when unpacking the multiple return values from a function. These could be individual values from a three-tuple, two variables and one catch-all starred expression, or anything shorter. If you need to unpack more return values than that, you’re better off defining a dataclass or namedtuple and having your function return an instance of that instead.\n\n#12. Use None and docstrings to specify dynamic default arguments (EP #24)\nSometimes you need to use a non-static type as a keyword argument’s default value. For example, say that I want to load a value encoded as JSON data; if decoding the data fails, I want an empty dictionary to be returned by default:\n\nimport json\n\n\ndef decode(data, default={}):\n    try:\n        return json.loads(data)\n    except ValueError:\n        return default\n\nThe problem here is that dictionary specified for default will be shared by all calls to decode because default argument values are evaluated only once (at module load time). This can cause extremely surprising behavior:\n\nfoo = decode(\"bad data\")\nfoo[\"stuff\"] = 5\nbar = decode(\"also bad\")\nbar[\"meep\"] = 1\nprint(\"Foo:\", foo)\nprint(\"Bar:\", bar)\n\nFoo: {'stuff': 5, 'meep': 1}\nBar: {'stuff': 5, 'meep': 1}\n\n\nYou might expect two different dictionaries, each with a single key and value. But modifying one seems to also modify the other. The culprit is that foo and bar are both equal to the default parameter. They are the same dictionary object:\n\nfoo is bar\n\nTrue\n\n\nThe fix is to set the keyword argument default value to None and then document the behavior in the function’s docstring:\n\ndef decode(data, default=None):\n    \"\"\"Load JSON data from a string.\n\n    Args:\n         data: JSON data to decode.\n         default: Value to return if decoding fails.\n             Defaults to an empty dictionary.\n    \"\"\"\n    try:\n        return json.loads(data)\n    except ValueError:\n        if default is None:\n            default = {}\n    return default\n\nNow, running the same test code as before produces the expected result:\n\nfoo = decode(\"bad data\")\nfoo[\"stuff\"] = 5\nbar = decode(\"also bad\")\nbar[\"meep\"] = 1\nprint(\"Foo:\", foo)\nprint(\"Bar:\", bar)\n\nFoo: {'stuff': 5}\nBar: {'meep': 1}\n\n\n\nfoo is not bar\n\nTrue\n\n\nSo remember: - A default argument value is evaluated only once: during function definition at module load time. This can cause odd behaviors for dynamic values (like {}, [], or datetime.now()). - Use None as the default value for any keyword argument that has a dynamic value. Document the actual default behavior in the function’s docstring.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python",
      "Effective Python and idiomatic pandas"
    ]
  },
  {
    "objectID": "posts/python/effective-python-and-idiomatic-pandas.html#comprehensions",
    "href": "posts/python/effective-python-and-idiomatic-pandas.html#comprehensions",
    "title": "Effective Python and idiomatic pandas",
    "section": "Comprehensions",
    "text": "Comprehensions\n\n#13. Use comprehensions instead of map and filter (EP #27)\nPython provides compact syntax for deriving a new list from another sequence or iterable. These expressions are called list comprehensions. For example, say that I want to compute the square of each number in a list. Here, I do this by using a simple for loop:\n\na = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nsquares = []\nfor x in a:\n    squares.append(x ** 2)\nprint(squares)\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\nWith a list comprehension, I can achieve the same outcome by specifying the expression for my computation along with the input sequence to loop over:\n\nsquares = [x ** 2 for x in a]  # List comprehension\nprint(squares)\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\nUnless you’re applying a single-argument function, list comprehensions are also clearer than the map built-in function for simple cases. map requires the creation of a lambda function for the computation, which is visually noisy:\n\nalt = map(lambda x: x ** 2, a)\n\nUnlike map, list comprehensions let you easily filter items from the input list, removing corresponding outputs from the result. For example, say I want to compute the squares of the numbers that are divisible by 2. Here, I do this by adding a conditional expression to the list comprehension after the loop:\n\neven_squares = [x ** 2 for x in a if x % 2 == 0]\nprint(even_squares)\n\n[4, 16, 36, 64, 100]\n\n\nThe filter built-in function can be used along with map to achieve the same outcome, but it is much harder to read:\n\nalt = map(lambda x: x ** 2, filter(lambda x: x % 2 == 0, a))\nassert even_squares == list(alt)\n\nDictionaries and sets have their own equivalents of list comprehensions (called dictionary comprehensions and set comprehensions, respectively). These make it easy to create other types of derivative data structures when writing algorithms:\n\n# note both dictionairy and set comprehensions use {} which may be confusing at first\neven_squares_dict = {x: x ** 2 for x in a if x % 2 == 0}\nthrees_cubed_set = {x ** 3 for x in a if x % 3 == 0}\nprint(even_squares_dict)\nprint(threes_cubed_set)\n\n{2: 4, 4: 16, 6: 36, 8: 64, 10: 100}\n{216, 729, 27}\n\n\nAchieving the same outcome is possible with map and filter if you wrap each call with a corresponding constructor. These statements get so long that you have to break them up across multiple lines, which is even noisier and should be avoided:\n\nalt_dict = dict(map(lambda x: (x, x ** 2), filter(lambda x: x % 2 == 0, a)))\nalt_set = set(map(lambda x: x ** 3, filter(lambda x: x % 3 == 0, a)))\n\n\n\n#14. Avoid more than two control subexpressions in comprehensions (EP #28)\nBeyond basic usage, comprehensions support multiple levels of looping. For example, say that I want to simplify a matrix (a list containing other list instances) into one flat list of all cells. Here, I do this with a list comprehension by including two for subexpressions. These subexpressions run in the order provided, from left to right:\n\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nflat = [x for row in matrix for x in row]\nprint(flat)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nThis example is simple, readable, and a reasonable usage of multiple loops in a comprehension. Another reasonable usage of multiple loops involves replicating the two-level-deep layout of the input list. For example, say that I want to square the value in each cell of a twodimensional matrix. This comprehension is noisier because of the extra [] characters, but it’s still relatively easy to read:\n\nsquared = [[x ** 2 for x in row] for row in matrix]\nprint(squared)\n\n[[1, 4, 9], [16, 25, 36], [49, 64, 81]]\n\n\nIf this comprehension included another loop, it would get so long that I’d have to split it over multiple lines:\n\nmy_lists = [\n    [[1, 2, 3], [4, 5, 6]],\n]\nflat = [x for sublist1 in my_lists for sublist2 in sublist1 for x in sublist2]\n\nAt this point, the multiline comprehension isn’t much shorter than the alternative. Here, I produce the same result using normal loop statements. The indentation of this version makes the looping clearer than the three-level-list comprehension:\n\nflat = []\nfor sublist1 in my_lists:\n    for sublist2 in sublist1:\n        flat.extend(sublist2)\n\nComprehensions support multiple if conditions. Multiple conditions at the same loop level have an implicit and expression. For example, say that I want to filter a list of numbers to only even values greater than 4. These two list comprehensions are equivalent:\n\na = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nb = [x for x in a if x &gt; 4 if x % 2 == 0]\nc = [x for x in a if x &gt; 4 and x % 2 == 0]\n\nConditions can be specified at each level of looping after the for subexpression. For example, say I want to filter a matrix so the only cells remaining are those divisible by 3 in rows that sum to 10 or higher. Expressing this with a list comprehension does not require a lot of code, but it is extremely difficult to read:\n\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nfiltered = [[x for x in row if x % 3 == 0] for row in matrix if sum(row) &gt;= 10]\nprint(filtered)\n\n[[6], [9]]\n\n\nAlthough this example is a bit convoluted, in practice you’ll see situations arise where such comprehensions seem like a good fit. I strongly encourage you to avoid using list, dict, or set comprehensions that look like this. The resulting code is very difficult for new readers to understand. The potential for confusion is even worse for dict comprehensions since they already need an extra parameter to represent both the key and the value for each item.\nThe rule of thumb is to avoid using more than two control subexpressions in a comprehension. This could be two conditions, two loops, or one condition and one loop. As soon as it gets more complicated than that, you should use normal if and for statements and write a helper function.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python",
      "Effective Python and idiomatic pandas"
    ]
  },
  {
    "objectID": "posts/python/effective-python-and-idiomatic-pandas.html#pandas",
    "href": "posts/python/effective-python-and-idiomatic-pandas.html#pandas",
    "title": "Effective Python and idiomatic pandas",
    "section": "pandas",
    "text": "pandas\n\n#15. Be aware of memory consumption and downcast where necessary\nFor larger datasets, it pays off to optimize memory usage by converting string columns to categorical, and downcasting numeric columns. Use df.info() to get a first view. We will demonstrate this with the Ames housing dataset, which in itselft is not so large (1.8 MB). If anything gets beyond hundreds of MBs in memory, it helps to optimize, by using pd.to_numeric(downcast=…) and converting categorical variables to pd.categorical. Doing so reduces the memory to 22% of the orginal (400 KB).\n\ndf = pd.read_csv(\n    \"https://github.com/eaisi/discover-projects/blob/main/ames-housing/AmesHousing.csv?raw=true\"\n)\ndf.memory_usage().sum()\n\n1922208\n\n\n\n# objects to categorical\ndf[df.select_dtypes(include=\"object\").columns] = df.select_dtypes(\n    include=\"object\"\n).astype(\"category\")\n\n# convert integers to smallest unsigned integer and floats to smallest\nfor old, new in [(\"integer\", \"unsigned\"), (\"float\", \"float\")]:\n    for col in df.select_dtypes(include=old).columns:\n        df[col] = pd.to_numeric(df[col], downcast=new)\n\ndf.memory_usage().sum()\n\n419054\n\n\n\n\n#16. Use .loc and .iloc for indexing dataframes (and forget about .ix)\nOne-third of the top-15 voted pandas questions on Stackoverflow are about indexing. Another one-third are about slicing. This seems as good a place as any to start.\nBy indexing, we mean the selection of subsets of a DataFrame or Series. DataFrames (and to a lesser extent, Series) provide a difficult set of challenges:\n\nLike lists, you can index by location.\nLike dictionaries, you can index by label.\nLike NumPy arrays, you can index by boolean masks.\nAny of these indexers could be scalar indexes, or they could be arrays, or they could be slices.\nAny of these should work on the index (row labels) or columns of a DataFrame.\nAnd any of these should work on hierarchical indexes.\n\nThe complexity of pandas’ indexing is a microcosm for the complexity of the pandas API in general. There’s a reason for the complexity (well, most of it), but that’s not much consolation while you’re learning. Still, all of these ways of indexing really are useful enough to justify their inclusion in the library.\nSince pandas 0.12, these tasks have been cleanly separated into two methods:\n\n.loc for label-based indexing\n.iloc for positional indexing\n\nPreviously, .ix was used to handle both cases. You may encounter this in old blog-posts. .ix is deprecated, so stop using it.\nFor more details, please read any (or all) of the following online tutorials: - Using Pandas and Python To Explore Your Dataset on Real Python; - Data Indexing and Slicing notebok by Jake VanderPlas - pandas documentation on indexing and selecting data\n\n\n#17. Use method chaining to make your data preparation more readable\nMethod chaining, where you call methods on an object one after another, is in vogue at the moment. It’s always been a style of programming that’s been possible with pandas:\n\nassign (0.16.0): For adding new columns to a DataFrame in a chain (inspired by dplyr’s mutate)\npipe (0.16.2): For including user-defined methods in method chains.\nrename (0.18.0): For altering axis names (in additional to changing the actual labels as before).\nWindow methods (0.18): Took the top-level pd.rolling_* and pd.expanding_* functions and made them NDFrame methods with a groupby-like API.\nResample (0.18.0) Added a new groupby-like API\nselection by callable (0.18.1): You can pass a callable to the indexing methods, to be evaluated within the DataFrame’s context (like .query, but with code instead of strings).\n\nTo illustrate the potential of method chaining, let’s look at this example with the Titanic dataset which explores the data of passengers that have embarked in Southampton:\n\nimport sys\nimport pandas as pd\n\n\nTITANIC_DATA = \"https://github.com/BindiChen/machine-learning/blob/master/data-analysis/007-method-chaining/data/train.csv?raw=true\"\nbins = [0, 13, 19, 61, sys.maxsize]\nlabels = [\"&lt;12\", \"Teen\", \"Adult\", \"Older\"]\n\npclass_age_map = {\n    1: 37,\n    2: 29,\n    3: 24,\n}\n\n\ndef replace_age_na(x_df, fill_map):\n    cond = x_df[\"Age\"].isna()\n    res = x_df.loc[cond, \"Pclass\"].map(fill_map)\n    x_df.loc[cond, \"Age\"] = res\n    return x_df\n\n\nview_southampton = (\n    pd.read_csv(TITANIC_DATA)\n    .pipe(replace_age_na, pclass_age_map)\n    .query('Embarked == \"S\"')\n    .assign(ageGroup=lambda df: pd.cut(df[\"Age\"], bins=bins, labels=labels))\n    .pivot_table(values=\"Survived\", columns=\"Pclass\", index=\"ageGroup\", aggfunc=\"mean\")\n    .rename_axis(\"\", axis=\"columns\")\n    .rename(\"Class {}\".format, axis=\"columns\")\n    .style.format(\"{:.2%}\")\n)\n\nview_southampton\n\n\n\n\n\n\n\nClass 1\nClass 2\nClass 3\n\n\nageGroup\n \n \n \n\n\n\n\n&lt;12\n75.00%\n100.00%\n37.14%\n\n\nTeen\n80.00%\n40.00%\n17.78%\n\n\nAdult\n58.10%\n40.77%\n16.61%\n\n\nOlder\n25.00%\n33.33%\n50.00%\n\n\n\n\n\n\n_string = 'sdfjghsdfgjk'\n\ndef clean_string(str):\n    return _string.upper().replace('S', 'Z')\n\n\nlambda s: s.upper().replace('S', 'Z')\n\n&lt;function __main__.&lt;lambda&gt;(s)&gt;\n\n\nRead more on method chaining in these tutorials:\n\nBest practice method chaining by B. Chen (with the Titanic example).\nTom Augspurger’s explanation on method chaining goes a bit more in depth on the reasoning and rationale behind it.\n\n\n\n#18. Don’t use inplace=True operations on dataframes\nMost pandas methods have an inplace keyword that’s False by default. In general, you shouldn’t do inplace operations.\nFirst, if you like method chains then you simply can’t use inplace since the return value is None, terminating the chain.\nSecond, I suspect people have a mental model of inplace operations happening, you know, inplace. That is, extra memory doesn’t need to be allocated for the result. But that might not actually be true. Quoting Jeff Reback from that answer\n\nTheir is no guarantee that an inplace operation is actually faster. Often they are actually the same operation that works on a copy, but the top-level reference is reassigned.\n\nThat is, the pandas code might look something like this\ndef dataframe_method(self, inplace=False):\n    data = self.copy()  # regardless of inplace\n    result = ...\n    if inplace:\n        self._update_inplace(data)\n    else:\n        return result\nThere’s a lot of defensive copying in pandas. Part of this comes down to pandas being built on top of NumPy, and not having full control over how memory is handled and shared. Without the copy, adding the columns would modify the input DataFrame, which just isn’t polite.\nFinally, inplace operations don’t make sense in projects like ibis or dask, where you’re manipulating expressions or building up a DAG of tasks to be executed, rather than manipulating the data directly.\n\n\n#19. Use .query and eval for fast, complex indexing\nThe power of the PyData stack is built upon the ability of NumPy and pandas to push basic operations into C via an intuitive syntax: examples are vectorized/broadcasted operations in NumPy, and grouping-type operations in pandas. While these abstractions are efficient and effective for many common use cases, they often rely on the creation of temporary intermediate objects, which can cause undue overhead in computational time and memory use.\nAs of version 0.13 (released January 2014), pandas includes some experimental tools that allow you to directly access C-speed operations without costly allocation of intermediate arrays. These are the eval() and query() functions, which rely on the Numexpr package.\nPlease refer to the following notebooks and documentation on how and when to use them: - High-Performance Pandas: eval() and query() by Jake VanderPlas - pandas user guide on the query() method",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python",
      "Effective Python and idiomatic pandas"
    ]
  },
  {
    "objectID": "posts/analytical-problem-solving/index.html",
    "href": "posts/analytical-problem-solving/index.html",
    "title": "Analytical problem solving",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Perspectives on data science",
      "Analytical problem solving"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html",
    "href": "posts/ibis-analytics/index.html",
    "title": "Modern, hybrid, open analytics",
    "section": "",
    "text": "As a Python data user, I’ve wanted a more modular, composable, and scalable ecosystem. I think it’s here. Wes McKinney released pandas c. 2009 to bring dataframes into Python and it became one of the most used software packages. It was built when small data was smaller and has some downsides Wes wrote about in his “Apache Arrow and the ‘10 things I hate about pandas’” blog post. Wes created Ibis (and co-created Apache Arrow) to address these issues and more with a different approach than any other Python dataframe library by decoupling the dataframe API from the backend implementation. This allows Ibis to support 20+ backends today including pandas, DuckDB, Polars, Postgres, BigQuery, Snowflake, PySpark, and over a dozen more.\nI’ve experienced many pains of the ecosystem myself. I’ve downloaded a 1 GB CSV from a database GUI and used pandas to munge it because I couldn’t figure it out in SQL. When building larger ML demos that included data preprocessing on GBs, I hit OOM errors and learned about how much data expands in memory. Schema inference issues and file read speeds led me understand why storing data in compressed Parquet files might be better for my use cases. I learned about partitioning strategies and the joy of subtle differences and incompatibilities across data systems. I started using PySpark and Dask to overcome hurdles. I slowly learned about databases. Fortunately while I’ve been learning, others (including many Ibis contributors) have been building a modern, open-source Python data ecosystem! Now I can use what they’ve built to create a modern analytics application.\nIn this blog, we’ll look at how to build an end-to-end analytics project using Ibis as the frontend for data in Python. Ibis plays a key role, but is only one component in any real data project.\nWe’ll combine a variety of open-source tools and freemium services including:\n\nIbis (dataframe library)\nDuckDB (database and query engine)\nDagster (orchestration library)\nPlotly (visualization library)\nStreamlit (dashboard library)\njustfile (command runner)\nTOML (configuration)\nGitHub (source control, CI/CD)\nAzure VM (self-hosted runner)\nGitHub web APIs (source data)\nGoogle BigQuery (source data)\nZulip (source data)\nMotherDuck (cloud service for DuckDB)\nStreamlit Community Cloud (cloud service for Streamlit)\n\nto build an open-source, modular, composable, scalable, automated, hybrid-cloud, end-to-end analytics project processing a few million rows of data that provides real business value.",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#introduction",
    "href": "posts/ibis-analytics/index.html#introduction",
    "title": "Modern, hybrid, open analytics",
    "section": "",
    "text": "As a Python data user, I’ve wanted a more modular, composable, and scalable ecosystem. I think it’s here. Wes McKinney released pandas c. 2009 to bring dataframes into Python and it became one of the most used software packages. It was built when small data was smaller and has some downsides Wes wrote about in his “Apache Arrow and the ‘10 things I hate about pandas’” blog post. Wes created Ibis (and co-created Apache Arrow) to address these issues and more with a different approach than any other Python dataframe library by decoupling the dataframe API from the backend implementation. This allows Ibis to support 20+ backends today including pandas, DuckDB, Polars, Postgres, BigQuery, Snowflake, PySpark, and over a dozen more.\nI’ve experienced many pains of the ecosystem myself. I’ve downloaded a 1 GB CSV from a database GUI and used pandas to munge it because I couldn’t figure it out in SQL. When building larger ML demos that included data preprocessing on GBs, I hit OOM errors and learned about how much data expands in memory. Schema inference issues and file read speeds led me understand why storing data in compressed Parquet files might be better for my use cases. I learned about partitioning strategies and the joy of subtle differences and incompatibilities across data systems. I started using PySpark and Dask to overcome hurdles. I slowly learned about databases. Fortunately while I’ve been learning, others (including many Ibis contributors) have been building a modern, open-source Python data ecosystem! Now I can use what they’ve built to create a modern analytics application.\nIn this blog, we’ll look at how to build an end-to-end analytics project using Ibis as the frontend for data in Python. Ibis plays a key role, but is only one component in any real data project.\nWe’ll combine a variety of open-source tools and freemium services including:\n\nIbis (dataframe library)\nDuckDB (database and query engine)\nDagster (orchestration library)\nPlotly (visualization library)\nStreamlit (dashboard library)\njustfile (command runner)\nTOML (configuration)\nGitHub (source control, CI/CD)\nAzure VM (self-hosted runner)\nGitHub web APIs (source data)\nGoogle BigQuery (source data)\nZulip (source data)\nMotherDuck (cloud service for DuckDB)\nStreamlit Community Cloud (cloud service for Streamlit)\n\nto build an open-source, modular, composable, scalable, automated, hybrid-cloud, end-to-end analytics project processing a few million rows of data that provides real business value.",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#whats-the-business-value",
    "href": "posts/ibis-analytics/index.html#whats-the-business-value",
    "title": "Modern, hybrid, open analytics",
    "section": "What’s the business value?",
    "text": "What’s the business value?\nToday, Ibis is a thriving open-source project primarily backed by Voltron Data with contributors at Google, Claypot AI, Singlestore, Exasol, RisingWave, Starburst Data, and anyone who submits a PR and makes a commit. It aims to be a standard frontend for data in Python that scales to your backend. To understand the project’s health, we want to track some key adoption metrics. There was already a dashboard internally at Voltron Data, but it was written in R and I don’t know R. I do know Ibis and a few other OSS tools, and saw this as a cool opportunity to try out a few more while showcasing Ibis in a real-world project.\nLet’s get started!",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#finished-product",
    "href": "posts/ibis-analytics/index.html#finished-product",
    "title": "Modern, hybrid, open analytics",
    "section": "Finished product",
    "text": "Finished product\nThe final result is a MotherDuck database powering a dashboard deployed to Streamlit Community Cloud.\n\n\n\n\n\n\nNote\n\n\n\nSince writing this blog, I’ve added documentation metrics. This is ongoing and that page may be broken.\nLook out for a follow up post on how new metrics are added to the dashboard!\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile I consider this “production” it does go down occasionally, usually because of a stale connection to MotherDuck. There are screenshots below in case that happens while you’re reading this, or you can reproduce it locally (an exercise for the reader).\n\n\nThe dashboard can be viewed as a Streamlit app in your browser or embedded in this page below.",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#architecture",
    "href": "posts/ibis-analytics/index.html#architecture",
    "title": "Modern, hybrid, open analytics",
    "section": "Architecture",
    "text": "Architecture\nThe source of truth for the project’s architecture is the repository of code. A point-in-time description and snapshots of the code are below.\n\nGoals\nI had the following goals in mind:\n\nModular: while this project uses Dagster, it should be easily swappable for other orchestration tools like Airflow or Prefect or Kedro. While this project uses DuckDB, it should be easily swappable for Polars or Clickhouse or any other engine. While this project uses Streamlit, it should be swappable for Quarto dashboards…\nComposable: the project should be easy to understand and extend. It should be easy to add new data sources, new data transformations, new metrics, and new visualizations.\nAutomated: the project should be easy to run locally and in production. It should be easy to run the entire pipeline or just a single step. It should be easy to run the pipeline on a schedule or on a pull request.\nIterative: the project should be easy to iterate on. It should be easy to explore the data, transform it, and visualize it. It should be easy to switch between local and production data sources.\n\nOur open-source tools and freemium services allow us to accomplish these goals.\n\n\nOverview\nThe architecture consists of Python code. Steps in the overall data pipeline are captured in a justfile, which is a Makefile-like command runner. This allows us to run the pipeline locally or in automated CI/CD workflows. The data ingestion is a Python script that makes raw request calls to GitHub web APIs, uses the Zulip Python client for Zulip web APIs, and Ibis + SQL to extract PyPI data from a public Google BigQuery project.\nThe data is then extracted from its ingested file formats (JSON and Parquet), transformed with Ibis using the default DuckDB backend, and loaded into final DuckDB database files. Further postprocessing is done to combine these into a single DuckDB database file for convenience, though this step isn’t necessary. The tables are then copied using Ibis to connect to MotherDuck, a serverless cloud service for DuckDB. This allows us to use MotherDuck’s servers to access the production data from anywhere, including my laptop and a Streamlit Community Cloud server. The dashboard deployed there updates automatically when the data is updated. Finally, we can run some tests by executing the dashboard code.\nFor the most part, you just need a Python environment with pip installs including Ibis and a few other packages.\n\n\nPython pip install requirements\n\n# python\nruff\npython-dotenv\n\n# web clients\nzulip\nPyGithub # unused for now\n\n# data\nduckdb&gt;=0.9.0\nibis-framework[duckdb,bigquery,deltalake]\n\n# ML\nibisml # unused for now\n\n# viz\nplotly\nstreamlit\n\n# ops\ndagster\ndagster-webserver\n\nVarious environment variables set to access the data sources and cloud services are stored in a .env file and set in the respective cloud services (GitHub Actions secrets and Streamlit Community Cloud secrets).\n\n\n.env file\n\nBQ_PROJECT_ID=\"XXXXXXXX\"\nGITHUB_TOKEN=\"ghp_XXXXXXXX\"\nMOTHERDUCK_TOKEN=\"XXXXXXX\"\nZULIP_KEY=\"XXXXXXX\"\n\n\n\nConfiguration\nWe use a config.toml file to configure the project. This allows easily switching out the database (i.e. changing over to local for development) and adding new data sources. Most of the ingested data is currently unused, but this makes it easy in the future to set up additional dashboards for other Ibis projects as they grow and need to be tracked.\n[app]\n#database=\"data/app.ddb\"\ndatabase=\"md:ibis_analytics\"\n\n[eda]\ndatabase=\"md:ibis_analytics\"\n#database=\"data/data.ddb\"\n\n[ingest.pypi]\npackages = [\n    \"ibis-framework\",\n    \"ibis-examples\",\n    \"ibis-substrait\",\n    \"ibisml\",\n    \"ibis-birdbrain\",\n]\n[ingest.github]\nrepos = [\n    \"ibis-project/ibis\",\n    \"ibis-project/ibis-examples\",\n    \"ibis-project/ibis-substrait\",\n    \"ibis-project/ibisml\",\n    \"ibis-project/ibis-birdbrain\",\n]\nendpoints = [\n    \"repo\",\n    \"stargazers\",\n    \"subscribers\",\n    \"commits\",\n    \"releases\",\n    \"forks\",\n    \"issues\",\n    \"contributors\",\n]\n[ingest.zulip]\nurl = \"https://ibis-project.zulipchat.com\"\n\n\nAutomation\nAnything run frequently is put in a justfile. This makes it easy to run the same code locally or in a GitHub Action.\n\n\nShow the justfile of project commands\n\n# justfile\n\n# load environment variables\nset dotenv-load\n\n# variables\nmodule := \"dag\"\n\n# aliases\nalias fmt:=format\nalias etl:=run\nalias open:=open-dash\nalias dag-open:=open-dag\nalias preview:=app\n\n# format\nformat:\n    @ruff format .\n\n# smoke-test\nsmoke-test:\n    @ruff format --check .\n\n# list justfile recipes\ndefault:\n    just --list\n\n# setup\nsetup:\n    @pip install --upgrade -r requirements.txt\n\n# eda\neda:\n    @ipython -i eda.py\n\n# ingest\ningest:\n    @python {{module}}/ingest.py\n\n# run\nrun:\n    @dagster job execute -j all_assets -m {{module}}\n\n# postprocess\npostprocess:\n    @python {{module}}/postprocess.py\n\n# deploy\ndeploy:\n    @python {{module}}/deploy.py\n\n# test\ntest:\n    @python metrics.py\n    @python pages/0_github.py\n    @python pages/1_pypi.py\n    @python pages/2_zulip.py\n    @python pages/3_about.py\n\n# dag\ndag:\n    @dagster dev -m {{module}}\n\n# streamlit stuff\napp:\n    @streamlit run metrics.py\n\n# clean\nclean:\n    @rm -r *.ddb* || true\n    @rm -r data/system || true\n    @rm -r data/backup || true\n    @rm data/backup.ddb || true\n\n# open dag\nopen-dag:\n    @open http://localhost:3000/asset-groups\n\n# open dash\nopen-dash:\n    @open https://ibis-analytics.streamlit.app\n\n# cicd\ncicd:\n    @gh workflow run cicd.yaml\n\nThen our CI/CD workflow in cicd.yaml is just:\nname: cicd\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: \"0 0/3 * * *\"\n  pull_request:\n    paths:\n      - '.github/workflows/cicd.yaml'\n      - 'requirements.txt'\n      - '**.py'\n      - 'justfile'\n\njobs:\n\n  ingest-etl-postprocess-deploy-test:\n    runs-on: self-hosted\n    steps:\n      - uses: actions/checkout@v3\n      - uses: google-github-actions/auth@v1\n        with:\n          credentials_json: ${{ secrets.GCLOUD_JSON }}\n      - uses: extractions/setup-just@v1\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.11\n      - name: install requirements\n        run: just setup\n      - name: ingest\n        run: just ingest\n        env:\n          BQ_PROJECT_ID: voltrondata-demo\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          ZULIP_KEY: ${{ secrets.ZULIP_KEY }}\n      - name: run ETL job\n        run: just run\n      - name: postprocess\n        run: just postprocess\n      - name: deploy to prod\n        run: just deploy\n        env:\n          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}\n      - name: test\n        run: just test\n        env:\n          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}\nThis will run the entire pipeline on a schedule, on a pull request, or manually. Everything is automated!",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#data-ingestion",
    "href": "posts/ibis-analytics/index.html#data-ingestion",
    "title": "Modern, hybrid, open analytics",
    "section": "Data ingestion",
    "text": "Data ingestion\n\n\n\n\n\n\nWarning\n\n\n\nNote that ingesting the PyPI download data from BigQuery will incur costs of 8 euros if you are above your free tier. To save cost, you can download a snapshot of the data from 23 June 2024 here.\n\n\nAfter you just ingest data you end up with:\ndata/ingest/\n├── github/\n│   └── ibis-project/\n│       ├── ibis/\n│       │   ├── commits.000001.json\n│       │   ├── commits.000002.json\n│       │   ├── ...\n│       │   ├── forks.000001.json\n│       │   ├── forks.000002.json\n│       │   ├── ...\n│       │   ├── issues.000001.json\n│       │   ├── issues.000002.json\n│       │   ├── ...\n│       │   ├── pullRequests.000001.json\n│       │   ├── pullRequests.000054.json\n│       │   ├── ...\n│       │   ├── stargazers.000001.json\n│       │   ├── stargazers.000002.json\n│       │   ├── ...\n│       │   └── watchers.000001.json\n│       ├── ibis-birdbrain/\n│       │   ├── commits.000001.json\n│       │   ├── forks.000001.json\n│       │   ├── issues.000001.json\n│       │   ├── pullRequests.000001.json\n│       │   ├── stargazers.000001.json\n│       │   └── watchers.000001.json\n│       ├── ibis-examples/\n│       │   ├── commits.000001.json\n│       │   ├── forks.000001.json\n│       │   ├── issues.000001.json\n│       │   ├── pullRequests.000001.json\n│       │   ├── stargazers.000001.json\n│       │   └── watchers.000001.json\n│       ├── ibis-substrait/\n│       │   ├── commits.000001.json\n│       │   ├── commits.000002.json\n│       │   ├── ...\n│       │   ├── forks.000001.json\n│       │   ├── issues.000001.json\n│       │   ├── pullRequests.000001.json\n│       │   ├── pullRequests.000002.json\n│       │   ├── ...\n│       │   ├── stargazers.000001.json\n│       │   └── watchers.000001.json\n│       └── ibisml/\n│           ├── commits.000001.json\n│           ├── forks.000001.json\n│           ├── issues.000001.json\n│           ├── pullRequests.000001.json\n│           ├── stargazers.000001.json\n│           └── watchers.000001.json\n├── pypi/\n│   ├── ibis-birdbrain/\n│   │   └── file_downloads.parquet\n│   ├── ibis-examples/\n│   │   └── file_downloads.parquet\n│   ├── ibis-framework/\n│   │   └── file_downloads.parquet\n│   ├── ibis-substrait/\n│   │   └── file_downloads.parquet\n│   └── ibisml/\n│       └── file_downloads.parquet\n└── zulip/\n    ├── members.json\n    └── messages.json\nThis runs a Python data ingestion script and takes a few minutes. It’s not the best code, but it works!\n\n\nShow the data ingestion script\n\nimport os\nimport ibis\nimport toml\nimport json\nimport zulip\nimport inspect\nimport requests\n\nimport logging as log\n\nfrom ibis import _\nfrom dotenv import load_dotenv\nfrom datetime import datetime, timedelta, date\n\nfrom graphql_queries import (\n    issues_query,\n    pulls_query,\n    forks_query,\n    commits_query,\n    stargazers_query,\n    watchers_query,\n)\n\n\n# main function\ndef main():\n    # load environment variables\n    load_dotenv()\n\n    # ingest data\n    ingest_zulip()\n    ingest_pypi()\n    ingest_gh()\n    # ingest_ci() # TODO: fix permissions, add assets\n\n\n# helper functions\ndef write_json(data, filename):\n    # write the data to a file\n    with open(filename, \"w\") as f:\n        json.dump(data, f, indent=4)\n\n\n# ingest functions\ndef ingest_gh():\n    \"\"\"\n    Ingest the GitHub data.\n    \"\"\"\n    # configure logger\n    log.basicConfig(level=log.INFO)\n\n    # constants\n    GRAPH_URL = \"https://api.github.com/graphql\"\n\n    # load environment variables\n    GH_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n\n    # load config\n    config = toml.load(\"config.toml\")[\"ingest\"][\"github\"]\n    log.info(f\"Using repos: {config['repos']}\")\n\n    # construct header\n    headers = {\n        \"Authorization\": f\"Bearer {GH_TOKEN}\",\n    }\n\n    # map queries\n    queries = {\n        \"issues\": issues_query,\n        \"pullRequests\": pulls_query,\n        \"commits\": commits_query,\n        \"forks\": forks_query,\n        \"stargazers\": stargazers_query,\n        \"watchers\": watchers_query,\n    }\n\n    # define helper functions\n    def get_filename(query_name, page):\n        # return the filename\n        return f\"{query_name}.{page:06}.json\"\n\n    def get_next_link(link_header):\n        # if there is no link header, return None\n        if link_header is None:\n            return None\n\n        # split the link header into links\n        links = link_header.split(\", \")\n        for link in links:\n            # split the link into segments\n            segments = link.split(\"; \")\n\n            # if there are two segments and the second segment is rel=\"next\"\n            if len(segments) == 2 and segments[1] == 'rel=\"next\"':\n                # remove the &lt; and &gt; around the link\n                return segments[0].strip(\"&lt;&gt;\")\n\n        # if there is no next link, return None\n        return None\n\n    def fetch_data(client, owner, repo, query_name, query, output_dir, num_items=100):\n        # initialize variables\n        variables = {\n            \"owner\": owner,\n            \"repo\": repo,\n            \"num_items\": num_items,\n            \"before\": \"null\",\n        }\n\n        # initialize page number\n        page = 1\n\n        # while True\n        while True:\n            # request data\n            try:\n                log.info(f\"\\t\\tFetching page {page}...\")\n                resp = requests.post(\n                    GRAPH_URL,\n                    headers=headers,\n                    json={\"query\": query, \"variables\": variables},\n                )\n                json_data = resp.json()\n\n                log.info(f\"\\t\\t\\tStatus code: {resp.status_code}\")\n                # log.info(f\"\\t\\t\\tResponse: {resp.text}\")\n                # log.info(f\"\\t\\t\\tJSON: {json_data}\")\n\n                if resp.status_code != 200:\n                    log.error(\n                        f\"\\t\\tFailed to fetch data for {owner}/{repo}; url={GRAPH_URL}\\n\\n {resp.status_code}\\n {resp.text}\"\n                    )\n                    return\n\n                # extract data\n                if query_name == \"commits\":\n                    data = json_data[\"data\"][\"repository\"][\"defaultBranchRef\"][\n                        \"target\"\n                    ][\"history\"][\"edges\"]\n                    # get the next link\n                    cursor = json_data[\"data\"][\"repository\"][\"defaultBranchRef\"][\n                        \"target\"\n                    ][\"history\"][\"pageInfo\"][\"endCursor\"]\n                    has_next_page = json_data[\"data\"][\"repository\"][\"defaultBranchRef\"][\n                        \"target\"\n                    ][\"history\"][\"pageInfo\"][\"hasNextPage\"]\n\n                else:\n                    data = json_data[\"data\"][\"repository\"][query_name][\"edges\"]\n                    cursor = json_data[\"data\"][\"repository\"][query_name][\"pageInfo\"][\n                        \"endCursor\"\n                    ]\n                    has_next_page = json_data[\"data\"][\"repository\"][query_name][\n                        \"pageInfo\"\n                    ][\"hasNextPage\"]\n\n                # save json to a file\n                filename = get_filename(query_name, page)\n                output_path = os.path.join(output_dir, filename)\n                log.info(f\"\\t\\tWriting data to {output_path}\")\n                write_json(data, output_path)\n\n                variables[\"cursor\"] = f\"{cursor}\"\n                print(f\"has_next_page={has_next_page}\")\n                print(f\"cursor={cursor}\")\n                if not has_next_page:\n                    break\n\n                # increment page number\n                page += 1\n            except:\n                # print error if response\n                log.error(f\"\\t\\tFailed to fetch data for {owner}/{repo}\")\n\n                try:\n                    log.error(f\"\\t\\t\\tResponse: {resp.text}\")\n                except:\n                    pass\n\n                break\n\n    # create a requests session\n    with requests.Session() as client:\n        for repo in config[\"repos\"]:\n            log.info(f\"Fetching data for {repo}...\")\n            for query in queries:\n                owner, repo_name = repo.split(\"/\")\n                output_dir = os.path.join(\n                    \"data\",\n                    \"ingest\",\n                    \"github\",\n                    owner,\n                    repo_name,\n                )\n                os.makedirs(output_dir, exist_ok=True)\n                log.info(f\"\\tFetching data for {owner}/{repo_name} {query}...\")\n                fetch_data(client, owner, repo_name, query, queries[query], output_dir)\n\n\ndef ingest_pypi():\n    \"\"\"\n    Ingest the PyPI data.\n    \"\"\"\n    # constants\n    # set DEFAULT_BACKFILL to the number of days\n    # since July 19th, 2015 until today\n    DEFAULT_BACKFILL = (datetime.now() - datetime(2015, 7, 19)).days\n    BIGQUERY_DATASET = \"bigquery-public-data.pypi.file_downloads\"\n\n    # configure logger\n    log.basicConfig(level=log.INFO)\n\n    # load environment variables\n    project_id = os.getenv(\"BQ_PROJECT_ID\")\n    log.info(f\"Project ID: {project_id}\")\n\n    # load config\n    config = toml.load(\"config.toml\")[\"ingest\"][\"pypi\"]\n    log.info(f\"Packages: {config['packages']}\")\n\n    # configure lookback window\n    backfill = config[\"backfill\"] if \"backfill\" in config else DEFAULT_BACKFILL\n    log.info(f\"Backfill: {backfill}\")\n\n    # for each package\n    for package in config[\"packages\"]:\n        log.info(f\"Package: {package}\")\n        # create output directory\n        output_dir = os.path.join(\"data\", \"ingest\", \"pypi\", package)\n        os.makedirs(output_dir, exist_ok=True)\n\n        # construct query\n        query = f\"\"\"\n        SELECT *\n        FROM `{BIGQUERY_DATASET}`\n        WHERE file.project = '{package}'\n        AND DATE(timestamp)\n        BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL {backfill} DAY)\n        AND CURRENT_DATE()\n        \"\"\".strip()\n        query = inspect.cleandoc(query)\n\n        # connect to bigquery and execute query\n        con = ibis.connect(f\"bigquery://{project_id}\")\n        log.info(f\"Executing query:\\n{query}\")\n        t = con.sql(query)\n\n        # write to parquet\n        filename = f\"file_downloads.parquet\"\n        output_path = os.path.join(output_dir, filename)\n        log.info(f\"Writing to: {output_path}\")\n        t.to_parquet(output_path)\n\n\ndef ingest_ci():\n    \"\"\"\n    Ingest the CI data.\n    \"\"\"\n    # constants\n    # set DEFAULT_BACKFILL to the number of days\n    # since July 19th, 2015 until today\n    DEFAULT_BACKFILL = (datetime.now() - datetime(2015, 7, 19)).days\n\n    # configure logger\n    log.basicConfig(level=log.INFO)\n\n    # load environment variables\n    project_id = os.getenv(\"BQ_PROJECT_ID\")\n    log.info(f\"Project ID: {project_id}\")\n\n    # load config\n    config = toml.load(\"config.toml\")[\"ingest\"][\"ci\"]\n\n    # configure lookback window\n    backfill = config[\"backfill\"] if \"backfill\" in config else DEFAULT_BACKFILL\n    log.info(f\"Backfill: {backfill}\")\n\n    # make sure the data directory exists\n    os.makedirs(\"data/ingest/ci/ibis\", exist_ok=True)\n\n    # connect to databases\n    con = ibis.connect(\"duckdb://data/ingest/ci/ibis/raw.ddb\")\n    bq_con = ibis.connect(f\"bigquery://{project_id}/workflows\")\n\n    # copy over tables\n    for table in bq_con.list_tables():\n        log.info(f\"Writing table: {table}\")\n        con.create_table(table, bq_con.table(table).to_pyarrow(), overwrite=True)\n\n\ndef ingest_zulip():\n    \"\"\"Ingest the Zulip data.\"\"\"\n    # constants\n    email = \"cody@dkdc.dev\"\n\n    # load config\n    config = toml.load(\"config.toml\")[\"ingest\"][\"zulip\"]\n    log.info(f\"Using url: {config['url']}\")\n\n    # configure logger\n    log.basicConfig(level=log.INFO)\n\n    # load environment variables\n    zulip_key = os.getenv(\"ZULIP_KEY\")\n\n    # create the client\n    client = zulip.Client(email=email, site=config[\"url\"], api_key=zulip_key)\n\n    # get the users\n    r = client.get_members()\n    if r[\"result\"] != \"success\":\n        log.error(f\"Failed to get users: {r}\")\n    else:\n        members = r[\"members\"]\n        # make sure the directory exists\n        os.makedirs(\"data/ingest/zulip\", exist_ok=True)\n\n        # write the users to a file\n        filename = \"members.json\"\n        output_path = os.path.join(\"data\", \"ingest\", \"zulip\", filename)\n        log.info(f\"Writing members to: {output_path}\")\n        write_json(members, output_path)\n\n    # get the messages\n    all_messages = []\n    r = client.get_messages(\n        {\"anchor\": \"newest\", \"num_before\": 100, \"num_after\": 0, \"type\": \"stream\"}\n    )\n    if r[\"result\"] != \"success\":\n        log.error(f\"Failed to get messages: {r}\")\n    else:\n        messages = r[\"messages\"]\n        all_messages.extend(messages)\n        while len(messages) &gt; 1:\n            r = client.get_messages(\n                {\n                    \"anchor\": messages[0][\"id\"],\n                    \"num_before\": 100,\n                    \"num_after\": 0,\n                    \"type\": \"stream\",\n                }\n            )\n            if r[\"result\"] != \"success\":\n                log.error(f\"Failed to get messages: {r}\")\n                break\n            else:\n                messages = r[\"messages\"]\n                all_messages.extend(messages)\n\n        # make sure the directory exists\n        os.makedirs(\"data/ingest/zulip\", exist_ok=True)\n\n        # write the messages to a file\n        filename = \"messages.json\"\n        output_path = os.path.join(\"data\", \"ingest\", \"zulip\", filename)\n        log.info(f\"Writing messages to: {output_path}\")\n        write_json(all_messages, output_path)\n\n\nif __name__ == \"__main__\":\n    main()\n\nPython’s requests module is used for web API calls to GitHub’s REST and GraphQL APIs. The Zulip Python client is used for extracting data from there. Both of these sets of data are saved as JSON files, which we can read in with the Ibis DuckDB or other local backends.\nThe PyPI data is extracted using Ibis from BigQuery using raw SQL strings and saved as Parquet files. This is over ten million rows of data for the main Ibis package, so it takes a few minutes to run. We can also read in the Parquet files with Ibis.",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#extract-transform-load-etl",
    "href": "posts/ibis-analytics/index.html#extract-transform-load-etl",
    "title": "Modern, hybrid, open analytics",
    "section": "Extract, transform, load (ETL)",
    "text": "Extract, transform, load (ETL)\nIt’s fun to just run and watch my system usage spike as DuckDB goes brrrrr:\n\n\n\nSystem usage\n\n\nThis executes the entire Dagster DAG, processing over 10 million rows of data in under a minute on my laptop.\n\nDirected acyclic graph (DAG) and input/output management\nWe use Dagster to manage the ETL pipeline. I chose Dagster to try something new and I like its software-defined assets. Relationships between them in the DAG don’t need to be explicitly defined, and rather are inferred from function names and input parameters. Outside of the input/output management code (shown below), there is almost no Dagster-specific code other than some Python decorators. The rest is just Python code, using Ibis for data processing.\nWe can just dag and view the DAG in a web browser with the Dagster GUI:\n\n\n\nDagster GUI\n\n\nSince this pipeline is so simple, there are no joins and thus no assets with multiple inputs. Of course, joins and other more complex operations are possible with Ibis.\nThe DAG is defined in the assets directory, separated into the three conventional ETL stages:\ndag/assets/\n├── __init__.py\n├── extract/\n│   ├── __init__.py\n│   ├── backends.py\n│   ├── github.py\n│   ├── pypi.py\n│   └── zulip.py\n├── load/\n│   ├── __init__.py\n│   ├── backends.py\n│   ├── github.py\n│   ├── pypi.py\n│   └── zulip.py\n└── transform/\n    ├── __init__.py\n    ├── backends.py\n    ├── github.py\n    ├── pypi.py\n    └── zulip.py\nIbis makes data input/output management easy with file formats or tables in backends. I define some table IO managers and use the DuckDBIOManager as the default for the project.\n\n\nShow the input/output management code\n\nimport os\nimport ibis\n\nfrom dagster import ConfigurableIOManager\n\n\nclass ParquetIOManager(ConfigurableIOManager):\n    \"\"\"\n    Manage tables as parquet files.\n    \"\"\"\n\n    extension: str = \"parquet\"\n    base_path: str = os.path.join(\"data\", \"system\", \"parquet\")\n\n    def handle_output(self, context, obj):\n        dirname, filename = self._get_paths(context)\n        os.makedirs(dirname, exist_ok=True)\n        output_path = os.path.join(dirname, filename)\n        obj.to_parquet(output_path)\n\n    def load_input(self, context):\n        dirname, filename = self._get_paths(context)\n        input_path = os.path.join(dirname, filename)\n        return ibis.read_parquet(input_path)\n\n    def _get_paths(self, context):\n        group_name = context.step_context.job_def.asset_layer.assets_def_for_asset(\n            context.asset_key\n        ).group_names_by_key[context.asset_key]\n        dirname = os.path.join(self.base_path, group_name, *context.asset_key.path[:-1])\n        filename = f\"{context.asset_key.path[-1]}.{self.extension}\"\n        return dirname, filename\n\n\nclass DeltaIOManager(ConfigurableIOManager):\n    \"\"\"\n    Manage tables as delta tables.\n    \"\"\"\n\n    extension: str = \"delta\"\n    base_path: str = os.path.join(\"data\", \"system\", \"delta\")\n    delta_write_mode: str = \"overwrite\"\n\n    def handle_output(self, context, obj):\n        dirname, filename = self._get_paths(context)\n        os.makedirs(dirname, exist_ok=True)\n        output_path = os.path.join(dirname, filename)\n        obj.to_delta(output_path, mode=self.delta_write_mode)\n\n    def load_input(self, context):\n        dirname, filename = self._get_paths(context)\n        input_path = os.path.join(dirname, filename)\n        return ibis.read_delta(input_path)\n\n    def _get_paths(self, context):\n        group_name = context.step_context.job_def.asset_layer.assets_def_for_asset(\n            context.asset_key\n        ).group_names_by_key[context.asset_key]\n        dirname = os.path.join(self.base_path, *context.asset_key.path)\n        filename = f\"{context.asset_key.path[-1]}.{self.extension}\"\n        return dirname, filename\n\n\nclass DuckDBIOManager(ConfigurableIOManager):\n    \"\"\"\n    Manage tables as duckdb files.\n    \"\"\"\n\n    extension: str = \"ddb\"\n    base_path: str = os.path.join(\"data\", \"system\", \"duckdb\")\n\n    def handle_output(self, context, obj):\n        dirname, filename = self._get_paths(context)\n        os.makedirs(dirname, exist_ok=True)\n        output_path = os.path.join(dirname, filename)\n        con = ibis.duckdb.connect(output_path)\n        con.create_table(context.asset_key.path[-1], obj.to_pyarrow(), overwrite=True)\n\n    def load_input(self, context):\n        dirname, filename = self._get_paths(context)\n        input_path = os.path.join(dirname, filename)\n        con = ibis.duckdb.connect(input_path)\n        return con.table(context.asset_key.path[-1])\n\n    def _get_paths(self, context):\n        group_name = context.step_context.job_def.asset_layer.assets_def_for_asset(\n            context.asset_key\n        ).group_names_by_key[context.asset_key]\n        dirname = os.path.join(self.base_path, *context.asset_key.path)\n        filename = f\"{context.asset_key.path[-1]}.{self.extension}\"\n        return dirname, filename\n\nWith this setup, we can swap file formats or backends for storage as desired. After executing the DAG, the following data is written to our local filesystem:\ndata/system/\n└── duckdb/\n    ├── extract_backends/\n    │   └── extract_backends.ddb\n    ├── extract_commits/\n    │   └── extract_commits.ddb\n    ├── extract_downloads/\n    │   └── extract_downloads.ddb\n    ├── extract_forks/\n    │   └── extract_forks.ddb\n    ├── extract_issues/\n    │   └── extract_issues.ddb\n    ├── extract_pulls/\n    │   └── extract_pulls.ddb\n    ├── extract_stars/\n    │   └── extract_stars.ddb\n    ├── extract_watchers/\n    │   └── extract_watchers.ddb\n    ├── extract_zulip_members/\n    │   └── extract_zulip_members.ddb\n    ├── extract_zulip_messages/\n    │   └── extract_zulip_messages.ddb\n    ├── load_backends/\n    │   └── load_backends.ddb\n    ├── load_commits/\n    │   └── load_commits.ddb\n    ├── load_downloads/\n    │   └── load_downloads.ddb\n    ├── load_forks/\n    │   └── load_forks.ddb\n    ├── load_issues/\n    │   └── load_issues.ddb\n    ├── load_pulls/\n    │   └── load_pulls.ddb\n    ├── load_stars/\n    │   └── load_stars.ddb\n    ├── load_watchers/\n    │   └── load_watchers.ddb\n    ├── load_zulip_members/\n    │   └── load_zulip_members.ddb\n    ├── load_zulip_messages/\n    │   └── load_zulip_messages.ddb\n    ├── transform_backends/\n    │   └── transform_backends.ddb\n    ├── transform_commits/\n    │   └── transform_commits.ddb\n    ├── transform_downloads/\n    │   └── transform_downloads.ddb\n    ├── transform_forks/\n    │   └── transform_forks.ddb\n    ├── transform_issues/\n    │   └── transform_issues.ddb\n    ├── transform_pulls/\n    │   └── transform_pulls.ddb\n    ├── transform_stars/\n    │   └── transform_stars.ddb\n    ├── transform_watchers/\n    │   └── transform_watchers.ddb\n    ├── transform_zulip_members/\n    │   └── transform_zulip_members.ddb\n    └── transform_zulip_messages/\n        └── transform_zulip_messages.ddb\nWe can change the configuration in the project to change these to Parquet or Delta Lake tables.\nCommon Python functions are defined in dag/functions.py, including a fancy user-defined function (UDF) for regex matching.\n\n\n\n\n\n\nNote\n\n\n\nA LLM wrote the entire clean_version function, I’m never writing a regex again.\n\n\nimport re\nimport ibis\nimport datetime\n\nimport ibis.selectors as s\n\n\n# udfs\n@ibis.udf.scalar.python\ndef clean_version(version: str, patch: bool = True) -&gt; str:\n    pattern = r\"(\\d+\\.\\d+\\.\\d+)\" if patch else r\"(\\d+\\.\\d+)\"\n    match = re.search(pattern, version)\n    if match:\n        return match.group(1)\n    else:\n        return version\n\n\n# functions\ndef now():\n    return datetime.datetime.now()\n\n\ndef today():\n    return now().date()\n\n\ndef clean_data(t):\n    t = t.rename(\"snake_case\")\n    # t = t.mutate(s.across(s.of_type(\"timestamp\"), lambda x: x.cast(\"timestamp('')\")))\n    return t\n\n\ndef add_ingested_at(t, ingested_at=now()):\n    t = t.mutate(ingested_at=ingested_at).relocate(\"ingested_at\")\n    return t\nThese are imported as from dag import functions as f by convention in the project.\n\n\nExtract\nWe extract the data from the ingested files using Ibis with the default DuckDB backend.\nThe PyPI download data is in Parquet format:\nimport ibis\nimport dagster\n\nfrom dag import functions as f\n\n\n# assets\n@dagster.asset\ndef extract_downloads():\n    \"\"\"\n    Extract the ingested PyPI downloads data.\n    \"\"\"\n    downloads = f.clean_data(\n        ibis.read_parquet(\"data/ingest/pypi/ibis-framework/*.parquet\")\n    )\n    return downloads\nWhile the GitHub (omitted for brevity) and Zulip data is in JSON format:\nimport ibis\nimport dagster\n\nfrom dag import functions as f\n\n\n# assets\n@dagster.asset\ndef extract_zulip_members():\n    \"\"\"\n    Extract the ingested Zulip members data.\n    \"\"\"\n    members = f.clean_data(ibis.read_json(\"data/ingest/zulip/members.json\"))\n    return members\n\n\n@dagster.asset\ndef extract_zulip_messages():\n    \"\"\"\n    Extract the ingested Zulip messages data.\n    \"\"\"\n    messages = f.clean_data(ibis.read_json(\"data/ingest/zulip/messages.json\"))\n    return messages\nAs shown above, Dagster assets are configured to use the DuckDB table manager so the output of these functions are written to separate DuckDB database files for use downstream in the DAG.\n\n\nTransform\nWith the data extracted, we can now transform it into its desired shape for downstream analytics. The most interesting transformation code is the PyPI data, shown here:\nimport ibis\nimport dagster\n\nfrom dag import functions as f\n\n\n# assets\n@dagster.asset\ndef transform_downloads(extract_downloads):\n    \"\"\"\n    Transform the PyPI downloads data.\n    \"\"\"\n    downloads = f.clean_data(\n        extract_downloads.drop(\"project\").unpack(\"file\").unpack(\"details\")\n    )\n    downloads = downloads.mutate(\n        major_minor_patch=f.clean_version(downloads[\"version\"], patch=True),\n        major_minor=f.clean_version(downloads[\"version\"], patch=False).cast(\"float\"),\n    )\n    downloads = downloads.rename(\n        {\n            \"version_raw\": \"version\",\n            \"version\": \"major_minor_patch\",\n        }\n    )\n    downloads = (\n        downloads.group_by(\n            [\n                ibis._.timestamp.truncate(\"D\").name(\"timestamp\"),\n                ibis._.country_code,\n                ibis._.version,\n                ibis._.python,\n                ibis._.system[\"name\"].name(\"system\"),\n            ]\n        )\n        .agg(\n            ibis._.count().name(\"downloads\"),\n        )\n        .order_by(ibis._.timestamp.desc())\n        .mutate(\n            ibis._.downloads.sum()\n            .over(\n                rows=(0, None),\n                group_by=[\"country_code\", \"version\", \"python\", \"system\"],\n                order_by=ibis._.timestamp.desc(),\n            )\n            .name(\"total_downloads\")\n        )\n        .order_by(ibis._.timestamp.desc())\n    )\n    downloads = downloads.mutate(ibis._[\"python\"].fillna(\"\").name(\"python_full\"))\n    downloads = downloads.mutate(\n        f.clean_version(downloads[\"python_full\"], patch=False).name(\"python\")\n    )\n    return downloads\nLike the extract stage, the output of the transform stage is written to DuckDB database files for use downstream in the DAG.\n\n\nLoad\nHonestly this step isn’t doing anything. It could use Ibis to directly upload the tables to MotherDuck, but as implemented these assets are just passing through the transformed data. This is a bit wasteful but allows for a three-stage DAG that conforms to the ETL paradigm.\nimport ibis\nimport dagster\n\nfrom dag import functions as f\n\n\n# assets\n@dagster.asset\ndef load_downloads(transform_downloads):\n    \"\"\"\n    Finalize the PyPI downloads data.\n    \"\"\"\n    return transform_downloads",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#postprocess",
    "href": "posts/ibis-analytics/index.html#postprocess",
    "title": "Modern, hybrid, open analytics",
    "section": "Postprocess",
    "text": "Postprocess\nWe run a postprocessing script to combine the separate data into a single DuckDB database. This step is not necessary, but it makes it easier to query the data locally from a single Ibis connection.\n\n\nShow the postprocessing script\n\nimport os\nimport ibis\nimport fnmatch\n\nimport logging as log\n\nfrom datetime import datetime, timedelta, date\n\n## local imports\nfrom dag import functions as f\n\n\ndef main():\n    postprocess()\n\n\ndef postprocess() -&gt; None:\n    \"\"\"\n    Postprocess the data.\n    \"\"\"\n    # configure logger\n    log.basicConfig(\n        level=log.INFO,\n    )\n\n    # backup loaded data as Delta Lake tables and a DuckDB database\n    source_path = \"data/system/duckdb\"\n    target_path = \"data/data.ddb\"\n\n    os.makedirs(source_path, exist_ok=True)\n\n    target = ibis.duckdb.connect(target_path)\n\n    ingested_at = f.now()\n\n    for root, dirs, files in os.walk(source_path):\n        for file in files:\n            if fnmatch.fnmatch(file, \"load_*.ddb\"):\n                full_path = os.path.join(root, file)\n                con = ibis.duckdb.connect(full_path)\n                tablename = file.replace(\".ddb\", \"\")\n                table = con.table(tablename)\n                tablename = tablename.replace(\"load_\", \"\")\n\n                log.info(f\"Backing up {tablename} to {target_path}...\")\n                target.create_table(tablename, table.to_pyarrow(), overwrite=True)\n\n                log.info(f\"Backing up {tablename} to data/backup/{tablename}.delta...\")\n                table.mutate(ingested_at=ingested_at).to_delta(\n                    f\"data/backup/{tablename}.delta\", mode=\"overwrite\"\n                )\n\n\nif __name__ == \"__main__\":\n    main()\n\nThis script also backs up the data as Delta Lake tables for good measure. After this, our data directory looks like:\ndata/\n├── backup/\n│   ├── backends.delta/\n│   ├── commits.delta/\n│   ├── downloads.delta/\n│   ├── forks.delta/\n│   ├── issues.delta/\n│   ├── pulls.delta/\n│   ├── stars.delta/\n│   ├── watchers.delta/\n│   ├── zulip_members.delta/\n│   └── zulip_messages.delta/\n├── data.ddb\n├── ingest/\n│   ├── github/\n│   ├── pypi/\n│   └── zulip/\n└── system/\n    └── duckdb/",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#exploratory-data-analysis-eda-and-iteration",
    "href": "posts/ibis-analytics/index.html#exploratory-data-analysis-eda-and-iteration",
    "title": "Modern, hybrid, open analytics",
    "section": "Exploratory data analysis (EDA) and iteration",
    "text": "Exploratory data analysis (EDA) and iteration\nWhile EDA is not part of the production pipeline, it is an essential part of the development workflow. The config.toml shown earlier makes it easy to switch between the local data or the production MotherDuck database.\nThe eda.py script in the root of the repo imports useful stuff and connects to the database with Ibis:\nimport re\nimport os\nimport sys\nimport toml\nimport ibis\nimport requests\n\nimport logging as log\nimport plotly.io as pio\nimport ibis.selectors as s\nimport plotly.express as px\n\nfrom rich import print\nfrom dotenv import load_dotenv\nfrom datetime import datetime, timedelta, date\n\n## local imports\nfrom dag import functions as f\nfrom dag.assets import extract, load, transform\n\n# configuration\n## logger\nlog.basicConfig(level=log.INFO)\n\n## config.toml\nconfig = toml.load(\"config.toml\")[\"eda\"]\n\n## load .env file\nload_dotenv()\n\n## ibis config\nibis.options.interactive = True\nibis.options.repr.interactive.max_rows = 20\nibis.options.repr.interactive.max_columns = None\n\n# variables\nNOW = datetime.now()\nNOW_7 = NOW - timedelta(days=7)\nNOW_30 = NOW - timedelta(days=30)\nNOW_90 = NOW - timedelta(days=90)\nNOW_180 = NOW - timedelta(days=180)\nNOW_365 = NOW - timedelta(days=365)\nNOW_10 = NOW - timedelta(days=3650)\n\n# connect to database\ndatabase = config[\"database\"]\nlog.info(f\"database: {database}\")\ncon = ibis.connect(f\"duckdb://{database}\")\nI can then just eda to open a quick iPython session and explore:\n(venv) cody@voda ibis-analytics % just eda\nPython 3.11.5 (main, Sep 14 2023, 13:17:51) [Clang 14.0.3 (clang-1403.0.22.14.1)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.20.0 -- An enhanced Interactive Python. Type '?' for help.\nINFO:root:database: md:ibis_analytics\n\n[ins] In [1]: con.list_tables()\nOut[1]:\n['backends',\n 'commits',\n 'downloads',\n 'forks',\n 'issues',\n 'pulls',\n 'stars',\n 'watchers',\n 'zulip_members',\n 'zulip_messages']\n\n[ins] In [2]: t = con.table(\"stars\")\n\n[ins] In [3]: t.schema()\nOut[3]:\nibis.Schema {\n  starred_at   timestamp\n  id           string\n  login        string\n  name         string\n  company      string\n  created_at   timestamp\n  updated_at   timestamp\n  total_stars  int64\n}\n\n[ins] In [4]: t = t.select(\"starred_at\", \"login\", \"company\")\n\n[ins] In [5]: t.filter(t.company.lower().contains(\"voltron\"))\nOut[5]:\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ starred_at          ┃ login               ┃ company      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ timestamp           │ string              │ string       │\n├─────────────────────┼─────────────────────┼──────────────┤\n│ 2023-08-24 21:09:55 │ ywc88               │ @voltrondata │\n│ 2023-08-10 22:47:14 │ EpsilonPrime        │ Voltron Data │\n│ 2023-08-10 08:14:52 │ fmichonneau         │ @voltrondata │\n│ 2023-08-09 20:16:42 │ paleolimbot         │ @voltrondata │\n│ 2023-08-09 18:31:02 │ ian-flores          │ @voltrondata │\n│ 2023-08-09 18:29:57 │ MattBBaker          │ Voltron Data │\n│ 2023-08-09 18:27:29 │ zeroshade           │ @voltrondata │\n│ 2023-08-09 18:27:10 │ kkraus14            │ @VoltronData │\n│ 2023-08-09 18:26:59 │ cryos               │ @VoltronData │\n│ 2023-08-09 18:26:48 │ austin3dickey       │ Voltron Data │\n│ 2023-08-09 18:26:41 │ assignUser          │ @voltrondata │\n│ 2023-08-09 18:26:39 │ boshek              │ Voltron Data │\n│ 2023-05-04 00:05:23 │ lostmygithubaccount │ Voltron Data │\n│ 2023-04-20 20:42:57 │ richtia             │ Voltron Data │\n│ 2023-04-12 18:58:06 │ ksuarez1423         │ @voltrondata │\n│ 2023-04-06 16:35:01 │ wmalpica            │ Voltron Data │\n│ 2023-03-15 17:04:46 │ felipecrv           │ Voltron Data │\n│ 2023-03-15 15:46:25 │ alistaire47         │ Voltron Data │\n│ 2023-03-14 18:41:58 │ mariusvniekerk      │ @voltrondata │\n│ 2023-02-23 15:36:15 │ andrewseidl         │ @voltrondata │\n│ …                   │ …                   │ …            │\n└─────────────────────┴─────────────────────┴──────────────┘\n\n[ins] In [6]:\nOr I can add from eda import * in a untitled.ipynb for a classic notebook experience in VSCode, which I prefer when plotting or just saving my work.",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#defining-metrics-and-dashboarding",
    "href": "posts/ibis-analytics/index.html#defining-metrics-and-dashboarding",
    "title": "Modern, hybrid, open analytics",
    "section": "Defining metrics and dashboarding",
    "text": "Defining metrics and dashboarding\nThe EDA makes it easy to iteratively define metrics and plot the data. Once I’ve settled on the code I want, I add it to metrics.py with sufficient comments to make it appear in the Streamlit dashboard. It’s convenient to have the metrics defined as code right where the visualization code happens. I can comment it for explanation of metrics definitions and ensure a level of consistency.\n\n\nShow the metrics code\n\nimport toml\nimport ibis\n\nimport streamlit as st\nimport plotly.express as px\n\nfrom dotenv import load_dotenv\nfrom datetime import datetime, timedelta\n\n# options\n## load .env\nload_dotenv()\n\n## config.toml\nconfig = toml.load(\"config.toml\")[\"app\"]\n\n## streamlit config\nst.set_page_config(layout=\"wide\")\n\n## ibis config\ncon = ibis.connect(f\"duckdb://{config['database']}\", read_only=True)\n\n# use precomputed data\nstars = con.table(\"stars\")\nforks = con.table(\"forks\")\npulls = con.table(\"pulls\")\nissues = con.table(\"issues\")\nbackends = con.table(\"backends\")\ndownloads = con.table(\"downloads\")\nmembers = con.table(\"zulip_members\")\nmessages = con.table(\"zulip_messages\")\n\n# display header stuff\nwith open(\"readme.md\") as f:\n    readme_code = f.read()\n\nf\"\"\"\n{readme_code} See [the about page](/about) for more details.\n\"\"\"\n\nwith open(\"requirements.txt\") as f:\n    metrics_code = f.read()\n\nwith st.expander(\"show `requirements.txt`\", expanded=False):\n    st.code(metrics_code, line_numbers=True, language=\"python\")\n\nwith open(\"config.toml\") as f:\n    config_code = f.read()\n\nwith st.expander(\"show `config.toml`\", expanded=False):\n    st.code(config_code, line_numbers=True, language=\"toml\")\n\nwith open(\"justfile\") as f:\n    justfile_code = f.read()\n\nwith st.expander(\"show `justfile`\", expanded=False):\n    st.code(justfile_code, line_numbers=True, language=\"makefile\")\n\nwith open(\".github/workflows/cicd.yaml\") as f:\n    cicd_code = f.read()\n\nwith st.expander(\"show `cicd.yaml`\", expanded=False):\n    st.code(cicd_code, line_numbers=True, language=\"yaml\")\n\nwith open(\"metrics.py\") as f:\n    metrics_code = f.read()\n\nwith st.expander(\"show `metrics.py` (source for this page)\", expanded=False):\n    st.code(metrics_code, line_numbers=True, language=\"python\")\n\n\"\"\"\n---\n\"\"\"\n\n\"\"\"\n## supported backends\n\"\"\"\n\n\ndef fmt_number(value):\n    return f\"{value:,}\"\n\n\ncurrent_backends_total = (\n    backends.filter(backends.ingested_at == backends.ingested_at.max())\n    .num_backends.max()\n    .to_pandas()\n)\ncurrent_backends = backends.backends.unnest().name(\"backends\").as_table()\n\nst.metric(\"Total\", f\"{current_backends_total:,}\")\nst.dataframe(current_backends, use_container_width=True)\n\n\"\"\"\n## totals (all time)\n\"\"\"\n\ntotal_stars_all_time = stars.login.nunique().to_pandas()\ntotal_forks_all_time = forks.login.nunique().to_pandas()\n\ntotal_closed_issues_all_time = issues.number.nunique(\n    where=issues.state == \"closed\"\n).to_pandas()\n\ntotal_merged_pulls_all_time, total_contributors_all_time = (\n    pulls.agg(\n        total_merged_pulls_all_time=pulls.number.nunique(where=pulls.state == \"merged\"),\n        total_contributors_all_time=pulls.login.nunique(\n            where=pulls.merged_at.notnull()\n        ),\n    )\n    .to_pandas()\n    .squeeze()\n)\n\ndownloads_all_time = downloads[\"downloads\"].sum().to_pandas()\n\ntotal_members_all_time = members.user_id.nunique().to_pandas()\ntotal_messages_all_time = messages.id.nunique().to_pandas()\n\n\ncol0, col1, col2, col3 = st.columns(4)\nwith col0:\n    st.metric(\n        label=\"GitHub stars\",\n        value=fmt_number(total_stars_all_time),\n        help=f\"{total_stars_all_time:,}\",\n    )\n    st.metric(\n        label=\"PyPI downloads\",\n        value=fmt_number(downloads_all_time),\n        help=f\"{downloads_all_time:,}\",\n    )\nwith col1:\n    st.metric(\n        label=\"GitHub contributors\",\n        value=fmt_number(total_contributors_all_time),\n        help=f\"{total_contributors_all_time:,}\",\n    )\n    st.metric(\n        label=\"GitHub forks\",\n        value=fmt_number(total_forks_all_time),\n        help=f\"{total_forks_all_time:,}\",\n    )\nwith col2:\n    st.metric(\n        label=\"GitHub PRs merged\",\n        value=fmt_number(total_merged_pulls_all_time),\n        help=f\"{total_merged_pulls_all_time:,}\",\n    )\n    st.metric(\n        label=\"GitHub issues closed\",\n        value=fmt_number(total_closed_issues_all_time),\n        help=f\"{total_closed_issues_all_time:,}\",\n    )\nwith col3:\n    st.metric(\n        label=\"Zulip members\",\n        value=fmt_number(total_members_all_time),\n        help=f\"{total_members_all_time:,}\",\n    )\n    st.metric(\n        label=\"Zulip messages\",\n        value=fmt_number(total_messages_all_time),\n        help=f\"{total_messages_all_time:,}\",\n    )\n\n# variables\nwith st.form(key=\"app\"):\n    days = st.number_input(\n        \"X days\",\n        min_value=1,\n        max_value=365,\n        value=90,\n        step=30,\n        format=\"%d\",\n    )\n    update_button = st.form_submit_button(label=\"update\")\n\n\nSTART = datetime.now() - timedelta(days=days * 2)\nSTOP = datetime.now() - timedelta(days=days)\n\n\n# compute metrics\ntotal_stars, total_stars_prev = (\n    stars.agg(\n        total_stars=stars.login.nunique(where=stars.starred_at &gt;= STOP),\n        total_stars_prev=stars.login.nunique(\n            where=stars.starred_at.between(START, STOP)\n        ),\n    )\n    .to_pandas()\n    .squeeze()\n)\n\ntotal_forks, total_forks_prev = (\n    forks.agg(\n        total_forks=forks.login.nunique(where=forks.created_at &gt;= STOP),\n        total_forks_prev=forks.login.nunique(\n            where=forks.created_at.between(START, STOP)\n        ),\n    )\n    .to_pandas()\n    .squeeze()\n)\n\n(\n    total_issues,\n    total_issues_prev,\n    total_issues_closed,\n    total_issues_closed_prev,\n) = (\n    issues.agg(\n        total_issues=issues.login.nunique(where=issues.created_at &gt;= STOP),\n        total_issues_prev=issues.login.nunique(\n            where=issues.created_at.between(START, STOP)\n        ),\n        total_issues_closed=issues.number.nunique(where=issues.closed_at &gt;= STOP),\n        total_issues_closed_prev=issues.number.nunique(\n            where=issues.closed_at.between(START, STOP)\n        ),\n    )\n    .to_pandas()\n    .squeeze()\n)\n\n(\n    total_pulls,\n    total_pulls_prev,\n    total_pulls_merged,\n    total_pulls_merged_prev,\n    total_contributors,\n    total_contributors_prev,\n) = (\n    pulls.agg(\n        total_pulls=pulls.number.nunique(where=pulls.created_at &gt;= STOP),\n        total_pulls_prev=pulls.number.nunique(\n            where=pulls.created_at.between(START, STOP)\n        ),\n        total_pulls_merged=pulls.number.nunique(where=pulls.merged_at &gt;= STOP),\n        total_pulls_merged_prev=pulls.number.nunique(\n            where=pulls.merged_at.between(START, STOP)\n        ),\n        total_contributors=pulls.login.nunique(where=pulls.merged_at &gt;= STOP),\n        total_contributors_prev=pulls.login.nunique(\n            where=pulls.merged_at.between(START, STOP)\n        ),\n    )\n    .to_pandas()\n    .squeeze()\n)\n\ntotal_downloads, total_downloads_prev = (\n    downloads.agg(\n        total_downloads=downloads.downloads.sum(where=downloads.timestamp &gt;= STOP),\n        total_downloads_prev=downloads.downloads.sum(\n            where=downloads.timestamp.between(START, STOP)\n        ),\n    )\n    .to_pandas()\n    .squeeze()\n)\n\n\ndef delta(current, previous):\n    delta = current - previous\n    pct_change = int(round(100.0 * delta / previous, 0))\n    return f\"{fmt_number(delta)} ({pct_change:d}%)\"\n\n\nf\"\"\"\n## totals (last {days} days)\n\"\"\"\ncol1, col2, col3, col4 = st.columns(4)\nwith col1:\n    st.metric(\n        label=\"GitHub stars\",\n        value=fmt_number(total_stars),\n        delta=delta(total_stars, total_stars_prev),\n        help=f\"{total_stars:,}\",\n    )\n    st.metric(\n        label=\"PyPI downloads\",\n        value=fmt_number(total_downloads),\n        delta=delta(total_downloads, total_downloads_prev),\n        help=f\"{total_downloads:,}\",\n    )\nwith col2:\n    st.metric(\n        label=\"GitHub contributors\",\n        value=fmt_number(total_contributors),\n        delta=delta(total_contributors, total_contributors_prev),\n        help=f\"{total_contributors:,}\",\n    )\n    st.metric(\n        label=\"GitHub forks created\",\n        value=fmt_number(total_forks),\n        delta=delta(total_forks, total_forks_prev),\n        help=f\"{total_forks:,}\",\n    )\nwith col3:\n    st.metric(\n        label=\"GitHub PRs opened\",\n        value=fmt_number(total_pulls),\n        delta=delta(total_pulls, total_pulls_prev),\n        help=f\"{total_pulls:,}\",\n    )\n    st.metric(\n        label=\"GitHub issues opened\",\n        value=fmt_number(total_issues),\n        delta=delta(total_issues, total_issues_prev),\n        help=f\"{total_issues:,}\",\n    )\nwith col4:\n    st.metric(\n        label=\"GitHub PRs merged\",\n        value=fmt_number(total_pulls_merged),\n        delta=delta(total_pulls_merged, total_pulls_merged_prev),\n        help=f\"{total_pulls_merged:,}\",\n    )\n    st.metric(\n        label=\"GitHub issues closed\",\n        value=fmt_number(total_issues_closed),\n        delta=delta(total_issues_closed, total_issues_closed_prev),\n        help=f\"{total_issues_closed:,}\",\n    )\n\nf\"\"\"\n## data (last {days} days)\n\"\"\"\n\n\"\"\"\n### downloads by system and version\n\"\"\"\nc0 = px.bar(\n    downloads.group_by([ibis._.system, ibis._.version])\n    .agg(downloads=lambda t: t.downloads.sum(where=t.timestamp &gt; STOP))\n    .order_by(ibis._.version.desc()),\n    x=\"version\",\n    y=\"downloads\",\n    color=\"system\",\n    title=\"downloads by system and version\",\n)\nst.plotly_chart(c0, use_container_width=True)\n\n\"\"\"\n### stars by company\n\"\"\"\nst.dataframe(\n    stars.group_by(ibis._.company)\n    .agg(stars=lambda t: t.count(where=t.starred_at &gt; STOP))\n    .filter(ibis._.stars &gt; 0)\n    .order_by(ibis._.stars.desc())\n    .to_pandas(),\n    use_container_width=True,\n)\n\n\"\"\"\n### issues by login\n\"\"\"\nc1 = px.bar(\n    issues.group_by([ibis._.login, ibis._.state])\n    .agg(issues=lambda t: t.count(where=t.created_at &gt; STOP))\n    .filter(ibis._.issues &gt; 0)\n    .order_by(ibis._.issues.desc()),\n    x=\"login\",\n    y=\"issues\",\n    color=\"state\",\n    title=\"issues by login\",\n)\nst.plotly_chart(c1, use_container_width=True)\n\n\"\"\"\n### PRs by login\n\"\"\"\nc2 = px.bar(\n    pulls.group_by([ibis._.login, ibis._.state])\n    .agg(pulls=lambda t: t.count(where=t.created_at &gt; STOP))\n    .filter(ibis._.pulls &gt; 0)\n    .order_by(ibis._.pulls.desc()),\n    x=\"login\",\n    y=\"pulls\",\n    color=\"state\",\n    title=\"PRs by login\",\n)\nst.plotly_chart(c2, use_container_width=True)\n\nI can just app to view the dashboard locally in a web browser:\n\n\n\nDashboard local\n\n\n\n\n\nDashboard metrics\n\n\nIf I need more detailed analysis, I can always go back to EDA above and iterate.",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#deploy",
    "href": "posts/ibis-analytics/index.html#deploy",
    "title": "Modern, hybrid, open analytics",
    "section": "Deploy",
    "text": "Deploy\nTo deploy to production, we use MotherDuck for a SaaS database and Streamlit Community Cloud for a SaaS dashboard.\n\nDeploy the database with MotherDuck\nWe just deploy to upload it to the production MotherDuck database.\n\nThis runs some Python code that takes the loaded tables and copies them to MotherDuck:\nimport os\nimport toml\nimport ibis\nimport fnmatch\n\nimport logging as log\n\nfrom datetime import datetime, timedelta, date\n\nimport functions as f\n\n\ndef main():\n    deploy()\n\n\ndef deploy() -&gt; None:\n    \"\"\"\n    Deploy the data.\n    \"\"\"\n    # constants\n    path = \"data/system/duckdb\"\n\n    # configure logging\n    log.basicConfig(\n        level=log.INFO,\n    )\n\n    # load config\n    config = toml.load(\"config.toml\")[\"app\"]\n    log.info(f\"Deploying to {config['database']}...\")\n\n    # connect to the database\n    target = ibis.duckdb.connect(f\"{config['database']}\")\n\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if fnmatch.fnmatch(file, \"load_*.ddb\"):\n                full_path = os.path.join(root, file)\n                con = ibis.duckdb.connect(full_path)\n                tablename = file.replace(\".ddb\", \"\")\n                table = con.table(tablename)\n                tablename = tablename.replace(\"load_\", \"\")\n\n                log.info(f\"\\tDeploying {tablename} to {config['database']}...\")\n                target.create_table(tablename, table.to_pyarrow(), overwrite=True)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\nDeploy the dashboard with Streamlit Community Cloud\nThen, just deploy a Streamlit app from your GitHub repository (a few clicks in the GUI) and you’re done!",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#conclusion",
    "href": "posts/ibis-analytics/index.html#conclusion",
    "title": "Modern, hybrid, open analytics",
    "section": "Conclusion",
    "text": "Conclusion\nThe future is now: it’s open-source, modular, composable, and scalable.\nAn end-to-end analytics pipeline this easy for a product manager to build was not possible just a few years ago. The increasingly modular, composable, and scalable Python data ecosystem has seen an abundance of new libraries that are pushing the limits of what individuals or small teams of data professionals can accomplish, all while efficiently utilizing local and cloud hardware at little to no cost.\nI would love any feedback on this project. The best part is if you don’t like any one component (or prefer another), just swap it out! Altair or plotnine or another visualization library for Plotly. The Polars or DataFusion or Snowflake or PySpark or BigQuery backend for Ibis. A different dashboard library. A different cloud service. A different CI/CD service…\nWhile I wouldn’t consider this a good project template for teams of engineers, it could be used to create one or with very little change adapted for other open-source Python projects. I hope you enjoyed!\nThe source code can be found and stolen from on the ibis-analytics repository – feel free to open an issue, PR, or comment below!",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/ibis-analytics/index.html#next-steps",
    "href": "posts/ibis-analytics/index.html#next-steps",
    "title": "Modern, hybrid, open analytics",
    "section": "Next steps",
    "text": "Next steps\nI plan to add some basic ML forecasting and likely a LLM interface to the dashboard. Follow along with the Ibis project to see what’s next!",
    "crumbs": [
      "Perspectives on data science",
      "Modern, hybrid, open analytics"
    ]
  },
  {
    "objectID": "posts/altair/ames-housing-eda-with-altair.html",
    "href": "posts/altair/ames-housing-eda-with-altair.html",
    "title": "Exploratory data analysis with Altair",
    "section": "",
    "text": "import altair as alt\nimport numpy as np \nimport pandas as pd\n\n\names_data = \"https://github.com/eaisi/discover-projects/blob/main/ames-housing/AmesHousing.csv?raw=true\"\ntrain = pd.read_csv(ames_data).rename(columns=lambda s: s.replace(\" \",\"\"))\n\ntrain\n\n\n\n\n\n\n\n\nOrder\nPID\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\n2926\n923275080\n80\nRL\n37.0\n7937\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nGdPrv\nNaN\n0\n3\n2006\nWD\nNormal\n142500\n\n\n2926\n2927\n923276100\n20\nRL\nNaN\n8885\nPave\nNaN\nIR1\nLow\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2006\nWD\nNormal\n131000\n\n\n2927\n2928\n923400125\n85\nRL\n62.0\n10441\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nShed\n700\n7\n2006\nWD\nNormal\n132000\n\n\n2928\n2929\n924100070\n20\nRL\n77.0\n10010\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2006\nWD\nNormal\n170000\n\n\n2929\n2930\n924151050\n60\nRL\n74.0\n9627\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n11\n2006\nWD\nNormal\n188000\n\n\n\n\n2930 rows × 82 columns",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Exploratory data analysis with Altair"
    ]
  },
  {
    "objectID": "posts/altair/ames-housing-eda-with-altair.html#bar-chart-with-highlighted-bar",
    "href": "posts/altair/ames-housing-eda-with-altair.html#bar-chart-with-highlighted-bar",
    "title": "Exploratory data analysis with Altair",
    "section": "Bar Chart with Highlighted Bar",
    "text": "Bar Chart with Highlighted Bar\nBasic bar chart with a bars highlighted based on the percentage of missing values.\n\nmissing = train.isnull().sum()*100/train.isnull().sum().sum()\nmissing = missing[missing &gt; 0].reset_index()\nmissing.columns = ['Column', 'Count missing']\nmissing.head()\n\n\n\n\n\n\n\n\nColumn\nCount missing\n\n\n\n\n0\nLotFrontage\n3.111309\n\n\n1\nAlley\n17.347133\n\n\n2\nMasVnrType\n11.270557\n\n\n3\nMasVnrArea\n0.146041\n\n\n4\nBsmtQual\n0.507969\n\n\n\n\n\n\n\n\nalt.Chart(missing).mark_bar().encode(\n    x=alt.X('Column', sort='-y'),\n    y='Count missing',\n    color=alt.condition(\n        alt.datum['Count missing'] &gt;10,  # If count missing is &gt; 10%, returns True,\n        alt.value('orange'),             # which sets the bar orange.\n        alt.value('steelblue')           # And if it's not true it sets the bar steelblue.\n    ),\n    tooltip=['Count missing']\n).properties(\n    width=500,\n    height=300\n).configure_axis(\n    grid=False\n)",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Exploratory data analysis with Altair"
    ]
  },
  {
    "objectID": "posts/altair/ames-housing-eda-with-altair.html#boxplot",
    "href": "posts/altair/ames-housing-eda-with-altair.html#boxplot",
    "title": "Exploratory data analysis with Altair",
    "section": "Boxplot",
    "text": "Boxplot\nCreation of a basic boxplot using .mark_boxplot() method\n\nalt.Chart(train).mark_boxplot().encode(\n    x='OverallQual:O',\n    y='SalePrice:Q',\n    color='OverallQual:N'\n).properties(\n    width=500,\n    height=300\n)",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Exploratory data analysis with Altair"
    ]
  },
  {
    "objectID": "posts/altair/ames-housing-eda-with-altair.html#heatmaps",
    "href": "posts/altair/ames-housing-eda-with-altair.html#heatmaps",
    "title": "Exploratory data analysis with Altair",
    "section": "Heatmaps",
    "text": "Heatmaps\nCreation of a basic heatmap using .mark_rect() method.\n\nalt.Chart(train).mark_rect().encode(\n    x='MSZoning',\n    y='ExterQual',\n    color='average(SalePrice)',\n    tooltip=['MSZoning', 'ExterQual', 'average(SalePrice)']\n).properties(\n    width=500,\n    height=300\n)",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Exploratory data analysis with Altair"
    ]
  },
  {
    "objectID": "posts/altair/ames-housing-eda-with-altair.html#bindings-selections-conditions",
    "href": "posts/altair/ames-housing-eda-with-altair.html#bindings-selections-conditions",
    "title": "Exploratory data analysis with Altair",
    "section": "Bindings, Selections & Conditions",
    "text": "Bindings, Selections & Conditions\nHere you can select the KitchenQual feature from a dropdown menu and see how the graph changes color!\n\ninput_dropdown = alt.binding_select(options=list(train['KitchenQual'].unique()), name='Lot Shape')\nselection = alt.selection_point(fields=['KitchenQual'], bind=input_dropdown)\ncolor = alt.condition(selection,\n                      alt.Color('KitchenQual:N', legend=None),\n                      alt.value('lightgray'))\n\nalt.Chart(train).mark_point().encode(\n    x='GrLivArea',\n    y='SalePrice',\n    color=color,\n    tooltip=['GrLivArea', 'SalePrice']\n).properties(\n    width=500,\n    height=300\n).add_params(\n    selection\n).configure_axis(\n    grid=False\n)",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Exploratory data analysis with Altair"
    ]
  },
  {
    "objectID": "posts/altair/ames-housing-eda-with-altair.html#interactive-chart-with-cross-highlight",
    "href": "posts/altair/ames-housing-eda-with-altair.html#interactive-chart-with-cross-highlight",
    "title": "Exploratory data analysis with Altair",
    "section": "Interactive Chart with Cross-Highlight",
    "text": "Interactive Chart with Cross-Highlight\nIn this more advanced example, I use the ExterQual feature as a filter for a binned heatmap.\nClick on the bar chart bars to change the heatmap!\n\npts = alt.selection_point(encodings=['x'])\n\nrect = alt.Chart(train).mark_rect().encode(\n    x=alt.X('GrLivArea', bin=alt.Bin(maxbins=40)),\n    y=alt.Y('GarageArea', bin=alt.Bin(maxbins=40)),\n    color='average(SalePrice)'\n).properties(\n    width=500,\n    height=300\n).transform_filter(\n    pts\n)\n\nbar = alt.Chart(train).mark_bar().encode(\n    x='ExterQual:N',\n    y='count()',\n    color=alt.condition(pts, alt.ColorValue(\"steelblue\"), alt.ColorValue(\"grey\")),\n    tooltip=['ExterQual', 'count()']\n).properties(\n    width=550,\n    height=200\n).add_params(\n    pts\n)\n\n\nalt.vconcat(\n    rect,\n    bar\n).resolve_legend(\n    color=\"independent\",\n    size=\"independent\"\n).configure_axis(\n    grid=False\n)",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Exploratory data analysis with Altair"
    ]
  },
  {
    "objectID": "posts/altair/ames-housing-eda-with-altair.html#dot-dash-plot",
    "href": "posts/altair/ames-housing-eda-with-altair.html#dot-dash-plot",
    "title": "Exploratory data analysis with Altair",
    "section": "Dot Dash Plot",
    "text": "Dot Dash Plot\nA Dot Dash Plot is basically a scatter plot with both axis removed and replaced with barcode plots (aka strip plots), which allow you to see the distribution of values of each measure used in the scatter plot.\n\n# Configure the options common to all layers\nbrush = alt.selection_interval()\nbase = alt.Chart(train).add_params(brush)\n\n# Configure the points\npoints = base.mark_point().encode(\n    x=alt.X('GrLivArea', title=''),\n    y=alt.Y('SalePrice', title=''),\n    color=alt.condition(brush, 'KitchenQual', alt.value('grey'))\n)\n\n# Configure the ticks\ntick_axis = alt.Axis(labels=False, domain=False, ticks=False)\n\nx_ticks = base.mark_tick().encode(\n    alt.X('GrLivArea', axis=tick_axis),\n    alt.Y('KitchenQual', title='', axis=tick_axis),\n    color=alt.condition(brush, 'KitchenQual', alt.value('lightgrey'))\n)\n\ny_ticks = base.mark_tick().encode(\n    alt.X('KitchenQual', title='', axis=tick_axis),\n    alt.Y('SalePrice', axis=tick_axis),\n    color=alt.condition(brush, 'KitchenQual', alt.value('lightgrey'))\n)\n\n# Build the chart\n(\n    y_ticks | (points & x_ticks)\n).configure_axis(\n    grid=False\n)",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Exploratory data analysis with Altair"
    ]
  },
  {
    "objectID": "posts/altair/ames-housing-eda-with-altair.html#multifeature-scatter-plot",
    "href": "posts/altair/ames-housing-eda-with-altair.html#multifeature-scatter-plot",
    "title": "Exploratory data analysis with Altair",
    "section": "Multifeature Scatter Plot",
    "text": "Multifeature Scatter Plot\nLet’s create a scatter plot with multiple feature encodings.\nWith .interactive() you can zoom in. You can also click on legend to select specific KitchenQual values.\n\nselection = alt.selection_point(fields=['KitchenQual'], bind='legend')\n\nalt.Chart(train).mark_circle().encode(\n    alt.X('GrLivArea', scale=alt.Scale(zero=False)),\n    alt.Y('GarageArea', scale=alt.Scale(zero=False, padding=1)),\n    color='KitchenQual',\n    size=alt.Size('SalePrice', bin=alt.Bin(maxbins=10), title='SalePrice'),\n    opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n    tooltip=['GrLivArea', 'SalePrice']\n).properties(\n    width=500,\n    height=500\n).add_params(\n    selection\n).configure_axis(\n    grid=False\n).interactive()",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Exploratory data analysis with Altair"
    ]
  },
  {
    "objectID": "posts/altair/ames-housing-eda-with-altair.html#scatter-matrix",
    "href": "posts/altair/ames-housing-eda-with-altair.html#scatter-matrix",
    "title": "Exploratory data analysis with Altair",
    "section": "Scatter Matrix",
    "text": "Scatter Matrix\nScatter matrix are one of the most common graph you’ll see on Kaggle. It consists of several pair-wise scatter plots of variables presented in a matrix format, useful to visualize multiple relationships between a pair of variables.\nIn Altair this can be achieved using a RepeatChart, let’s see how!\n\nalt.Chart(train).mark_circle().encode(\n    alt.X(alt.repeat(\"column\"), type='quantitative'),\n    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n    color='KitchenQual'\n).properties(\n    width=300,\n    height=300\n).repeat(\n    # Here we tell Altair we want to repeat out scatter plots for each row-column pair\n    row=['GrLivArea', 'GarageArea', 'TotalBsmtSF'],\n    column=['TotalBsmtSF', 'GarageArea', 'GrLivArea']\n).configure_axis(\n    grid=False\n)",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Exploratory data analysis with Altair"
    ]
  },
  {
    "objectID": "posts/altair/ames-housing-eda-with-altair.html#layered-histogram",
    "href": "posts/altair/ames-housing-eda-with-altair.html#layered-histogram",
    "title": "Exploratory data analysis with Altair",
    "section": "Layered Histogram",
    "text": "Layered Histogram\nUsing Altair, we can make overlapping histograms or layers histograms from data:\n\nalt.Chart(train).mark_bar(\n    opacity=0.5,\n    binSpacing=0\n).encode(\n    alt.X('SalePrice:Q', bin=alt.Bin(maxbins=50)),\n    alt.Y('count()', stack=None),\n    alt.Color('MSZoning:N')\n)",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Exploratory data analysis with Altair"
    ]
  },
  {
    "objectID": "posts/altair/comet-chart.html",
    "href": "posts/altair/comet-chart.html",
    "title": "Comet charts in Python",
    "section": "",
    "text": "Zan Armstrong’s comet chart has been on my list of hobby projects for a while now. I think it is an elegant solution to visualize statistical mix effects and address Simpson’s paradox, and particularly useful when working with longitudinal data involving different sub-populations. Recently I found a good excuse to spend some time to actually use it as part of a exploratory data analysis on a project.\nSince I mostly work in Python and have recently fallen in love with Altair — for the same reasons as Fernando explains here — I wondered how the comet chart could be implemented using the grammar of interactive graphics. It took me a while to figure out how to actually plot the comets. In a previous version, I had drawn glyphs using Bokeh. While Altair allows you to plot any SVG path in a graph, this felt a bit hacky and not quite in line with the philosophy of using a grammar of graphics.\nThankfully Mattijn was quick to suggest using trail-marks, after which it was almost as easy as pie. So here’s an example using a dataset of 20,000 flights for 59 destination airports.\n\nimport altair as alt\nimport pandas as pd\nimport vega_datasets\n\n\n# Use airline data to assess statistical mix effects of delays\nflights = vega_datasets.data.flights_20k()\naggregation = dict(\n    number_of_flights=(\"destination\", \"count\"),\n    mean_delay=(\"delay\", \"mean\"),\n    mean_distance=(\"distance\", \"mean\"),\n)\n\n# Compare delays by destination between month 1 and 3\ngrouped = flights.groupby(by=[flights.destination, flights.date.dt.month])\ndf = (\n    grouped.agg(**aggregation)\n    .loc[(slice(None), [1, 3]), :]\n    .assign(\n        change_mean_delay=lambda df:\n            df.groupby(\"destination\")[\"mean_delay\"].diff(),\n    )\n    .fillna(method=\"bfill\")\n    .reset_index()\n    .round(2)\n)\n\n# Calculate weigthed average of delays for month 1 and 3\ntotal = (\n    flights.groupby(flights.date.dt.month)\n    .agg(**aggregation)\n    .loc[[1, 3], :]\n    .assign(\n        change_mean_delay=lambda df: df.mean_delay.diff(),\n        destination='TOTAL'\n    )\n    .fillna(method=\"bfill\")\n    .round(2)\n    .reset_index()\n    .loc[:, df.columns]\n)\n\n\ndef comet_chart(df, stroke=\"white\"):\n    return (\n    alt.Chart(df, width=600, height=450)\n    .mark_trail(stroke=stroke)\n    .encode(\n        x=alt.X(\"number_of_flights\", scale=alt.Scale(type=\"log\")),\n        y=alt.Y(\"mean_delay\"),\n        detail=\"destination\",\n        size=alt.Size(\"date\", scale=alt.Scale(range=[0, 10]), legend=None),\n        tooltip=[\n            \"destination\",\n            \"number_of_flights\",\n            \"mean_delay\",\n            \"change_mean_delay\",\n            \"mean_distance\",\n        ],\n        # trails don't support continuous color\n        # see https://github.com/vega/vega/issues/1187\n        # hence use bins\n        color=alt.Color(\n            \"change_mean_delay:Q\",\n            bin=alt.Bin(step=2),\n            scale=alt.Scale(scheme=\"blueorange\"),\n            legend=alt.Legend(orient=\"top\"),\n        ),\n    )\n)\n\n\ncomet_chart(df) + comet_chart(total, stroke=\"black\")\n\nIn the example shown here, each comet represents one destination airport. The head of the comet corresponds to the most recent observation of the number of flight arrivals (x-axis, shown as logarithmic scale to accommodate the wide range of observations) against the mean delay of those flights (y-axis). The tail of the comet represents a similar (x,y) datum, but from an earlier point in time. Finally, the colour of the comet is encoded to show the change in the mean delay for each airport. A tooltip with a summary of the data is shown when hovering over the head of the comet.\nSo-called mix effects can often lead to misinterpretation of aggregate numbers. In the example of flight delays, the fact that only a small change is observed in the mean delay across all airports — visualized with the right-most comet outlined in black — hides the underlying variance between airports. Note that in this example the size of each sub-population (number of flights per airport) remains relatively constant, hence the comets here only go up and down. As explained in the original article, mix effects become harder to interpret when the relative size of the sub-populations change as well as their relative values. In the most extreme case this may lead to Simpson’s paradox.\nWith this base implementation of comet charts in Altair, you can really go to town and combine it with other interactive graphs. Using the overview-detail pattern, you could plot an accompanying density plot of all the flights for a given airport. That way you can quickly zoom in to the lowest level of detail and get a better understanding of the underlying mix effects.",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Comet charts in Python"
    ]
  },
  {
    "objectID": "posts/hamilton-ibis/index.html",
    "href": "posts/hamilton-ibis/index.html",
    "title": "Portable dataflows with Ibis and Hamilton",
    "section": "",
    "text": "This post showcases how Ibis and Hamilton enable dataflows that span execution over SQL and Python. Ibis is a portable dataframe library to write procedural data transformations in Python and be able to execute them directly on various SQL backends (DuckDB, Snowflake, Postgres, Flink, see full list). Hamilton provides a declarative way to define testable, modular, self-documenting dataflows, that encode lineage and metadata.\nLet’s introduce Ibis before exploring how it pairs with Hamilton.",
    "crumbs": [
      "Perspectives on data science",
      "Portable dataflows with Ibis and Hamilton"
    ]
  },
  {
    "objectID": "posts/hamilton-ibis/index.html#introduction",
    "href": "posts/hamilton-ibis/index.html#introduction",
    "title": "Portable dataflows with Ibis and Hamilton",
    "section": "",
    "text": "This post showcases how Ibis and Hamilton enable dataflows that span execution over SQL and Python. Ibis is a portable dataframe library to write procedural data transformations in Python and be able to execute them directly on various SQL backends (DuckDB, Snowflake, Postgres, Flink, see full list). Hamilton provides a declarative way to define testable, modular, self-documenting dataflows, that encode lineage and metadata.\nLet’s introduce Ibis before exploring how it pairs with Hamilton.",
    "crumbs": [
      "Perspectives on data science",
      "Portable dataflows with Ibis and Hamilton"
    ]
  },
  {
    "objectID": "posts/hamilton-ibis/index.html#standalone-ibis",
    "href": "posts/hamilton-ibis/index.html#standalone-ibis",
    "title": "Portable dataflows with Ibis and Hamilton",
    "section": "Standalone Ibis",
    "text": "Standalone Ibis\nHere’s an Ibis code snippet to load data from a parquet file, compute features, select columns, and filter rows, illustrating typical feature engineering operations.\nReading the code, you’ll notice that:\n\nWe use “expression chaining”, meaning there’s a series of .method() attached one after another.\nThe variable ibis._ is a special character referring to the current expression e.g., ibis._.pet accesses the column “pet” of the current table.\nThe table method .mutate(col1=, col2=, ...) assigns new columns or overwrites existing ones.\n\n\nimport ibis\n\nurl = \"https://storage.googleapis.com/ibis-blog-data-public/hamilton-ibis/absenteeism.parquet\"\nfeature_set = (\n  ibis.read_parquet(sources=url, table_name=\"absenteeism\")\n  .rename(\"snake_case\")\n  .mutate(  # allows us to define new columns\n    has_children=ibis.ifelse(ibis._.son &gt; 0, True, False),\n    has_pet=ibis.ifelse(ibis._.pet &gt; 0, True, False),\n    is_summer_brazil=ibis._.month_of_absence.isin([1, 2, 12]),\n  ).select(\n    \"id\", \"has_children\", \"has_pet\", \"is_summer_brazil\",\n    \"service_time\", \"seasons\", \"disciplinary_failure\",\n    \"absenteeism_time_in_hours\"\n  )\n)\n\n\nChallenge 1 – Maintain and test complex data transformations\nIbis has an SQL-like syntax and supports chaining operations, allowing for powerful queries in a few lines of code. Conversely, there’s a risk of sprawling complexity as expressions are appended, making them harder to test and debug. Preventing this issue requires a lot of upfront discipline and refactoring.\n\n\nChallenge 2 – Orchestrate Ibis code in production\nIbis alleviates a major pain point by enabling data transformations to work across backends. However, moving from dev to prod still requires some code changes such as changing backend connectors, swapping unsupported operators, adding some orchestration and logging execution, wanting to reuse prior code, etc. This is outside the scope of the Ibis project and is expected to be enabled by other means, which usually means bespoke constructs that turn into technical debt.",
    "crumbs": [
      "Perspectives on data science",
      "Portable dataflows with Ibis and Hamilton"
    ]
  },
  {
    "objectID": "posts/hamilton-ibis/index.html#what-is-hamilton",
    "href": "posts/hamilton-ibis/index.html#what-is-hamilton",
    "title": "Portable dataflows with Ibis and Hamilton",
    "section": "What is Hamilton?",
    "text": "What is Hamilton?\nHamilton is a general-purpose framework to write dataflows using regular Python functions. At the core, each function defines a transformation and its parameters indicates its dependencies. Hamilton automatically connects individual functions into a directed acyclic graph (DAG) that can be executed, visualized, optimized, and reported on.\n\n\n\nThe ABC of Hamilton",
    "crumbs": [
      "Perspectives on data science",
      "Portable dataflows with Ibis and Hamilton"
    ]
  },
  {
    "objectID": "posts/hamilton-ibis/index.html#how-hamilton-complements-ibis",
    "href": "posts/hamilton-ibis/index.html#how-hamilton-complements-ibis",
    "title": "Portable dataflows with Ibis and Hamilton",
    "section": "How Hamilton complements Ibis",
    "text": "How Hamilton complements Ibis\nHamilton was initially developed to structure pandas code for a large catalog of features, and has since been adopted by multiple organizations since, and expanded to cover any python object type (Polars, PySpark, ML Models, Numpy, you custom type, etc). Its syntax encourages users to chunk code into meaningful and reusable components, which facilitates documentation, unit testing, code reviews, and improves iteration speed, and the dev to production process. These benefits directly translate to organizing Ibis code.\n\nSolution 1 – Structure your Ibis code with Hamilton\nNow, we’ll refactor the above Ibis code to use Hamilton. Users have the flexibility to chunk code (i.e., what the contents of a function is), at the table or the column-level depending on the needed granularity. This modularity is particularly beneficial to Ibis because:\n\nWell-scoped functions with type annotations and docstring are easier to understand for new Ibis users and facilitate onboarding.\nUnit testing and data validation becomes easier with smaller expressions. These checks become more important when working across backends since the operation coverage varies and bugs may arise.\n\n\nTable-level dataflow\nTable-level operations might feel most familiar to SQL and Spark users. Also, Ibis + Hamilton is reminiscent of dbt for the Python ecosystem.\nWorking with tables is very efficient when your number of columns/features is limited, and you don’t need full column level lineage. As you want to reuse components, you can progressively breakdown “table-level code” in to “column-level code”.\nThe initial Ibis code is now 3 functions with type annotations and docstrings. We have a clear sense of the expected external outputs and we could implement schema checks between functions.\n\nfrom typing import Optional\nimport ibis\nimport ibis.expr.types as ir\n\ndef raw_table(raw_data_path: str) -&gt; ir.Table:\n    \"\"\"Load parquet from `raw_data_path` into a Table expression\n    and format column names to snakecase\n    \"\"\"\n    return (\n        ibis.read_parquet(sources=raw_data_path, table_name=\"absenteism\")\n        .rename(\"snake_case\")\n    )\n\ndef feature_table(raw_table: ir.Table) -&gt; ir.Table:\n    \"\"\"Add to `raw_table` the feature columns `has_children`\n    `has_pet`, and `is_summer_brazil`\n    \"\"\"\n    return raw_table.mutate(\n        has_children=(ibis.ifelse(ibis._.son &gt; 0, True, False)),\n        has_pet=ibis.ifelse(ibis._.pet &gt; 0, True, False),\n        is_summer_brazil=ibis._.month_of_absence.isin([1, 2, 12]),\n    )\n\ndef feature_set(\n    feature_table: ir.Table,\n    feature_selection: list[str],\n    condition: Optional[ibis.common.deferred.Deferred] = None,\n) -&gt; ir.Table:\n    \"\"\"Select feature columns and filter rows\"\"\"\n    return feature_table[feature_selection].filter(condition)\n\n\n\n\nTable-level lineage with Hamilton\n\n\n\n\nColumn-level dataflow\nHamilton was initially built to expose and manage column-level operations, which is most common in dataframe libraries (pandas, Dask, polars).\nColumn-level code leads to fully-reusable feature definitions and a highly granular level of lineage. Notably, this allows one to trace sensitive data and evaluate downstream impacts of code changes. However, it is more verbose to get started with, but remember that code is read more often than written.\nNow, the raw_table is loaded and the columns son, pet, and month_of_absence are extracted to engineer new features. After transformations, features are joined with raw_table to create feature_table.\n\nimport ibis\nimport ibis.expr.types as ir\nfrom hamilton.function_modifiers import extract_columns\nfrom hamilton.plugins import ibis_extensions\n\n# extract specific columns from the table\n@extract_columns(\"son\", \"pet\", \"month_of_absence\")\ndef raw_table(raw_data_path: str) -&gt; ir.Table:\n    \"\"\"Load the parquet found at `raw_data_path` into a Table expression\n    and format columns to snakecase\n    \"\"\"\n    return (\n        ibis.read_parquet(sources=raw_data_path, table_name=\"absenteism\")\n        .rename(\"snake_case\")\n    )\n\n# accesses a single column from `raw_table`\ndef has_children(son: ir.Column) -&gt; ir.BooleanColumn:\n    \"\"\"True if someone has any children\"\"\"\n    return ibis.ifelse(son &gt; 0, True, False)\n\n# narrows the return type from `ir.Column` to `ir.BooleanColumn`\ndef has_pet(pet: ir.Column) -&gt; ir.BooleanColumn:\n    \"\"\"True if someone has any pets\"\"\"\n    return ibis.ifelse(pet &gt; 0, True, False).cast(bool)\n\n# typing and docstring provides business context to features\ndef is_summer_brazil(month_of_absence: ir.Column) -&gt; ir.BooleanColumn:\n    \"\"\"True if it is summer in Brazil during this month\n\n    People in the northern hemisphere are likely to take vacations\n    to warm places when it's cold locally\n    \"\"\"\n    return month_of_absence.isin([1, 2, 12])\n\ndef feature_table(\n    raw_table: ir.Table,\n    has_children: ir.BooleanColumn,\n    has_pet: ir.BooleanColumn,\n    is_summer_brazil: ir.BooleanColumn,\n) -&gt; ir.Table:\n    \"\"\"Join computed features to the `raw_data` table\"\"\"\n    return raw_table.mutate(\n        has_children=has_children,\n        has_pet=has_pet,\n        is_summer_brazil=is_summer_brazil,\n    )\n\ndef feature_set(\n    feature_table: ir.Table,\n    feature_selection: list[str],\n    condition: Optional[ibis.common.deferred.Deferred] = None,\n) -&gt; ir.Table:\n    \"\"\"Select feature columns and filter rows\"\"\"\n    return feature_table[feature_selection].filter(condition)\n\nUsing pre-1.0.0 Polars integration -- we will stop supporting this in Hamilton 2.0, so please upgrade your version of polars! Current version: 0.20.31, minimum required version: 1.0.0.\n\n\n\n\n\nColumn-level lineage with Hamilton\n\n\n\n\n\nSolution 2 – Orchestrate Ibis anywhere\nHamilton is an ideal way to orchestrate Ibis code because it has a very small dependency footprint and will run anywhere Python does (script, notebook, FastAPI, Streamlit, pyodide, etc.) In fact, the Hamilton library only has four dependencies. You don’t need “framework code” to get started, just plain Python functions. When moving to production, Hamilton has all the necessary features to complement Ibis such as swapping components, configurations, and lifecycle hooks for logging, alerting, and telemetry.\nA simple usage pattern of Hamilton + Ibis is to use the @config.when function modifier. In the following example, we have alternative implementations for the backend connection, which will be used for computing and storing results. When running your code, specify in your config backend=\"duckdb\" or backend=\"bigquery\" to swap between the two.\n\n# ibis_dataflow.py\nimport ibis\nimport ibis.expr.types as ir\nfrom hamilton.function_modifiers import config\n\n# ... entire dataflow definition\n\n@config.when(backend=\"duckdb\")\ndef backend_connection__duckdb(\n    connection_string: str\n) -&gt; ibis.backends.BaseBackend:\n    \"\"\"Connect to DuckDB backend\"\"\"\n    return ibis.duckdb.connect(connection_string)\n\n@config.when(backend=\"bigquery\")\ndef backend_connection__bigquery(\n    project_id: str,\n    dataset_id: str,\n) -&gt; ibis.backends.BaseBackend:\n    \"\"\"Connect to BigQuery backend\n    Install dependencies via `pip install ibis-framework[bigquery]`\n    \"\"\"\n    return ibis.bigquery.connect(\n        project_id=project_id,\n        dataset_id=dataset_id,\n    )\n\ndef insert_results(\n    backend_connection: ibis.backends.BaseBackend,\n    result_table: ir.Table,\n    table_name: str\n) -&gt; None:\n    \"\"\"Execute expression and insert results\"\"\"\n    backend_connection.insert(\n        table_name=table_name,\n        obj=result_table\n    )",
    "crumbs": [
      "Perspectives on data science",
      "Portable dataflows with Ibis and Hamilton"
    ]
  },
  {
    "objectID": "posts/hamilton-ibis/index.html#performance-boost",
    "href": "posts/hamilton-ibis/index.html#performance-boost",
    "title": "Portable dataflows with Ibis and Hamilton",
    "section": "Performance boost",
    "text": "Performance boost\nLeveraging DuckDB as the default backend, Hamilton users migrating to Ibis should immediately find performance improvements both for local dev and production. In addition, the portability of Ibis has the potential to greatly reduce development time.",
    "crumbs": [
      "Perspectives on data science",
      "Portable dataflows with Ibis and Hamilton"
    ]
  },
  {
    "objectID": "posts/hamilton-ibis/index.html#atomic-data-transformation-documentation",
    "href": "posts/hamilton-ibis/index.html#atomic-data-transformation-documentation",
    "title": "Portable dataflows with Ibis and Hamilton",
    "section": "Atomic data transformation documentation",
    "text": "Atomic data transformation documentation\nHamilton can directly produce a dataflow visualization from code, helping with project documentation. Ibis pushes this one step further by providing a detailed view of the query plan and schemas. See this Ibis visualization for the column-level Hamilton dataflow defined above. It includes all renaming, type casting, and transformations steps (Please open the image in a new tab and zoom in 🔎).",
    "crumbs": [
      "Perspectives on data science",
      "Portable dataflows with Ibis and Hamilton"
    ]
  },
  {
    "objectID": "posts/hamilton-ibis/index.html#working-across-rows-with-user-defined-functions-udfs",
    "href": "posts/hamilton-ibis/index.html#working-across-rows-with-user-defined-functions-udfs",
    "title": "Portable dataflows with Ibis and Hamilton",
    "section": "Working across rows with user-defined functions (UDFs)",
    "text": "Working across rows with user-defined functions (UDFs)\nHamilton and most backends are designed to work primarily on tables and columns, but sometimes you’d like to operate over a row (think of pd.DataFrame.apply()). However, pivoting tables is costly and manually iterating over rows to collect values and create a new column is quickly inconvenient. By using scalar user-defined functions (UDFs), Ibis makes it possible to execute arbitrary Python code on rows directly on the backend.\n\n\n\n\n\n\nNote\n\n\n\nUsing @ibis.udf.scalar.python creates a non-vectorized function that iterates row-by-row. See the docs to use backend-specific UDFs with @ibis.udf.scalar.builtin and create vectorized scalar UDFs.\n\n\nFor instance, you could embed rows of a text column using an LLM API using your existing data warehouse infrastructure.\n\nimport ibis\nimport ibis.expr.types as ir\n\ndef documents(path: str) -&gt; ir.Table:\n    \"\"\"load text documents from file\"\"\"\n    return ibis.read_parquet(sources=path, table_name=\"documents\")\n\n# function name starts with `_` to prevent being added as a node\n@ibis.udf.scalar.python\ndef _generate_summary(author: str, text: str, prompt_template: str) -&gt; str:\n    \"\"\"UDF Function to call the OpenAI API line by line\"\"\"\n    prompt = prompt_template.format(author=author, text=text)\n    client = openai.OpenAI(...)\n    try:\n        response = client.chat.completions.create(...)\n        return_value = response.choices[0].message.content\n    except Exception:\n        return_value = \"\"\n    return return_value\n\n\ndef prompt_template() -&gt; str:\n    return \"\"\"summarize the following text from {author} and add\n    contextual notes based on it biography and other written work\n\n    TEXT\n    {text}\n    \"\"\"\n\ndef summaries(documents: ir.Table, prompt_template: str) -&gt; ir.Table:\n    \"\"\"Compute the UDF against the family\"\"\"\n    return documents.mutate(\n        summary=_generated_summary(\n            _.author,\n            _.text,\n            prompt_template=prompt_template\n        )\n    )",
    "crumbs": [
      "Perspectives on data science",
      "Portable dataflows with Ibis and Hamilton"
    ]
  },
  {
    "objectID": "posts/precision-recall/index.html",
    "href": "posts/precision-recall/index.html",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "",
    "text": "A receiver operating characteristic (ROC) curve displays how well a model can classify binary outcomes. An ROC curve is generated by plotting the false positive rate of a model against its true positive rate, for each possible cutoff value. Often, the area under the curve (AUC) is calculated and used as a metric showing how well a model can classify data points.\nIf you’re interest in learning more about ROC and AUC, I recommend this short Medium blog, which contains this neat graphic.\n\nDariya Sydykova, graduate student at the Wilke lab at the University of Texas at Austin, shared some great visual animations of how model accuracy and model cutoffs alter the ROC curve and the AUC metric. The quotes and animations are from the associated github repository.",
    "crumbs": [
      "Perspectives on data science",
      "The Precision-Recall Plot Is More Informative than the ROC Plot"
    ]
  },
  {
    "objectID": "posts/precision-recall/index.html#roc-auc-precision-and-recall-visually-explained",
    "href": "posts/precision-recall/index.html#roc-auc-precision-and-recall-visually-explained",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "",
    "text": "A receiver operating characteristic (ROC) curve displays how well a model can classify binary outcomes. An ROC curve is generated by plotting the false positive rate of a model against its true positive rate, for each possible cutoff value. Often, the area under the curve (AUC) is calculated and used as a metric showing how well a model can classify data points.\nIf you’re interest in learning more about ROC and AUC, I recommend this short Medium blog, which contains this neat graphic.\n\nDariya Sydykova, graduate student at the Wilke lab at the University of Texas at Austin, shared some great visual animations of how model accuracy and model cutoffs alter the ROC curve and the AUC metric. The quotes and animations are from the associated github repository.",
    "crumbs": [
      "Perspectives on data science",
      "The Precision-Recall Plot Is More Informative than the ROC Plot"
    ]
  },
  {
    "objectID": "posts/precision-recall/index.html#roc-auc",
    "href": "posts/precision-recall/index.html#roc-auc",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "ROC & AUC",
    "text": "ROC & AUC\nThe plot on the left shows the distributions of predictors for the two outcomes, and the plot on the right shows the ROC curve for these distributions. The vertical line that travels left-to-right is the cutoff value. The red dot that travels along the ROC curve corresponds to the false positive rate and the true positive rate for the cutoff value given in the plot on the left.\nThe traveling cutoff demonstrates the trade-off between trying to classify one outcome correctly and trying to classify the other outcome correcly. When we try to increase the true positive rate, we also increase the false positive rate. When we try to decrease the false positive rate, we decrease the true positive rate.\n\nThe shape of an ROC curve changes when a model changes the way it classifies the two outcomes.\nThe animation [below] starts with a model that cannot tell one outcome from the other, and the two distributions completely overlap (essentially a random classifier). As the two distributions separate, the ROC curve approaches the left-top corner, and the AUC value of the curve increases. When the model can perfectly separate the two outcomes, the ROC curve forms a right angle and the AUC becomes 1.",
    "crumbs": [
      "Perspectives on data science",
      "The Precision-Recall Plot Is More Informative than the ROC Plot"
    ]
  },
  {
    "objectID": "posts/precision-recall/index.html#precision-recall",
    "href": "posts/precision-recall/index.html#precision-recall",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Precision-recall",
    "text": "Precision-recall\nTwo other metrics that are often used to quantify model performance are precision and recall.\nPrecision (also called positive predictive value) is defined as the number of true positives divided by the total number of positive predictions. Hence, precision quantifies what percentage of the positive predictions were correct: How correct your model’s positive predictions were.\nRecall (also called sensitivity) is defined as the number of true positives divided by the total number of true postives and false negatives (i.e. all actual positives). Hence, recall quantifies what percentage of the actual positives you were able to identify: How sensitive your model was in identifying positives.\nDariya also made some visualizations of precision-recall curves: precision-recall curves also displays how well a model can classify binary outcomes. However, it does it differently from the way an ROC curve does. Precision-recall curve plots true positive rate (recall or sensitivity) against the positive predictive value (precision).\nIn the middle, here below, the ROC curve with AUC. On the right, the associated precision-recall curve. Similarly to the ROC curve, when the two outcomes separate, precision-recall curves will approach the top-right corner. Typically, a model that produces a precision-recall curve that is closer to the top-right corner is better than a model that produces a precision-recall curve that is skewed towards the bottom of the plot.",
    "crumbs": [
      "Perspectives on data science",
      "The Precision-Recall Plot Is More Informative than the ROC Plot"
    ]
  },
  {
    "objectID": "posts/precision-recall/index.html#class-imbalance",
    "href": "posts/precision-recall/index.html#class-imbalance",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Class imbalance",
    "text": "Class imbalance\nClass imbalance happens when the number of outputs in one class is different from the number of outputs in another class. For example, one of the distributions has 1000 observations and the other has 10. An ROC curve tends to be more robust to class imbalanace that a precision-recall curve.\nIn this animation [below], both distributions start with 1000 outcomes. The blue one is then reduced to 50. The precision-recall curve changes shape more drastically than the ROC curve, and the AUC value mostly stays the same. We also observe this behaviour when the other disribution is reduced to 50.\n\nHere’s the same, but now with the red distribution shrinking to just 50 samples.",
    "crumbs": [
      "Perspectives on data science",
      "The Precision-Recall Plot Is More Informative than the ROC Plot"
    ]
  },
  {
    "objectID": "posts/precision-recall/index.html#further-reading",
    "href": "posts/precision-recall/index.html#further-reading",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Further reading",
    "text": "Further reading\n\nSaito & Rehmsmeier (2015), The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets",
    "crumbs": [
      "Perspectives on data science",
      "The Precision-Recall Plot Is More Informative than the ROC Plot"
    ]
  },
  {
    "objectID": "cases/verkeersveiligheidmodel/index.html",
    "href": "cases/verkeersveiligheidmodel/index.html",
    "title": "Verkeersveiligheidmodel",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Use cases & applications",
      "Verkeersveiligheidmodel"
    ]
  },
  {
    "objectID": "cases/landscape-biomedical-research/index.html",
    "href": "cases/landscape-biomedical-research/index.html",
    "title": "Predict precence of wolf pairs in Germany with XGBoost and SHAP",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Use cases & applications",
      "Predict precence of wolf pairs in Germany with XGBoost and SHAP"
    ]
  },
  {
    "objectID": "cases/energiearmoede/index.html",
    "href": "cases/energiearmoede/index.html",
    "title": "Inzicht in energiearmoede",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Use cases & applications",
      "Inzicht in energiearmoede"
    ]
  },
  {
    "objectID": "cases/rice-grain-classification/index.html",
    "href": "cases/rice-grain-classification/index.html",
    "title": "Classifying rice grains using deep learning",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Use cases & applications",
      "Classifying rice grains using deep learning"
    ]
  },
  {
    "objectID": "cases/index.html",
    "href": "cases/index.html",
    "title": "Selected use cases & applications",
    "section": "",
    "text": "Predict precence of wolf pairs in Germany with XGBoost and SHAP\n\n\n\nNLP\n\n\nBERT\n\n\nLLM\n\n\n\nOver 1.5 million scientific articles on biomedicine and life sciences are now published and collected in the PubMed database every year. This vast scale makes it challenging…\n\n\n\nRita González-Márquez et al.\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredict precence of wolf pairs in Germany with XGBoost and SHAP\n\n\n\nExplainable AI\n\n\n\nWolves have returned to Germany since 2000. Numbers have grown to 209 territorial pairs in 2021. XGBoost machine learning, combined with SHAP analysis is applied to predict…\n\n\n\nJeanine Schoonemann et al.\n\n\nFeb 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying rice grains using deep learning\n\n\n\nDeep Learning\n\n\n\nRice, one of the world’s most significant agricultural products, is crucial for human nutrition, economies, and various industrial sectors. Classifying rice varieties, an…\n\n\n\nFarshad Farahnakian et al.\n\n\nNov 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInzicht in energiearmoede\n\n\n\nNederlands\n\n\n\nIn januari 2022 zijn energieprijzen met 86% gestegen ten opzichte van het jaar daarvoor. Met de oorlog in Oekraïne en de inflatie zijn de prijzen alleen nog maar hoger…\n\n\n\nAnouk Fredriksz\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVerkeersveiligheidmodel\n\n\n\nNederlands\n\n\n\nIn het Verkeersveiligheidsmodel koppelen we data over historische ongevallen aan gegevens over de weg, over het verkeer en over de omgeving binnen de invloedsfeer van de…\n\n\n\nArjan Knol\n\n\nAug 1, 2019\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Use cases & applications"
    ]
  },
  {
    "objectID": "cases/wolves-in-germany/index.html",
    "href": "cases/wolves-in-germany/index.html",
    "title": "Predict precence of wolf pairs in Germany with XGBoost and SHAP",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Use cases & applications",
      "Predict precence of wolf pairs in Germany with XGBoost and SHAP"
    ]
  },
  {
    "objectID": "books/islp.html",
    "href": "books/islp.html",
    "title": "An Introduction to Statistical Learning",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Books",
      "Introduction to Statistical Learning"
    ]
  },
  {
    "objectID": "books/udl.html",
    "href": "books/udl.html",
    "title": "Understanding Deep Learning",
    "section": "",
    "text": "Unable to display PDF file. Download instead.",
    "crumbs": [
      "Books",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/design-philosophy/index.html",
    "href": "posts/design-philosophy/index.html",
    "title": "The Design Philosophy of Great Tables",
    "section": "",
    "text": "We’ve spent a lot of time thinking about tables. Tables—like plots—are crucial as a last step toward presenting information. There is surprising sophistication and nuance in designing effective tables. Over the past 5,000 years, they’ve evolved from simple grids to highly structured displays of data. Although we argue that the mid-1900s served as a high point, the popularization and wider accessibility of computing seemingly brought us back to the simple, ancient times.\nOkay, it’s not all that bad but the workers of data are today confronted with an all-too-familiar dilemma: copy your data into a tool like Excel to make the table, or, display an otherwise unpolished table. Through the exploration of the qualities that make tables shine, the backstory of tables as a display of data, and the issues faced today, it’s clear how we can solve the great table dilemma with Great Tables.\nTables made with computers (left to right): (1) a DataFrame printed at the console, (2) an Excel table, and (3) a Great Tables table.",
    "crumbs": [
      "Perspectives on data science",
      "The Design Philosophy of Great Tables"
    ]
  },
  {
    "objectID": "posts/design-philosophy/index.html#footnotes",
    "href": "posts/design-philosophy/index.html#footnotes",
    "title": "The Design Philosophy of Great Tables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTaylor, B. (2021). Lunar timekeeping in Upper Paleolithic Cave Art. PRAEHISTORIA New Series, 3(13), 215–232.↩︎\nDuke, D. W. (2002). Hipparchus’ Coordinate System. Archive for History of Exact Sciences, 56(5), 427-433.↩︎\nhttps://en.wikipedia.org/wiki/Geography_(Ptolemy)↩︎\nPalet, J. M. and Orengo, H. A., The Roman Centuriated Landscape: Conception, Genesis, and Development as Inferred from the Ager Tarraconensis Case. American Journal of Archaeology, 115(3), 383-402.↩︎\nMarchese, F. T., Exploring the Origins of Tables for Information Visualization. Proceedings of the 2011 15th International Conference on Information Visualisation, 13-15 July 2011, doi:10.1109/IV.2011.36.↩︎\nM. W. Green, The construction and implementation of the cuneiform writing system, Visible Writing, 15, 1981, 345-72.↩︎\nRobson, E., “Tables and tabular formatting in Sumer, Babylonia, and Assyria, 2500-50 BCE” in M. Campbell-Kelly, M. Croarken, R.G. Flood, and E. Robson (eds.), The History of Mathematical Tables from Sumer to Spreadsheets. Oxford: Oxford University Press, 2003, 18–47.↩︎\nhttps://site.xavier.edu/polt/typewriters/varityper.html↩︎\nManual of Tabular Presentation: An Outline of Theory and Practice in the Presentation of Statistical Data in Tables for Publication. United States. Bureau of the Census. U.S. Government Printing Office, 1949. Resource available at: https://www2.census.gov/library/publications/1949/general/tabular-presentation.pdf.↩︎",
    "crumbs": [
      "Perspectives on data science",
      "The Design Philosophy of Great Tables"
    ]
  },
  {
    "objectID": "posts/altair/better-altair-theme.html",
    "href": "posts/altair/better-altair-theme.html",
    "title": "Amazing Altair with an even better theme",
    "section": "",
    "text": "I have been in love with Altair ever since I first ran into it. The logic, the structure, the syntax: it’s all just there and it helps me to get my work done. Paired with Quarto or Streamlit, I can create stunning pdf reports, interactive documents and single-page apps for my clients, all from one codebase. After attending a workshop by my fellow data viz afficianado Sara Sprinkhuizen, I wanted to implement her tried-and-tested best practices for charts into an Altair.\nIn this post, we will illustate how these improvements work for a handful of visualizations that are often used for exploratory data analysis. We will implement her design principles in an Altair theme, such that we can consistently generate better charts in our workflow. Let’s start and see how Altair’s default theme renders a scatterplot, bar chart and line chart.\nCode\nimport altair as alt\nimport ibis\nfrom vega_datasets import data\n\n\nibis.options.interactive = True\n\npenguins = ibis.examples.penguins.fetch()\nstocks = data.stocks()\nwheat = data.wheat()",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Amazing Altair with an even better theme"
    ]
  },
  {
    "objectID": "posts/altair/better-altair-theme.html#scatterplot",
    "href": "posts/altair/better-altair-theme.html#scatterplot",
    "title": "Amazing Altair with an even better theme",
    "section": "Scatterplot",
    "text": "Scatterplot\nA scatterplot is one of the most commonly used visualizations. In this case we encode three values for the Palmer penguins data: bill length vs. bill depth, per species. The only optimization we apply is to have the axis not go all the way to zero, so the charts is focused on the actual datapoints.\n\n\nCode\nscatterplot = (\n    alt.Chart(penguins)\n    .mark_circle(size=60)\n    .encode(\n        x=alt.X(\"bill_length_mm:Q\", scale=alt.Scale(zero=False)),\n        y=alt.Y(\"bill_depth_mm:Q\", scale=alt.Scale(zero=False)),\n        color=alt.Color(\"species\"),\n        tooltip=[\"species\", \"sex\", \"island\", \"bill_length_mm:Q\", \"bill_depth_mm:Q\"],\n    )\n)\nscatterplot",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Amazing Altair with an even better theme"
    ]
  },
  {
    "objectID": "posts/altair/better-altair-theme.html#bar-chart",
    "href": "posts/altair/better-altair-theme.html#bar-chart",
    "title": "Amazing Altair with an even better theme",
    "section": "Bar chart",
    "text": "Bar chart\nNext up is the trusty ol’ bar chart, which we demonstrate using the wheat dataset. This chart is challenging because it spans two and a half centuries of yearly data, leading to a wide chart with the default settings. Did you turn your head to read the year labels on the x-axis? Or to figure out that the y-axis title reads wheat? Surely there must be a better way.\n\n\nCode\nbarchart = (\n    alt.Chart(wheat)\n    .mark_bar()\n    .encode(x=\"year:O\", y=\"wheat:Q\", tooltip=[\"year:O\", \"wheat:Q\"])\n)\nbarchart",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Amazing Altair with an even better theme"
    ]
  },
  {
    "objectID": "posts/altair/better-altair-theme.html#line-chart",
    "href": "posts/altair/better-altair-theme.html#line-chart",
    "title": "Amazing Altair with an even better theme",
    "section": "Line chart",
    "text": "Line chart\nA line chart with a legend, in this case showing stock prices of a handful of tech companies, is also something we use on a regular basis. Did you even notice the effort you have to put in to lookup the color of the line in the legend? I actually had a hard time distinguishing Amazon and Google, as the red and orange are very alike to my eyes.\n\n\nCode\nchart = alt.Chart(stocks).encode(color=alt.Color(\"symbol\"))\nline = chart.mark_line().encode(\n    x=alt.X(\"date:T\", axis=alt.Axis(title=\"date\")),\n    y=alt.Y(\"price:Q\", axis=alt.Axis(title=\"price\")),\n    tooltip=[\"date\", \"price\"],\n)\nline",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Amazing Altair with an even better theme"
    ]
  },
  {
    "objectID": "posts/altair/better-altair-theme.html#some-simple-but-sensible-design-principles",
    "href": "posts/altair/better-altair-theme.html#some-simple-but-sensible-design-principles",
    "title": "Amazing Altair with an even better theme",
    "section": "Some simple but sensible design principles",
    "text": "Some simple but sensible design principles\nSara’s main points for improving the standard layout of charts basically boil down to the following princples:\n\nuse informative action titles and subtitles: much like effective presentations, using so-called action titles will greatly increase the impact of your data visualization. Formulated as a short, simple sentence, action titles should ideally be the main takeaway or ‘so what’ of the chart, and – if done right – allows the audience to only read the title to understand the primary message of the visual.\nless is more: before minimalism became all the rage in design, Edward Tufte coined the principle of maximizing the data-to-ink ratio. Charts are easier to read by removing unnecessary clutter. So get rid of gridlines, background colors and the like.\nkeep it horizontal: text is easier to read when it is horizontally aligned. So straighten up those rotated x-axis labels and y-axis titles\nlose the legends: having to lookup colors increases the cognitive load of reading a chart. Use annotations instead.\n\nAll these principles can be readily implemented in Altair. Sergio Sánchez in fact captured the first three principles when implementing the Urban Institute styleguide (shown below) as an Altair theme.\n\nBuilding on his work, we created our on theme called Okabe Ito, in honour of their work in defining colorblind-safe color palettes. Not only is it functional, but I really like the muted colors, too. Below is a visual from their original paper that also shows how different types of colorblindness (protan, deutan, tritan) results in different perception of the colors.\n\nTo use this template, download okabe-ito-theme.py, put it in the same folder as your notebook and add the following two cells.\nTo register the template as an Altair theme:\n\n\nCode\n%run ../resources/okabe-ito-theme.py\n\n\nTo download the fonts used in the template:\n\n\nCode\n%%html\n&lt;style&gt;\n@import url('https://fonts.googleapis.com/css?family=Lato');\n&lt;/style&gt;\n\n\n\n\n\nLet’s see how this changes the charts.\n\nAdding titles\nThe theme automatically has been configured to improve the data-to-ink ratio and align all texts horizontally. Adding action titles can be done using alt.TitleParams, with which you can not only add a title, but also add multiple subtitles lines which you pass as a list.\n\n\nCode\n(\n    scatterplot.properties(\n        title=alt.TitleParams(\n            \"Bill length vs. bill depth\", subtitle=[\"Did you spot Simpson's Paradox?\"]\n        )\n    )\n)\n\n\n\n\n\n\n\n\n\n\nEverything horizontal\nThe effect of aligning all text horizontally is a significant improvement on the bar chart, which is now much easier to read. Adding a two-line subtitle make the action title even more compelling. Using alt.mark_text() we can easily add footnotes as well, in this case to provide references to the claim.\n\n\nCode\nfootnote = alt.Chart(wheat).mark_text(\n    text=\"*Source: https://en.wikipedia.org/wiki/British_Agricultural_Revolution\",\n    color=\"#000000\",\n    x=0,\n    y=\"height\",\n    dy=60,\n    align=\"left\",\n)\n\nalt.layer(barchart, footnote).properties(\n    title=alt.TitleParams(\n        \"What's Up With Wheat\",\n        subtitle=[\n            \"An example of the impact of the agricultural revolution\",\n            \"Productivity increased through mechanization*\",\n        ],\n    ),\n)\n\n\n\n\n\n\n\n\n\n\nLose the legends\nAlthough legends are fine for exploratory data analysis, where you don’t want to spend too much time tweaking your charts, for explanatory data analysis we want to make it as easy on the reader as possible. Unforunately, I couldn’t find a way to add annotations (instead of a legend) using Altair’s theming engine. Here’s a first shot by implementing it manually.\n\nchart = alt.Chart(stocks).encode(color=alt.Color(\"symbol\", legend=None))\nline = chart.mark_line().encode(\n    x=alt.X(\"date:T\", axis=alt.Axis(title=\"date\")),\n    y=alt.Y(\"price:Q\", axis=alt.Axis(title=\"price\")),\n    tooltip=[\"date\", \"price\"],\n)\nlabel = chart.encode(\n    x=alt.X(\"date:T\", aggregate=\"max\"),\n    y=alt.Y(\"price:Q\", aggregate={\"argmax\": \"date\"}),\n    text=\"symbol\",\n)\ntext = label.mark_text(align=\"left\", dx=4)\ncircle = label.mark_circle()\n(\n    (line + circle + text).configure_axisX(labelAngle=0, ticks=False)\n    # .configure_axisY(titleAngle=0, titleAlign=\"center\", titleX=-10, titleY=-10)\n).properties(\n    width=600,\n    title=alt.TitleParams(\n        \"The Rise of Google\",\n        subtitle=[\n            \"Google went public in 2004\",\n            \"and outperformed other tech stocks until 2010\",\n        ],\n    ),\n)\n\n\n\n\n\n\n\nI could go on for hours, tweaking this and that. For example, the labels in the last chart overlap. Vega has the label transform method to deal with that, but that implementation still needs to cascade down to Vega-Lite and Altair. Someday, I will package all this as a proper theme, re-using feedzai’s code.",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair",
      "Amazing Altair with an even better theme"
    ]
  },
  {
    "objectID": "posts/altair/index.html",
    "href": "posts/altair/index.html",
    "title": "Data visualization with Vega-Altair",
    "section": "",
    "text": "Implementing best practices for data visualization as an Altair theme.\n\n\n\nDaniel Kapitan\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis notebook collects explorations of Altair’s most interesting features. Originally published on &lt;a…\n\n\n\nJacopo Repossi\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA data visualization curriculum of interactive notebooks, using Vega-Lite and Altair.\n\n\n\nJeffrey Heer, Dominik Moritz, Jake VanderPlas, Brock Craft\n\n\nMar 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing statistical mix effects and Simpson’s paradox with Altair\n\n\n\nDaniel Kapitan\n\n\nJan 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding your data is critical in creating visualizations. This video outlines Altair’s data types and explains how they can influence the visualization process.\n\n\n\nEitan Lees\n\n\nAug 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis video outlines the visualization grammar Altair is built on. Understanding the ways in which the elements of the visualization grammar interact is important when using…\n\n\n\nEitan Lees\n\n\nAug 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis video describes the Python package Altair and the software stack it is built on.\n\n\n\nEitan Lees\n\n\nAug 4, 2020\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair"
    ]
  },
  {
    "objectID": "posts/altair/index.html#section",
    "href": "posts/altair/index.html#section",
    "title": "Data visualization with Vega-Altair",
    "section": "",
    "text": "Implementing best practices for data visualization as an Altair theme.\n\n\n\nDaniel Kapitan\n\n\nJan 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis notebook collects explorations of Altair’s most interesting features. Originally published on &lt;a…\n\n\n\nJacopo Repossi\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA data visualization curriculum of interactive notebooks, using Vega-Lite and Altair.\n\n\n\nJeffrey Heer, Dominik Moritz, Jake VanderPlas, Brock Craft\n\n\nMar 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing statistical mix effects and Simpson’s paradox with Altair\n\n\n\nDaniel Kapitan\n\n\nJan 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding your data is critical in creating visualizations. This video outlines Altair’s data types and explains how they can influence the visualization process.\n\n\n\nEitan Lees\n\n\nAug 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis video outlines the visualization grammar Altair is built on. Understanding the ways in which the elements of the visualization grammar interact is important when using…\n\n\n\nEitan Lees\n\n\nAug 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis video describes the Python package Altair and the software stack it is built on.\n\n\n\nEitan Lees\n\n\nAug 4, 2020\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Perspectives on data science",
      "Data visualization with Vega-Altair"
    ]
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Perspectives on data science",
    "section": "",
    "text": "The Design Philosophy of Great Tables\n\n\n\nvisualization\n\n\n\n\n\n\n\nRich Iannone and Michael Chow\n\n\nApr 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPortable dataflows with Ibis and Hamilton\n\n\n\ncomposable data stack\n\n\ndata engineering\n\n\n\n\n\n\n\nThierry Jean\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModern, hybrid, open analytics\n\n\n\ncomposable data stack\n\n\ndata engineering\n\n\n\n\n\n\n\nCody Peterson\n\n\nJan 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData visualization with Vega-Altair\n\n\nI am a big fan of the Vega-Altair ecosystem for data visualization, because it not only helps me in creating appealing, interactive visualizations, but it also helps me to…\n\n\n\nDaniel Kapitan\n\n\nDec 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGetting into Python\n\n\nHow to get into Python, the most widely used programming language for data science.\n\n\n\nDaniel Kapitan\n\n\nDec 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalytical problem solving\n\n\n\nmodeling\n\n\n\nI co-authored a peer-reviewed paper that compares the different modeling approaches for analytical problem solving.\n\n\n\nDaniel Kapitan, Jeroen de Mast, Stefan Steiner, Wim Nuijten\n\n\nMar 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nStop aggregating away the signal in your data\n\n\n\nexploratory data analysis\n\n\nvisualization\n\n\n\nBy aggregating our data in an effort to simplify it, we lose the signal and the context we need to make sense of what we’re seeing. Originally published on &lt;a…\n\n\n\nZan Armstrong\n\n\nMar 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRIP correlation. Introducing the Predictive Power Score.\n\n\n\nexploratory data analysis\n\n\n\nWe define the Predictive Power Score (PPS), an alternative to the correlation that finds more patterns in your data. Originally published on &lt;a…\n\n\n\nFlorian Wetschoreck\n\n\nApr 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nThe Precision-Recall Plot Is More Informative than the ROC Plot\n\n\n\nperformance metrics\n\n\n\nAn introduction to performance metrics for binary classification. Originally published on &lt;a…\n\n\n\nPaul van der Laken, Daniel Kapitan\n\n\nAug 16, 2019\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Perspectives on data science"
    ]
  },
  {
    "objectID": "posts/stop-aggregating-signal/index.html",
    "href": "posts/stop-aggregating-signal/index.html",
    "title": "Stop aggregating away the signal in your data",
    "section": "",
    "text": "For five years as a data analyst, I forecasted and analyzed Google’s revenue. For six years as a data visualization specialist, I’ve helped clients and colleagues discover new features of the data they know best. Time and time again, I’ve found that by being more specific about what’s important to us and embracing the complexity in our data, we can discover new features in that data. These features can lead us to ask better data-driven questions that change how we analyze our data, the parameters we choose for our models, our scientific processes, or our business strategies.\nMy colleagues Ian Johnson, Mike Freeman, and I recently collaborated on a series of data-driven stories about electricity usage in Texas and California to illustrate best practices of Analyzing Time Series Data. We found ourselves repeatedly changing how we visualized the data to reveal the underlying signals, rather than treating those signals as noise by following the standard practice of aggregating the hourly data to days, weeks, or months.Behind many of the best practices we recommended for time series analysis was a deeper theme: actually embracing the complexity of the data.\nAggregation is the standard best practice for analyzing time series data, but it can create problems by stripping away crucial context so that you’re not even aware of how much potential insight you’ve lost. In this article, I’ll start by discussing how aggregation can be problematic, before walking through three specific alternatives to aggregation with before/after examples that illustrate:",
    "crumbs": [
      "Perspectives on data science",
      "Stop aggregating away the signal in your data"
    ]
  },
  {
    "objectID": "posts/stop-aggregating-signal/index.html#what-is-the-problem-with-aggregation",
    "href": "posts/stop-aggregating-signal/index.html#what-is-the-problem-with-aggregation",
    "title": "Stop aggregating away the signal in your data",
    "section": "What is the problem with aggregation?",
    "text": "What is the problem with aggregation?\nWe praise the importance of large, rich datasets when we talk about algorithms and teaching machines to learn from data. However, too often when we visualize data so that we as humans can make sense of it, especially time series data, we make the data smaller and smaller.\nAggregation is the default for a reason. It can feel overwhelming to handle the quantities of data we now have at our fingertips. It doesn’t have to be very “big data” to have more than 1M data points, more than the number of pixels on a basic laptop screen. There are many robust statistical approaches to effective aggregation and aggregation that can provide valuable context (comparing to median, for example). In other cases, we need to see more details while trying to find the key insight, but once we’ve finished the analysis and know which features of the data matter most, then aggregation can be a useful tool for focusing attention to communicate that insight.\nBut every time you aggregate, you make a decision about which features of your data matter and which ones you are willing to drop: which are the signal and which are the noise. When you smooth out a line chart, are you doing it because you’ve decided that the daily average is most important and that you don’t care about the distribution or seasonal variation in your peak usage hours? Or are you doing it because it’s the only way you know how to make the jagged lines in your chart go away?\nInformed aggregation simplifies and prioritizes. Uninformed aggregation means you’ll never know what insights you lost.\nIn our rush to aggregate, we sometimes forget that the numbers are tied to real things. To people’s actions. To the hourly, daily, weekly, monthly, and seasonal patterns that are so familiar that they’re almost forgettable. Or maybe it’s that we so rarely see disaggregated data presented effectively in practice that we don’t even realize it’s an option. By considering these seasonal patterns, these human factors, we could embrace complexity in more meaningful ways.\nConsider how much energy we use. If we take a moment to think about it, it’s obvious that we use a lot more energy in the late afternoon than the early morning, so we’d expect big dips and troughs every day. It also shouldn’t be a surprise that the daily energy usage patterns on a summer day and a winter day are different. These patterns aren’t noise, but rather are critical to making sense of this data. We especially need this context to tell what is expected and what is noteworthy.\nHowever, when our dataset has big, regular fluctuations from day to day or hour to hour, our line charts end up looking like an overwhelming mess of jagged lines. Consider this chart showing the 8,760 data points representing one year of data on hourly energy use in California.\n\nA standard way to deal with this overwhelming chart is to apply a moving average by day, week, or month (defined as a four-week period).\n\nYes, now we have a simple chart, and can easily see that the lowest energy usage is in April, and the peak in late August. But we could see that in the first chart. Moreover, we’ve smoothed out anything else of interest. We’ve thrown away so much information that we don’t even know what we’ve lost.\nIs this annual pattern, with a dip in April and a peak in August, consistent for all hours of the day? Do some hours of day or days of week change more than others through the seasons? Were there any hours, days, or weeks that were unusual for their time of year/time of day? What are the outliers? Is energy usage equally variable through all times of year, or are some weeks/seasons/hours more consistent than others?\nDespite starting with data that should contain the answers to these questions, we can’t answer them. Moreover, the smoothed line doesn’t even give us any hints about what questions to ask or what’s worth digging into more.",
    "crumbs": [
      "Perspectives on data science",
      "Stop aggregating away the signal in your data"
    ]
  },
  {
    "objectID": "posts/stop-aggregating-signal/index.html#solution-embrace-the-complexity-by-rearranging-augmenting-and-using-the-data-itself-to-provide-context.",
    "href": "posts/stop-aggregating-signal/index.html#solution-embrace-the-complexity-by-rearranging-augmenting-and-using-the-data-itself-to-provide-context.",
    "title": "Stop aggregating away the signal in your data",
    "section": "Solution: Embrace the complexity by rearranging, augmenting, and using the data itself to provide context.",
    "text": "Solution: Embrace the complexity by rearranging, augmenting, and using the data itself to provide context.\n\n#1: Don’t aggregate: Rearrange\nWhat if we considered what categories likely matter based on what we know of human behavior and environmental factors, especially temperature? Factors like time of day and time of year? In Discovering Data Patterns, we grouped the data into 96 small, aligned tick charts, one for each hour of the day for each season of the year and organized the visualization around the concepts most likely to matter. The x-axis for each mini chart is the amount of electricity used, and each tick mark represents a single hour on a specific day.\n\nThis way we can immediately see what’s typical or unusual for each hour and quarter. For example, generally more energy is used at midnight in winter than at 3am. Skimming down a column, we can see the shape of a day for each season. And, we can see how energy demand by hour changes across seasons by comparing each column to the next.\nNow the “noise” has become the signal. We can clearly answer the questions we posed above:\n\nIs this annual pattern consistent for all hours of the day?\n\nNo, the “shape” of energy used during the course of the day is different in winter vs. summer, with a double peak in Q1 and a single peak in Q3. Also, Q4 looks a lot like Q1, except for a few unusual days. And Q2 shows the most variability in “shape” of day.\n\nDo some hours of day or days of week change more than other hours through the seasons?\n\nYes, late afternoon and evening hours show much more of an increase in energy usage from Q1 to Q3 than early morning hours.\n\nWere there any hours, days, or weeks that were unusual for their time of year/time of day?\n\nYes. For example, in Q4 some very unusual days saw high energy usage in the evening.\nYes. In Q3 in the early morning hours (between 4am and 6am), there were some outlier days with much higher energy usage.\n\nIs energy usage equally variable through all times of year, or are some weeks/seasons/hours more consistent than others?\n\nNo! Q1 has very more consistent energy usage, with a very narrow range of energy used for any particular hour of the day. Meanwhile, Q2 shows very inconsistent energy usage, with a lot of variability especially in the higher-energy evening hours.\n\n\nNot only do we notice some patterns immediately, but this view of the data also gives us the chance to go deeper just by looking more closely (and doing some basic research into what was going on in California at that time).\nLet’s look closer at the early morning hours of Q3. There were some abnormally high values between 4pm and 6pm. Interactive tooltips reveal that these took place on August 19. A quick Google search for “California Aug 19th 2020” shows that the region was suffering from wildfires, so perhaps people kept windows closed and the AC on instead of opening their windows to the cooler nighttime air. September 6 also shows up among the highest values, and a search indicates a likely cause: a record-breaking heat wave in California that hit the national news while the fires continued to burn.\n\nOverall, our faceted tick heatmap chart has the same number of data points as the original jagged line, but now we can see the underlying daily and seasonal patterns (and how daily patterns vary by season) as well as the relative outliers. The more time we spend with the chart, the more we notice, as it invites us to ask new data-driven questions.\n\n\n#2: Augment first, then group or color.\n\nBring in common knowledge: augment with familiar classifications\n\nAt another point in our exploratory analysis, we looked at a chart showing 52 weeks of hourly energy usage in California (shown above) and noticed that the higher-energy weeks seemed to have a single bump each day in the evening while lower-energy weeks seemed to have more of a double bump (see above). This is actually the same pattern revealed in section #1 on rearranging.\nWe guessed that the single bump/double hump might be related to seasonal differences in temperatures. To test this hypothesis we added a column to the dataset to designate “summer” vs. “winter,” and then made two charts (faceted) by splitting data on that parameter. Suddenly it was obvious. No longer were we squinting to notice a pattern hidden in the squiggles.\n\nThe “faceting” itself was simple, a feature built into many charting APIs including Observable Plot (which we were using to visualize our data). In hindsight, this seems like an obvious way to split the data. But how often do we step back and actually augment our data with these human-relevant concepts? The key was having the summer/winter parameter to facet on. It doesn’t have to be perfect. Guessing at a date boundary for summer/winter is enough to see that a distinct pattern emerges. Once we have the double-bump/single-bump insight visible here, we can use that insight to go back and look at our data more closely. For example, it appears that there are some daily “double-bump” weeks in the “summer.” Are those boundary weeks that should be classified as winter (or fall or spring)? Or are they unusual weeks during the summer? Moreover, now that we know a defining signal, we could use that signal to classify the data and thereby use the data to identify when energy usage transitions from a “summer” pattern to a “winter” pattern.\n\n\nAugment with data-driven classifications\nThis line chart shows the daily energy use by a single household in Atlanta from March through July 2021. What do you notice? Lots of spikes? Higher energy use in the summer months?\n\nSwitching to a scatterplot makes it more obvious that there are normal-energy days and also high-energy days. Drawing in a line for the moving average plus a (5kwh) buffermakes this split between “normal” and “high-energy” days more clear and shows that the gap is maintained even as energy use overall increases in the summer months.\n\nNow that our exploratory data analysis has revealed two distinct categories (normal and high-energy), we can augment our data by using the moving average to define which points fall into each category. We can then color the points by those new classifications to make it easier to analyze.\n\nIn this way, we complete the circle: we use visualization to notice a key feature of the data and leverage this insight to further classify our data, making the visualization easier to read. And we can take it a step further, continuing to analyze our data based on this classification by creating a histogram showing the frequency of high-energy vs. normal usage days by month. In this view, we can see that in the summer the amount of energy used on normal days went up, and that there were more high-energy days in June and July than in March and April (even after taking into account that the baseline energy usage also went up). Therefore, we can now say with confidence that overall energy consumption increased for two reasons: (1) baseline energy usage increased and (2) a higher percent of days were high-energy days.\n\nThis pattern of looking, then augmenting, then looking again using the classification can also reveal any issues with our classification, like the high point that occurred on our sixth day of data which is mislabeled because the moving average was not defined until the seventh day (as a trailing moving average). This gives us a chance to improve our classification algorithm.\nWhile this example used a very simple algorithm of “moving average + 5kwh” to classify days as “normal” or “high-energy,” this cycle of “look, augment, look, refine classification” becomes more important for machine learning as our algorithms become more opaque.\n\n\n\n#3: Split your data into foreground and background\n\nSplit based on a time period of interest\nWe also dug into data on energy generated by fuel type in Texas in January and February of 2021, including a critical time period in February leading up to and during the rolling blackouts that were initiated to save the electricity grid from collapse following an unusual winter storm. In the analysis story, my colleague Ian faceted the data, creating a chart for each fuel type. This was quite effective: you can immediately see which fuels make up the bulk of energy in Texas, as well as some of the abnormal patterns in mid-February.\n\nKnowing that the critical time period was around February 7 to February 21, Ian further focused attention on those two weeks by making the weeks before and after partially transparent and adding vertical gridlines. He might have been tempted to delete the data outside the period. After all, why waste space on data outside the time period of interest?\n\nBut it’s that granular background data that helps us understand what is so unusual for each fuel type during the critical time period. For example, in coal we’d notice the dip after February 15 regardless, but we need the data from January to notice how unusual the nearly flat plateau between February 1 and February 15 is. Similarly, the January and late-February data for nuclear shows how steady that fuel source typically is, helping us to understand just how strange the dip that we see after February 15 is.\n\n\n\nSplit by comparing each category of interest to the full dataset\nWhen we want to know if there is a relationship between metric A and metric B, the first step is to create a scatterplot. For example, the scatterplot below shows the outdoor temperature and energy demand in Texas for each hour over the course of a year. It’s immediately clear that there is a strong relationship between temperature and energy use (even though that relationship is also obviously non-linear!).\n\nWhile there is clearly a correlation between temperature and electricity demand, it’s also clear that temperature doesn’t tell the whole story. For any given temperature, there is a roughly 10-15K MWh difference from minimum to maximum energy use. Knowing that in our own homes we crank the thermostat a lot higher on a cold afternoon than on a cold night, we guessed that the hour of day could play a key role in the relationship between temperature and energy use.\nThe standard approach to adding an additional category to a scatterplot is to apply a categorical color, thereby comparing everything to everything (comparing all hours, temperatures, and energy demand in one chart). If we do that, we do see that something is going on. More greens and blues in the top right, more pinks low. But to understand what the colors refer to, you have to look back and forth between the legend and the data a lot. Moreover, it’s impossible to answer a question like, “What’s the relationship between temperature and energy at 10am?” Or, “How does early morning compare to evening?\n\nInstead we can apply two techniques: grouping and splitting the data into foreground and background.\nIn the three charts below, the dots representing 5am, 10am, and 6pm are colored brightly. Meanwhile, the entire dataset is shown in grey in the background. This gives us the context to see the relationship between temp and energy for each hour, and see that in the context of the full dataset.\nBy specifically comparing “5am” to “all other times of day,” we can see that 5am is relatively low energy use regardless of temperature (and temperatures are never very high at 5am). Meanwhile, at 6pm energy use is generally higher at all temperatures.\n10am is in some ways the most interesting: at lower temperatures (in the left half of the graph) the yellow dots are relatively high compared to the grey dots, indicating high energy use relative to other hours of the day at the same temperature. Meanwhile, for high temperatures on the right half of the graph, the yellow dots hug the bottom of the grey area. At hot temperatures, relatively little energy is used at 10am compared to the rest of the day. This type of insight is made possible not just by grouping, but also by using the full “noisy” dataset as a consistent background providing context for all the faceted charts.",
    "crumbs": [
      "Perspectives on data science",
      "Stop aggregating away the signal in your data"
    ]
  },
  {
    "objectID": "posts/stop-aggregating-signal/index.html#summary-embrace-the-complexity-of-your-data",
    "href": "posts/stop-aggregating-signal/index.html#summary-embrace-the-complexity-of-your-data",
    "title": "Stop aggregating away the signal in your data",
    "section": "Summary: Embrace the complexity of your data",
    "text": "Summary: Embrace the complexity of your data\nIn the course of creating the Analyzing Time Series Data collection, Ian Johnson, Mike Freeman, and I employed a range of strategies to embrace the complexity of the data instead of relying on standard methods that aggregate it away. Those frustratingly jagged lines are the signal, not the noise.\nWe embraced complexity by:\n\nRearranging data to compare “like to like.”\nAugmenting our data based on the concepts that we know matter and on what we discovered in the data.\nUsing the larger dataset to provide background context for the data of interest (the foreground).\n\nThese approaches are especially powerful for time series data because the underlying daily, weekly, and seasonal patterns can feel so distracting. In particular, consider how these strategies might power real-time data analysis by putting incoming data in a richer historical context for quick visual pattern-matching to identify normal vs. worrisome patterns. At the same time, these foundational techniques also apply to any data that can feel overwhelming and noisy, like machine learning classifications or data resulting from high-throughput scientific experiments.\nAfter seeing each of these techniques in action, perhaps the next time you are about to aggregate your data in order to simplify it, you might instead try to rearrange, augment, or split your data into foreground/background. See the data in its full context to reveal unexpected patterns and prompt new data-driven questions. Embrace complexity by (literally) changing how you look at your data.",
    "crumbs": [
      "Perspectives on data science",
      "Stop aggregating away the signal in your data"
    ]
  },
  {
    "objectID": "posts/python/realpython.html",
    "href": "posts/python/realpython.html",
    "title": "Getting into Python with RealPython.com",
    "section": "",
    "text": "The table below lists the topics that you should have covered _as a bare minimum. You often have a choice to suit your prefered learning style: courses are videos, tutorials are text. Note that the tutorials are free of charge, while the videos are only accessible with a paid subscription.\n\n\n\nchapter\ntopic\ncourse\ntutorial\n\n\n\n\n2. Setting up Python\nInstalling Python 3\nlink\nlink\n\n\n\nPython in VS Code\nlink\nlink\n\n\n3. Your First Python Program\nCode Your First Python Program\nlink\n\n\n\n\nBasic Data Types in Python\nlink\nlink\n\n\n\nVariables in Python\nlink\nlink\n\n\n4. Strings and String Methods\nStrings and Character Data in Python\nlink\nlink\n\n\n\nReading Input and Writing Output in Python\nlink\nlink\n\n\n5. Numbers and Math\nOperators and Expressions in Python\nlink\nlink\n\n\n\nNumbers in Python\n\nlink\n\n\n6. Functions and Loops\n“for” loops (Definite Iteration)\nlink\nlink\n\n\n\n“while” loops (Indefinite Iteration)\nlink\nlink\n\n\n\nDefining and Calling Python Functions\nlink\nlink\n\n\n8. Conditional Logic and Control Flow\nConditional Statements in Python (if/elis/else)\nlink\nlink\n\n\n9. Tuples, Lists and Dictinairies\nLists and Tuples in Python\nlink\nlink\n\n\n\nDictionairies in Python\nlink\nlink\n\n\n11. Modules and Packages\nPython Modules and Packages: An Introduction\nlink\nlink\n\n\n12. File Input and Output\nReading and Writing Files in Python\nlink",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python",
      "Getting into Python with RealPython.com"
    ]
  },
  {
    "objectID": "posts/python/index.html",
    "href": "posts/python/index.html",
    "title": "Getting into Python",
    "section": "",
    "text": "Python has emerged over the last couple decades as a first-class tool for scientific computing tasks, including the analysis and visualization of large datasets. This may have come as a surprise to early proponents of the Python language: the language itself was not specifically designed with data analysis or scientific computing in mind. The usefulness of Python for data science stems primarily from the large and active ecosystem of third-party packages, particularly:\n\nNumPy for manipulation of homogeneous array-based data;\nPandas for manipulation of heterogeneous and labeled data, and the more recent high-performace dataprocessing libraries such as polars and ibis;\nSciPy for common scientific computing tasks, Matplotlib for publication-quality visualizations;\nIPython for interactive execution and sharing of code;\nScikit-Learn for machine learning.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python"
    ]
  },
  {
    "objectID": "posts/python/index.html#why-python-for-data-science",
    "href": "posts/python/index.html#why-python-for-data-science",
    "title": "Getting into Python",
    "section": "",
    "text": "Python has emerged over the last couple decades as a first-class tool for scientific computing tasks, including the analysis and visualization of large datasets. This may have come as a surprise to early proponents of the Python language: the language itself was not specifically designed with data analysis or scientific computing in mind. The usefulness of Python for data science stems primarily from the large and active ecosystem of third-party packages, particularly:\n\nNumPy for manipulation of homogeneous array-based data;\nPandas for manipulation of heterogeneous and labeled data, and the more recent high-performace dataprocessing libraries such as polars and ibis;\nSciPy for common scientific computing tasks, Matplotlib for publication-quality visualizations;\nIPython for interactive execution and sharing of code;\nScikit-Learn for machine learning.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python"
    ]
  },
  {
    "objectID": "posts/python/index.html#how-much-python-should-i-know",
    "href": "posts/python/index.html#how-much-python-should-i-know",
    "title": "Getting into Python",
    "section": "How much Python should I know?",
    "text": "How much Python should I know?\nAs with any other (programming) language, it takes years to master it fluently which is beyond the scope this anthology. Instead, our objective is to have a working knowledge of Python to be able to learn and apply machine learning. To make this explicit we take the following book and online resources as our point of reference.\n\nA Whirlwind Tour of Python (pages number from the pdf version):\n\nKnow how to install and use Python on your own computer (pages 1 to 13)\nKnow basic semantics of variables, objects and operators (pages 13 to 24)\nKnow built-in simple values and data structures (pages 24 to 37)\nKnow how to use control flow and functions (pages 37 to 45)\nKnow how to iterate and use list comprehensions (pages 52 to 61)\n\nPython for Data Analysis\n\nKnow how to manipulate data with pandas, with Python for Data Analysis, Third Edition as your reference guide\n\n\n\n\n\n\n\n\nPCEP™ – Certified Entry-Level Python Programmer\n\n\n\nThe learning path proposed here is similar to the PCEP™ – Certified Entry-Level Python Programmer certification. The PCEP™ certification is a good way to assess your current Python knowledge and to prepare for the Machine Learning Foundation course. The certification is offered by the Python Institute. You may opt to obtain this certificate.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python"
    ]
  },
  {
    "objectID": "posts/python/index.html#how-should-i-learn-python",
    "href": "posts/python/index.html#how-should-i-learn-python",
    "title": "Getting into Python",
    "section": "How should I learn Python?",
    "text": "How should I learn Python?\nRealPython.com is the recommended online learning environment for Python. We have collated a learning path for data science.",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python"
    ]
  },
  {
    "objectID": "posts/python/index.html#which-python-environment-should-i-use",
    "href": "posts/python/index.html#which-python-environment-should-i-use",
    "title": "Getting into Python",
    "section": "Which Python environment should I use?",
    "text": "Which Python environment should I use?\nOptions how to start using Python are listed below.\n\nOnline notebook (easy)Visual Studio Code (intermediate)\n\n\nFor those new to Python, it is probably easiest to start with one of these online notebook environments:\n\nDeepnote: there is a generous free-tier. If you decide to upgrade, you can collaborate and share notebooks privately.\nGoogle Colab:\n\nActivate a Google account if you haven’t got one yet.\nWork your way through the Colab introduction notebook.\n\n\nOnce you have gained some traction, you can move on to install Python on your local machine.\n\n\nVisual Studio Code is the recommended data science workbench. To setup your local machine/laptop for data science and machine learning, do the following:\n\nFollow these two tutorials on RealPython.com:\n\nInstalling Python\nPython in VS Code\n\nFor more advanced use, including using VS Code in the cloud:\n\nRead the documentation on GitHub Codespaces\nRead the documentation on Azure Dev Containers\nUse these excellent Data Science Dev Containers by Olivier Benz",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python"
    ]
  },
  {
    "objectID": "posts/python/index.html#guidelines-for-using-python-for-data-science",
    "href": "posts/python/index.html#guidelines-for-using-python-for-data-science",
    "title": "Getting into Python",
    "section": "Guidelines for using Python for data science",
    "text": "Guidelines for using Python for data science\nUsing Python for data science is inherently different than using it for, say, building a website. To provide you with some guidance to the many different ways c.q. styles of using Python, please consider the following:\n\nFocus on using existing data science libraries, instead of writing your own basic functions. If you find yourself spending a lot of time reading documentation, you are on the right track.\nTake a functional approach to programming instead of an object-oriented approach. The former is more fitting for data science, where it is common to structure your work in terms of pipelines and think about each processing step as a function. The latter is more suitable for application development.\n\nFor those wanting to further develop their Python skills for data science, the following books are recommended:\n\nPython for Data Analysis 3rd Edition by Wes McKinney, the creator of pandas.\nData Science With Python Core Skills on Real Python provides an extensive learning path.\nHands-On Machine Learning with Scikit-Learn, Keras and Tensorflow (2nd edition) by Aurélien Géron. You will need to purchase the book, but the notebooks with example code are freely available.\nEffective Python: 90 Specific Ways to Write Better Python (second edition).",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python"
    ]
  },
  {
    "objectID": "posts/python/index.html#more-on-python",
    "href": "posts/python/index.html#more-on-python",
    "title": "Getting into Python",
    "section": "More on Python",
    "text": "More on Python\n\n\n\n\n\n\n\n\n\n\nGetting into Python with RealPython.com\n\n\nWe have compiled a learning path for those who are new to Python, using a selection of chapters from Real Python.\n\n\n\nDaniel Kapitan\n\n\nNov 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Python and idiomatic pandas\n\n\nGuidelines on how to continue to develop your skills to write effective Python and use pandas in an idiomatic way.\n\n\n\nDaniel Kapitan\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Whirlwind Tour of Python\n\n\nA brief but comprehensive tour of the Python language for those who have (sometimes extensive) backgrounds in computing in some language.\n\n\n\nJake VanderPlas\n\n\nAug 10, 2016\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Perspectives on data science",
      "Getting into Python"
    ]
  },
  {
    "objectID": "posts/predictive-power-score/index.html",
    "href": "posts/predictive-power-score/index.html",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "",
    "text": "It is Friday afternoon and your boss tells you that the data delivery surprisingly arrived early — after only 4 weeks of back and forth. This was the missing piece for your predictive model. You’re excited but also a little bit anxious because you know what’s next: exploring the data. All. 45. Columns. This will take many hours but you know it’s worth it because without data understanding you are walking blind. One obvious step is to have a look at all the univariate column distributions. But this won’t be enough.\n\nYou ask yourself: what relationships exist between columns?\n\nTo answer this question, you just repeat the typical drill: calculate a correlation matrix and check for some surprising relationships. Whenever you are surprised, you take a moment to plot a scatterplot of the two columns at hand and see if you can make any sense of it. Hopefully you can but too often you cannot because you don’t even know what the columns mean in the first place. But this is a story for another day.\n\nAfter inspecting the correlation matrix you move on and you don’t even know what you don’t know (scary).\n\nLet’s take a moment to review the correlation. The score ranges from -1 to 1 and indicates if there is a strong linear relationship — either in a positive or negative direction. So far so good. However, there are many non-linear relationships that the score simply won’t detect. For example, a sinus wave, a quadratic curve or a mysterious step function. The score will just be 0, saying: “Nothing interesting here”. Also, correlation is only defined for numeric columns. So, let’s drop all the categoric columns. In my last project more than 60% of the columns were categoric, but hey. Never mind. And no, I won’t convert the columns because they are not ordinal and OneHotEncoding will create a matrix that has more values than there are atoms in the universe.\n\nIf you are a little bit too well educated you know that the correlation matrix is symmetric. So you basically can throw away one half of it. Great, we saved ourselves some work there! Or did we? Symmetry means that the correlation is the same whether you calculate the correlation of A and B or the correlation of B and A. However, relationships in the real world are rarely symmetric. More often, relationships are asymmetric. Here is an example: The last time I checked, my zip code of 60327 tells strangers quite reliably that I am living in Frankfurt, Germany. But when I only tell them my city, somehow they are never able to deduce the correct zip code. Pff … amateurs. Another example is this: a column with 3 unique values will never be able to perfectly predict another column with 100 unique values. But the opposite might be true. Clearly, asymmetry is important because it is so common in the real world.\n\nThinking about those shortcomings of correlation, I started to wonder: can we do better?\n\nThe requirements: One day last year, I was dreaming about a score that would tell me if there is any relationship between two columns — no matter if the relationship is linear, non-linear, gaussian or only known by aliens. Of course, the score should be asymmetric because I want to detect all the weird relationships between cities and zip codes. The score should be 0 if there is no relationship and the score should be 1 if there is a perfect relationship. And as the icing on the cake, the score should be able to handle categoric and numeric columns out of the box. Summing it up for all my academic friends: an asymmetric, data-type-agnostic score for predictive relationships between two columns that ranges from 0 to 1.",
    "crumbs": [
      "Perspectives on data science",
      "RIP correlation. Introducing the Predictive Power Score."
    ]
  },
  {
    "objectID": "posts/predictive-power-score/index.html#too-many-problems-with-the-correlation",
    "href": "posts/predictive-power-score/index.html#too-many-problems-with-the-correlation",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "",
    "text": "It is Friday afternoon and your boss tells you that the data delivery surprisingly arrived early — after only 4 weeks of back and forth. This was the missing piece for your predictive model. You’re excited but also a little bit anxious because you know what’s next: exploring the data. All. 45. Columns. This will take many hours but you know it’s worth it because without data understanding you are walking blind. One obvious step is to have a look at all the univariate column distributions. But this won’t be enough.\n\nYou ask yourself: what relationships exist between columns?\n\nTo answer this question, you just repeat the typical drill: calculate a correlation matrix and check for some surprising relationships. Whenever you are surprised, you take a moment to plot a scatterplot of the two columns at hand and see if you can make any sense of it. Hopefully you can but too often you cannot because you don’t even know what the columns mean in the first place. But this is a story for another day.\n\nAfter inspecting the correlation matrix you move on and you don’t even know what you don’t know (scary).\n\nLet’s take a moment to review the correlation. The score ranges from -1 to 1 and indicates if there is a strong linear relationship — either in a positive or negative direction. So far so good. However, there are many non-linear relationships that the score simply won’t detect. For example, a sinus wave, a quadratic curve or a mysterious step function. The score will just be 0, saying: “Nothing interesting here”. Also, correlation is only defined for numeric columns. So, let’s drop all the categoric columns. In my last project more than 60% of the columns were categoric, but hey. Never mind. And no, I won’t convert the columns because they are not ordinal and OneHotEncoding will create a matrix that has more values than there are atoms in the universe.\n\nIf you are a little bit too well educated you know that the correlation matrix is symmetric. So you basically can throw away one half of it. Great, we saved ourselves some work there! Or did we? Symmetry means that the correlation is the same whether you calculate the correlation of A and B or the correlation of B and A. However, relationships in the real world are rarely symmetric. More often, relationships are asymmetric. Here is an example: The last time I checked, my zip code of 60327 tells strangers quite reliably that I am living in Frankfurt, Germany. But when I only tell them my city, somehow they are never able to deduce the correct zip code. Pff … amateurs. Another example is this: a column with 3 unique values will never be able to perfectly predict another column with 100 unique values. But the opposite might be true. Clearly, asymmetry is important because it is so common in the real world.\n\nThinking about those shortcomings of correlation, I started to wonder: can we do better?\n\nThe requirements: One day last year, I was dreaming about a score that would tell me if there is any relationship between two columns — no matter if the relationship is linear, non-linear, gaussian or only known by aliens. Of course, the score should be asymmetric because I want to detect all the weird relationships between cities and zip codes. The score should be 0 if there is no relationship and the score should be 1 if there is a perfect relationship. And as the icing on the cake, the score should be able to handle categoric and numeric columns out of the box. Summing it up for all my academic friends: an asymmetric, data-type-agnostic score for predictive relationships between two columns that ranges from 0 to 1.",
    "crumbs": [
      "Perspectives on data science",
      "RIP correlation. Introducing the Predictive Power Score."
    ]
  },
  {
    "objectID": "posts/predictive-power-score/index.html#calculating-the-predictive-power-score-pps",
    "href": "posts/predictive-power-score/index.html#calculating-the-predictive-power-score-pps",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Calculating the Predictive Power Score (PPS)",
    "text": "Calculating the Predictive Power Score (PPS)\n\nFirst of all, there is not the one and only way to calculate the predictive power score. In fact, there are many possible ways to calculate a score that satisfies the requirements mentioned before. So, let’s rather think of the predictive power score as a framework for a family of scores.\n\nLet’s say we have two columns and want to calculate the predictive power score of A predicting B. In this case, we treat B as our target variable and A as our (only) feature. We can now calculate a cross-validated Decision Tree and calculate a suitable evaluation metric. When the target is numeric we can use a Decision Tree Regressor and calculate the Mean Absolute Error (MAE). When the target is categoric, we can use a Decision Tree Classifier and calculate the weighted F1. You might also use other scores like the ROC etc but let’s put those doubts aside for a second because we have another problem:\n\nMost evaluation metrics are meaningless if you don’t compare them to a baseline\n\nI guess you all know the situation: you tell your grandma that your new model has a F1 score of 0.9 and somehow she is not as excited as you are. In fact, this is very smart of her because she does not know if anyone can score 0.9 or if you are the first human being who ever scored higher than 0.5 after millions of awesome KAGGLErs tried. So, we need to “normalize” our evaluation score. And how do you normalize a score? You define a lower and an upper limit and put the score into perspective. So what should the lower and upper limit be? Let’s start with the upper limit because this is usually easier: a perfect F1 is 1. A perfect MAE is 0. Boom! Done. But what about the lower limit? Actually, we cannot answer this in absolute terms.\n\nThe lower limit depends on the evaluation metric and your data set. It is the value that a naive predictor achieves.\n\nIf you achieve a F1 score of 0.9 this might be super bad or really good. If your super fancy cancer detection model always predicts “benign” and it still scores 0.9 on that highly skewed dataset then 0.9 is obviously not so good. So, we need to calculate a score for a very naive model. But what is a naive model? For a classification problem, always predicting the most common class is pretty naive. For a regression problem, always predicting the median value is pretty naive.\n\nLet’s have a look at a detailed, fictional example:\nGetting back to the example of the zip codes and the city name. Imagine both columns are categoric. First, we want to calculate the PPS of zip code to city. We use the weighted F1 score because city is categoric. Our cross-validated Decision Tree Classifier achieves a score of 0.95 F1. We calculate a baseline score via always predicting the most common city and achieve a score of 0.1 F1. If you normalize the score, you will get a final PPS of 0.94 after applying the following normalization formula: (0.95–0.1) / (1–0.1). As we can see, a PPS score of 0.94 is rather high, so the zip code seems to have a good predictive power towards the city. However, if we calculate the PPS in the opposite direction, we might achieve a PPS of close to 0 because the Decision Tree Classifier is not substantially better than just always predicting the most common zip code.\n\nPlease note: the normalization formula for the MAE is different from the F1. For MAE lower is better and the best value is 0.",
    "crumbs": [
      "Perspectives on data science",
      "RIP correlation. Introducing the Predictive Power Score."
    ]
  },
  {
    "objectID": "posts/predictive-power-score/index.html#comparing-the-pps-to-correlation",
    "href": "posts/predictive-power-score/index.html#comparing-the-pps-to-correlation",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Comparing the PPS to correlation",
    "text": "Comparing the PPS to correlation\nIn order to get a better feeling for the PPS and its differences to the correlation, let’s have a look at the following two examples:\n\npps2Example 1: Non-linear effects and asymmetry\n\nLet’s use a typical quadratic relationship: the feature x is a uniform variable ranging from -2 to 2 and the target y is the square of x plus some error. In this case, x can predict y very well because there is a clear non-linear, quadratic relationship — after all that’s how we generated the data. However, this is not true in the other direction from y to x. For example, if y is 4, it is impossible to predict whether x was roughly 2 or -2. Thus, the predictive relationship is asymmetric and the scores should reflect this.\nWhat are the values of the scores in this example? If you don’t already know what you are looking for, the correlation will leave you hanging because the correlation is 0. Both from x to y and from y to x because the correlation is symmetric. However, the PPS from x to y is 0.67, detecting the non-linear relationship and saving the day. Nevertheless, the PPS is not 1 because there exists some error in the relationship. In the other direction, the PPS from y to x is 0 because your prediction cannot be better than the naive baseline and thus the score is 0.\n\n\nExample 2: Categorical columns and hidden patterns\nLet’s compare the correlation matrix to the PPS matrix on the Titanic dataset. “The Titanic dataset? Again??” I know, you probably think you already have seen everything about the Titanic dataset but maybe the PPS will give you some new insights.\n\n\n\nTwo findings about the correlation matrix:\n\nThe correlation matrix is smaller and leaves out many interesting relationships. Of course, that makes sense because columns like Sex, TicketID or Port are categoric and the correlation cannot be computed for them.\nThe correlation matrix shows a negative correlation between TicketPrice and Class of medium strength (-0.55). We can double-check this relationship if we have a look at the PPS. We will see that the TicketPrice is a strong predictor for the Class (0.9 PPS) but not vice versa. The Class only predicts the TicketPrice with a PPS of 0.2. This makes sense because whether your ticket did cost 5.000$ or 10.000$ you were most likely in the highest class. In contrast, if you know that someone was in the highest class you cannot say whether they paid 5.000$ or 10.000$ for their ticket. In this scenario, the asymmetry of the PPS shines again.\n\n\n\nFour findings about the PPS matrix:\n\nThe first row of the matrix tells you that the best univariate predictor of the column Survived is the column Sex. This makes sense because women were prioritized during the rescue. (We could not find this information in the correlation matrix because the column Sex was dropped.)\nIf you have a look at the column for TicketID, you can see that TicketID is a fairly good predictor for a range of columns. If you further dig into this pattern, you will find out that multiple persons had the same TicketID. Thus, the TicketID is actually referencing a latent group of passengers who bought the ticket together, for example the big Italian Rossi family that turns any evening into a spectacle. Thus, the PPS helped me to detect a hidden pattern.\nWhat’s even more surprising than the strong predictive power of TicketID is the strong predictive power of TicketPrice across a wide range of columns. Especially, the fact that the TicketPrice is fairly good at predicting the TicketID (0.67) and vice versa (0.64). Upon further research you will find out that tickets often had unique prices. For example, only the Italian Rossi family paid a price of 72,50$. This is a critical insight! It means that the TicketPrice contains information about the TicketID and thus about our Italian family. An information that you need to have when considering potential information leakage.\nLooking at the PPS matrix, we can see effects that might be explained by causal chains. (Did he just say causal? — Of course, those causal hypotheses have to be treated carefully but this is beyond the scope of this article.) For example, you might be surprised why the TicketPrice has predictive power on the survival rate (PPS 0.39). But if you know that the Class influences your survival rate (PPS 0.36) and that the TicketPrice is a good predictor for your Class (PPS 0.9), then you might have found an explanation.",
    "crumbs": [
      "Perspectives on data science",
      "RIP correlation. Introducing the Predictive Power Score."
    ]
  },
  {
    "objectID": "posts/predictive-power-score/index.html#applications-of-the-pps-and-the-pps-matrix",
    "href": "posts/predictive-power-score/index.html#applications-of-the-pps-and-the-pps-matrix",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Applications of the PPS and the PPS matrix",
    "text": "Applications of the PPS and the PPS matrix\nAfter we learned about the advantages of the PPS, let’s see where we can use the PPS in the real life.\n\nDisclaimer: There are use cases for both the PPS and the correlation. The PPS clearly has some advantages over correlation for finding predictive patterns in the data. However, once the patterns are found, the correlation is still a great way of communicating found linear relationships.\n\n\nFind patterns in the data: The PPS finds every relationship that the correlation finds — and more. Thus, you can use the PPS matrix as an alternative to the correlation matrix to detect and understand linear or nonlinear patterns in your data. This is possible across data types using a single score that always ranges from 0 to 1.\nFeature selection: In addition to your usual feature selection mechanism, you can use the predictive power score to find good predictors for your target column. Also, you can eliminate features that just add random noise. Those features sometimes still score high in feature importance metrics. In addition, you can eliminate features that can be predicted by other features because they don’t add new information. Besides, you can identify pairs of mutually predictive features in the PPS matrix — this includes strongly correlated features but will also detect non-linear relationships.\nDetect information leakage: Use the PPS matrix to detect information leakage between variables — even if the information leakage is mediated via other variables.\nData Normalization: Find entity structures in the data via interpreting the PPS matrix as a directed graph. This might be surprising when the data contains latent structures that were previously unknown. For example: the TicketID in the Titanic dataset is often an indicator for a family.",
    "crumbs": [
      "Perspectives on data science",
      "RIP correlation. Introducing the Predictive Power Score."
    ]
  },
  {
    "objectID": "posts/predictive-power-score/index.html#how-to-use-the-pps-in-your-own-python-project",
    "href": "posts/predictive-power-score/index.html#how-to-use-the-pps-in-your-own-python-project",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "How to use the PPS in your own (Python) project",
    "text": "How to use the PPS in your own (Python) project\nIf you are still following along you are one of the rare human beings who still have an attention span — you crazy beast! If you can’t wait to see what the PPS will reveal on your own data, we have some good news for you: we open-sourced an implementation of the PPS as a Python library named ppscore.\n\nBefore using the Python library, please take a moment to read through the calculation details\n\nInstalling the package is as simple as\n\npip install ppscore\n\nCalculating the PPS for a given pandas dataframe:\n\nimport ppscore as pps\npps.score(df, \"feature_column\", \"target_column\")\n\nYou can also calculate the whole PPS matrix:\n\npps.matrix(df)\n\n\nHow fast is the PPS in comparison to the correlation?\nAlthough the PPS has many advantages over the correlation, there is some drawback: it takes longer to calculate. But how bad is it? Does it take multiple weeks or are we done in a couple of minutes or even seconds? When calculating a single PPS using the Python library, the time should be no problem because it usually takes around 10–500ms. The calculation time mostly depends on the data types, the number of rows and the used implementation. However, when calculating the whole PPS matrix for 40 columns this results in 40*40=1600 individual calculations which might take 1–10 minutes. So you might want to start the calculation of the PPS matrix in the background and go on that summer vacation you always dreamed of! 🏖 ️For our projects and datasets the computational performance was always good enough but of course there is room for improvement. Fortunately, we see many ways how the calculation of the PPS can be improved to achieve speed gains of a factor of 10–100. For example, using intelligent sampling, heuristics or different implementations of the PPS. If you like the PPS and are in need of a faster calculation, please reach out to us.",
    "crumbs": [
      "Perspectives on data science",
      "RIP correlation. Introducing the Predictive Power Score."
    ]
  },
  {
    "objectID": "posts/predictive-power-score/index.html#limitations",
    "href": "posts/predictive-power-score/index.html#limitations",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Limitations",
    "text": "Limitations\nWe made it — you are excited and want to show the PPS to your colleagues. However, you know they are always so critical about new methods. That’s why you better be prepared to know the limitations of the PPS:\n\nThe calculation is slower than the correlation (matrix).\nThe score cannot be interpreted as easily as the correlation because it does not tell you anything about the type of relationship that was found. Thus, the PPS is better for finding patterns but the correlation is better to communicate found linear relationships.\nYou cannot compare the scores for different target variables in a strict mathematical way because they are calculated using different evaluation metrics. The scores are still valuable in the real world, but you need to keep this in mind.\nThere are limitations of the components used underneath the hood. Please remember: you might exchange the components e.g. using a GLM instead of a Decision Tree or using ROC instead of F1 for binary classifications.\nIf you use the PPS for feature selection you still want to perform forward and backward selection in addition. Also, the PPS cannot detect interaction effects between features towards your target.",
    "crumbs": [
      "Perspectives on data science",
      "RIP correlation. Introducing the Predictive Power Score."
    ]
  },
  {
    "objectID": "posts/predictive-power-score/index.html#conclusion",
    "href": "posts/predictive-power-score/index.html#conclusion",
    "title": "RIP correlation. Introducing the Predictive Power Score.",
    "section": "Conclusion",
    "text": "Conclusion\nAfter years of using the correlation we were so bold (or crazy?) to suggest an alternative that can detect linear and non-linear relationships. The PPS can be applied to numeric and categoric columns and it is asymmetric. We proposed an implementation and open-sourced a Python package. In addition, we showed the differences to the correlation on some examples and discussed some new insights that we can derive from the PPS matrix. Now it is up to you to decide what you think about the PPS and if you want to use it on your own projects.\nGithub: https://github.com/8080labs/ppscore.",
    "crumbs": [
      "Perspectives on data science",
      "RIP correlation. Introducing the Predictive Power Score."
    ]
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Notebooks and explainers with code",
    "section": "",
    "text": "Predicting diabetes with pycaret\n\n\n\n\n\n\nDaniel Kapitan\n\n\nNov 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting house prices with pycaret\n\n\n\n\n\n\nDaniel Kapitan\n\n\nOct 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemonstration of overfitting and underfitting\n\n\nThis notebook illustrates why we require a train/test split (or cross-validation) to balance overfitting vs. underfitting.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeconfounding explained\n\n\nA demonstration howcorrelations ‘magically’ disappear if confounders are added to your model.\n\n\n\n\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Notebooks"
    ]
  },
  {
    "objectID": "notebooks/ames-housing/index.html#objectives",
    "href": "notebooks/ames-housing/index.html#objectives",
    "title": "Predicting house prices with pycaret",
    "section": "Objectives",
    "text": "Objectives\n\nExample end-to-end supervised learning workflow with Ames Housing dataset\nFocus on conceptual understanding of machine learning\nDemonstrate use of Predictive Power Score (PPS)\nDemonstrate capabilities of low-code tools",
    "crumbs": [
      "Notebooks",
      "Predicting house prices with pycaret"
    ]
  },
  {
    "objectID": "notebooks/ames-housing/index.html#attribution",
    "href": "notebooks/ames-housing/index.html#attribution",
    "title": "Predicting house prices with pycaret",
    "section": "Attribution",
    "text": "Attribution\n\nDataset\n\nAmes Housing dataset paper (original paper)\nKaggle competition advanced regression techniques (link)\n\n\n\nPython libraries\n\nAltair (docs)\nydata-profiling (docs)\nPredictive Power Score (PPS, GitHub, blog)\nPyCaret: open-source, low-code machine learning library in Python that automates machine learning workflows (link)\n\n\nimport altair as alt\nimport pandas as pd\nimport ppscore as pps\nfrom pycaret.regression import *\nfrom ydata_profiling import ProfileReport\n\n\n# customize Altair\ndef y_axis():\n    return {\n        \"config\": {\n            \"axisX\": {\"grid\": False},\n            \"axisY\": {\n                \"domain\": False,\n                \"gridDash\": [2, 4],\n                \"tickSize\": 0,\n                \"titleAlign\": \"right\",\n                \"titleAngle\": 0,\n                \"titleX\": -5,\n                \"titleY\": -10,\n            },\n            \"view\": {\n                \"stroke\": \"transparent\",\n                # To keep the same height and width as the default theme:\n                \"continuousHeight\": 300,\n                \"continuousWidth\": 400,\n            },\n        }\n    }\n\n\nalt.themes.register(\"y_axis\", y_axis)\nalt.themes.enable(\"y_axis\")\n\n\ndef get_descriptions():\n    \"Parse descriptions of columns of Ames Housing dataset\"\n    with open(\"data_description.txt\") as reader:\n        descriptions = {}\n        for line in reader.readlines():\n            if \":\" in line and \"2nd level\" not in line:\n                descriptions[line.split(\": \")[0].strip()] = line.split(\": \")[1].strip()\n    return pd.Series(descriptions).rename(\"descriptions\")\n\n\ndescriptions = get_descriptions()",
    "crumbs": [
      "Notebooks",
      "Predicting house prices with pycaret"
    ]
  },
  {
    "objectID": "notebooks/ames-housing/index.html#read-and-explore-the-data",
    "href": "notebooks/ames-housing/index.html#read-and-explore-the-data",
    "title": "Predicting house prices with pycaret",
    "section": "Read and explore the data",
    "text": "Read and explore the data\n\n%%time\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\nprofile = ProfileReport(train, minimal=True, title=\"Ames Housing Profiling Report\")\nprofile.to_file(\"ames-housing-profiling-report-minimal.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 45 s, sys: 1.78 s, total: 46.7 s\nWall time: 16.4 s\n\n\n\nprofile.to_notebook_iframe()",
    "crumbs": [
      "Notebooks",
      "Predicting house prices with pycaret"
    ]
  },
  {
    "objectID": "notebooks/ames-housing/index.html#investigate-features-with-largest-predictive-power",
    "href": "notebooks/ames-housing/index.html#investigate-features-with-largest-predictive-power",
    "title": "Predicting house prices with pycaret",
    "section": "Investigate features with largest predictive power",
    "text": "Investigate features with largest predictive power\nWe use the Predictive Power Score to evaluate which features have the highest predictive power with respect to SalePrice.\n\npredictors = (\n    pps.predictors(train, \"SalePrice\")\n    .round(3)\n    .iloc[:, :-1]\n    .merge(descriptions, how=\"left\", left_on=\"x\", right_index=True)\n)\nbase = (\n    alt.Chart(predictors)\n    .encode(\n        x=alt.Y(\"x:N\").sort(\"-y\"),\n        y=\"ppscore\",\n        tooltip=[\"x\", \"ppscore\", \"descriptions\"],\n    )\n    .transform_filter(\"datum.ppscore &gt; 0\")\n)\nbase.mark_bar() + base.mark_text(align=\"center\", dy=-5)",
    "crumbs": [
      "Notebooks",
      "Predicting house prices with pycaret"
    ]
  },
  {
    "objectID": "notebooks/ames-housing/index.html#investigate-colinearity",
    "href": "notebooks/ames-housing/index.html#investigate-colinearity",
    "title": "Predicting house prices with pycaret",
    "section": "Investigate colinearity",
    "text": "Investigate colinearity\n\npps_matrix = (\n    pps.matrix(\n        train.loc[:, predictors.query(\"ppscore &gt; 0\")[\"x\"].tolist()],\n    )\n    .loc[:, [\"x\", \"y\", \"ppscore\"]]\n    .round(3)\n)\n(\n    alt.Chart(pps_matrix)\n    .mark_rect()\n    .encode(\n        x=\"x:O\",\n        y=\"y:O\",\n        color=\"ppscore:Q\",\n        tooltip=[\"x\", \"y\", \"ppscore\"])\n)",
    "crumbs": [
      "Notebooks",
      "Predicting house prices with pycaret"
    ]
  },
  {
    "objectID": "notebooks/ames-housing/index.html#build-models",
    "href": "notebooks/ames-housing/index.html#build-models",
    "title": "Predicting house prices with pycaret",
    "section": "Build models",
    "text": "Build models\nWe select the 30 features that have the highest predictive power score\n\nselected_predictors = (\n    predictors.sort_values(\"ppscore\", ascending=False).head(30)[\"x\"].to_list()\n)\nreg = setup(data = train.loc[:, selected_predictors + [\"SalePrice\"]], \n             target = 'SalePrice',\n             numeric_imputation = 'mean',\n             categorical_features =  list(train.loc[:, selected_predictors].select_dtypes(\"object\").columns), \n             feature_selection = False,\n             pca=False,\n             remove_multicollinearity=True,\n             remove_outliers = False,\n             normalize = True,\n             )\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n8378\n\n\n1\nTarget\nSalePrice\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(1460, 31)\n\n\n4\nTransformed data shape\n(1460, 116)\n\n\n5\nTransformed train set shape\n(1021, 116)\n\n\n6\nTransformed test set shape\n(439, 116)\n\n\n7\nOrdinal features\n1\n\n\n8\nNumeric features\n16\n\n\n9\nCategorical features\n14\n\n\n10\nRows with missing values\n94.7%\n\n\n11\nPreprocess\nTrue\n\n\n12\nImputation type\nsimple\n\n\n13\nNumeric imputation\nmean\n\n\n14\nCategorical imputation\nmode\n\n\n15\nMaximum one-hot encoding\n25\n\n\n16\nEncoding method\nNone\n\n\n17\nRemove multicollinearity\nTrue\n\n\n18\nMulticollinearity threshold\n0.900000\n\n\n19\nNormalize\nTrue\n\n\n20\nNormalize method\nzscore\n\n\n21\nFold Generator\nKFold\n\n\n22\nFold Number\n10\n\n\n23\nCPU Jobs\n-1\n\n\n24\nUse GPU\nFalse\n\n\n25\nLog Experiment\nFalse\n\n\n26\nExperiment Name\nreg-default-name\n\n\n27\nUSI\n81f6\n\n\n\n\n\n\n%%time\nselected_models = [model for model in models().index if model not in [\"lar\", \"lr\", \"ransac\"]]\nbest_model = compare_models(sort='RMSLE', include=selected_models)\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\nlightgbm\nLight Gradient Boosting Machine\n18267.8967\n969345616.0929\n30245.0381\n0.8400\n0.1474\n0.1051\n0.3780\n\n\ngbr\nGradient Boosting Regressor\n18349.4461\n1064907228.1139\n31464.4286\n0.8221\n0.1497\n0.1059\n0.0810\n\n\nrf\nRandom Forest Regressor\n18834.6022\n1052157810.7295\n31669.8884\n0.8263\n0.1530\n0.1091\n0.1370\n\n\npar\nPassive Aggressive Regressor\n18695.3332\n1145943934.1128\n32527.7429\n0.8093\n0.1535\n0.1061\n0.0560\n\n\nen\nElastic Net\n19941.1185\n1212771199.2709\n33679.9238\n0.8018\n0.1536\n0.1131\n0.0370\n\n\net\nExtra Trees Regressor\n19749.8604\n1158574471.9795\n33510.6172\n0.8073\n0.1591\n0.1138\n0.1370\n\n\nhuber\nHuber Regressor\n18580.7407\n1172797296.1965\n32571.8573\n0.8024\n0.1602\n0.1069\n0.0420\n\n\nbr\nBayesian Ridge\n20557.3468\n1251454965.3245\n34036.3809\n0.7934\n0.1715\n0.1191\n0.0380\n\n\nard\nAutomatic Relevance Determination\n20446.5401\n1229331466.4696\n33711.3986\n0.7969\n0.1747\n0.1193\n0.2740\n\n\nomp\nOrthogonal Matching Pursuit\n21882.7966\n1294135379.9217\n34955.8947\n0.7847\n0.1849\n0.1296\n0.0340\n\n\nada\nAdaBoost Regressor\n24866.3282\n1379609584.9159\n36498.7175\n0.7707\n0.2036\n0.1621\n0.0580\n\n\nknn\nK Neighbors Regressor\n26571.2016\n1730405638.7521\n40931.3774\n0.7200\n0.2050\n0.1518\n0.0360\n\n\ndt\nDecision Tree Regressor\n27747.5148\n2157234242.4490\n45330.0191\n0.6512\n0.2169\n0.1564\n0.0350\n\n\nllar\nLasso Least Angle Regression\n21458.2025\n1320695830.3446\n35006.4301\n0.7809\n0.2187\n0.1268\n0.0380\n\n\nlasso\nLasso Regression\n21455.6793\n1320742178.3808\n35006.1951\n0.7809\n0.2189\n0.1268\n0.2100\n\n\nridge\nRidge Regression\n21439.2241\n1318937040.3720\n34981.0548\n0.7812\n0.2196\n0.1266\n0.0360\n\n\nsvm\nSupport Vector Regression\n55543.3805\n6417749387.4994\n79739.3850\n-0.0524\n0.3979\n0.3195\n0.0450\n\n\ndummy\nDummy Regressor\n57352.4774\n6133919031.8184\n78021.7431\n-0.0086\n0.4061\n0.3635\n0.0340\n\n\ntr\nTheilSen Regressor\n29178.3219\n2564758742.0908\n49572.3895\n0.5667\n0.4258\n0.1978\n4.0290\n\n\nkr\nKernel Ridge\n182040.0692\n34133507087.4154\n184731.2672\n-4.7500\n1.7994\n1.1623\n0.0380\n\n\nmlp\nMLP Regressor\n166456.5847\n32851392125.7179\n181040.0031\n-4.4796\n2.7703\n0.9182\n0.2890\n\n\n\n\n\n\n\n\nCPU times: user 4.09 s, sys: 446 ms, total: 4.53 s\nWall time: 1min 3s",
    "crumbs": [
      "Notebooks",
      "Predicting house prices with pycaret"
    ]
  },
  {
    "objectID": "notebooks/ames-housing/index.html#evaluation",
    "href": "notebooks/ames-housing/index.html#evaluation",
    "title": "Predicting house prices with pycaret",
    "section": "Evaluation",
    "text": "Evaluation\n\nWith a standard, AutoML-like workflow, we achive RMSLE of 0.13 - 0.14 (over different runs), which is already in the top 25% of the 4,200 submissions on the leaderboard\nWe can now make predictions on the test set\n\n\npredictions = (\n    predict_model(best_model, data=test)\n    .rename(columns={\"prediction_label\": \"SalePrice\"})\n    .loc[:, [\"Id\", \"SalePrice\"]]\n)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nId\nSalePrice\n\n\n\n\n0\n1461\n126951.931078\n\n\n1\n1462\n142402.002648\n\n\n2\n1463\n185086.014955\n\n\n3\n1464\n191718.590497\n\n\n4\n1465\n186412.972060\n\n\n\n\n\n\n\n\nPipeline\n\nplot_model(best_model, 'pipeline')\n\n\n\n\n\n\n\n\n\nplot_model(best_model, 'feature')\n\n\n\n\n\n\n\n\n\n\n\n\nplot_model(best_model, 'residuals')",
    "crumbs": [
      "Notebooks",
      "Predicting house prices with pycaret"
    ]
  },
  {
    "objectID": "notebooks/overfitting/index.html",
    "href": "notebooks/overfitting/index.html",
    "title": "Demonstration of overfitting and underfitting",
    "section": "",
    "text": "import altair as alt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures",
    "crumbs": [
      "Notebooks",
      "Demonstration of overfitting and underfitting"
    ]
  },
  {
    "objectID": "notebooks/overfitting/index.html#python-explainer---how-does-the-polynomialfeatures-function-work",
    "href": "notebooks/overfitting/index.html#python-explainer---how-does-the-polynomialfeatures-function-work",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Python explainer - How does the ‘PolynomialFeatures()’ function work?",
    "text": "Python explainer - How does the ‘PolynomialFeatures()’ function work?\nIn this notebook, we use the PolynomialFeatures function from sklearn, which generates n-th order polynomial features, expanding a given X matrix. Depending on the value assigned to the parameter interaction_only, the features include all main terms and interaction terms (False; default), or only the first order main terms and the interaction term only (True). So, for example PolynomialFeatures(degree = 2) generates a matrix with a column for each term in the following equation:\n\\[1 + X_1 + X_2 + X_3 + X_1^2 + X_2^2 + X_3^2 + X_1X_2 + X_1X_3 + X_2X3.\\]\nNote, the default value for degree is 2. Below, we demonstrate the workings of PolynomialFeatures() for a simple dummy matrix.\n\n# Create a dummy X matrix (3x3).\nX = np.arange(9).reshape(3, 3)\n\nprint(\"X matrix:\")\nprint(X)\nprint(\"\")\n\n# Expanded matrix containing the first order main features and the interaction features. \npoly = PolynomialFeatures(degree = 2, interaction_only=True)\nprint(\"Expanded X matrix with first order and interaction features:\")\nprint(poly.fit_transform(X))\nprint(\"\")\n\n# Expanded matrix containing the first and second order main features and the interaction features.\npoly = PolynomialFeatures(degree = 2, interaction_only=False)\nprint(\"Expanded X matrix with first and second order and interaction features:\")\nprint(poly.fit_transform(X))\n\nX matrix:\n[[0 1 2]\n [3 4 5]\n [6 7 8]]\n\nExpanded X matrix with first order and interaction features:\n[[ 1.  0.  1.  2.  0.  0.  2.]\n [ 1.  3.  4.  5. 12. 15. 20.]\n [ 1.  6.  7.  8. 42. 48. 56.]]\n\nExpanded X matrix with first and second order and interaction features:\n[[ 1.  0.  1.  2.  0.  0.  0.  1.  2.  4.]\n [ 1.  3.  4.  5.  9. 12. 15. 16. 20. 25.]\n [ 1.  6.  7.  8. 36. 42. 48. 49. 56. 64.]]\n\n\nWe observe that when we set interaction_only to False, we also get the quadratic main terms. In addition, we also observe that the first column is filled with ones. Why is that?",
    "crumbs": [
      "Notebooks",
      "Demonstration of overfitting and underfitting"
    ]
  },
  {
    "objectID": "notebooks/overfitting/index.html#initialize-objects",
    "href": "notebooks/overfitting/index.html#initialize-objects",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Initialize objects",
    "text": "Initialize objects\n\n# Maximum number of degrees in polynomial.\nMAX_DEGREE = 20",
    "crumbs": [
      "Notebooks",
      "Demonstration of overfitting and underfitting"
    ]
  },
  {
    "objectID": "notebooks/overfitting/index.html#define-functions",
    "href": "notebooks/overfitting/index.html#define-functions",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Define functions",
    "text": "Define functions\nWe define a number of functions that are used in this notebook down below: (1) Create and evaluate models for different levels of flexibility. (2) Plot fitted data. (3a) Plot RMSE - Evaluate model based full data set (3b) Plot RMSE - Evaluate model based on train and test set\nFunction descriptions were added using docstring in Numpy format. In case you want an explanation about a function, simply type help(function name) and you will get to see the description.\n\n1 - Create and evaluate models for different levels of flexibility\n\ndef f_create_and_evaluate_model(df_data, b_split):\n\n    \"\"\"Creates and_evaluates models for different degrees of flexibility.\n\n    Parameters\n    ----------\n    df_data : dataframe\n        Dataframe with attribute 'Y' in the first column and X features in the second column and onwards (if applicable).\n    b_split : bool\n        Should the models be based on the full data set (b_split = False) or on a train/test split (b_split = True).\n\n    Returns\n    -------\n    dataframe\n        a dataframe with features, including: degree, RMSE, and predictions.\n    \"\"\"  \n\n    # Initialize.\n    results = []\n\n    if b_split:\n        X_train, X_val, y_train, y_val = train_test_split(df_data.iloc[:, 1:], df_data.Y, test_size = 0.2, random_state = 42)\n\n    # Go through each degree of freedom from 1 to MAX_DEGREE and fit each polynomial to X features. \n    for n in range(1, MAX_DEGREE + 1):\n        \n        # Create object with framework for MAX_DEGREE polynomials.\n        poly = PolynomialFeatures(n, interaction_only=False)\n\n        # Apply X features to each of the MAX_DEGREE polynomials.\n        Xp_all = poly.fit_transform(df_data.iloc[:, 1:])\n\n        if not b_split:\n\n            # Fit MAX_DEGREE polynomials to the same response 'Y'.\n            fit = LinearRegression().fit(Xp_all, df_data.Y)\n\n            # Use fit object to calculate predicted 'Y' for all X's.\n            Y_hat_all = fit.predict(Xp_all)\n\n            # Append performance metrics to results list.\n            results.append(\n                {\n                    \"degree\":  n,\n                    \"rmse\":    round(np.sqrt(mean_squared_error(df_data.Y, Y_hat_all))),\n                    \"X_hat\":   Xp_all[:,1],\n                    \"Y_hat\":   Y_hat_all\n                }\n            )\n\n        else:\n\n            # Apply X_train to each of the MAX_DEGREE polynomials.\n            Xp_train = poly.fit_transform(X_train)\n\n            # Apply X_val to each of the MAX_DEGREE polynomials.\n            Xp_val = poly.fit_transform(X_val)\n\n            # Fit the training data to y_train.\n            fit = LinearRegression().fit(Xp_train, y_train)\n\n            # Use fit object to calculate predicted Y for training set.\n            Y_hat_train = fit.predict(Xp_train)\n\n            # Use fit object to calculate predicted Y for test set.\n            Y_hat_val = fit.predict(Xp_val)\n\n            # Use fit object to calculate predicted 'Y' for full set.\n            Y_hat_all = fit.predict(Xp_all)\n\n            # Extend with list of performance metrics to results list, 'results'.\n            results.extend(\n                [\n                    {\n                        \"degree\": n,\n                        \"fold\":   \"train\",\n                        \"rmse\":   np.sqrt(mean_squared_error(y_train, Y_hat_train)),\n                        \"X_hat\":  Xp_train[:,1],\n                        \"Y_hat\":  Y_hat_train\n                    },\n\n                    {\n                        \"degree\": n,\n                        \"fold\":   \"test\",\n                        \"rmse\":   np.sqrt(mean_squared_error(y_val, Y_hat_val)),\n                        \"X_hat\":  Xp_val[:,1],\n                        \"Y_hat\":  Y_hat_val\n                    },\n\n                    {\n                        \"degree\": n,\n                        \"fold\":   \"full_set\",\n                        \"X_hat\":  Xp_all[:,1],\n                        \"Y_hat\":  Y_hat_all,\n                        \n                    },\n                ]\n            )\n\n    # Convert list to data frame.\n    df_results = pd.DataFrame.from_records(results)\n\n    return df_results\n\n\n\n2 - Plot fitted data\n\ndef f_plot_data_and_two_models(df_data, df_results, n_i, n_ii, b_split):\n\n    \"\"\"Make a plot of data and of two selected fitted models.\n\n    Parameters\n    ----------\n    df_data : dataframe\n        Dataframe with attribute 'Y' in the first column and X features in the second column and onwards (if applicable).\n    df_results : dataframe\n        Dataframe with results from f_create_and_evaluate_model().\n    n_i : int\n        Degrees of flexibility (polynomial order) you want to plot predict model for (first model).\n    n_ii : int\n        Degrees of flexibility (polynomial order) you want to plot predict model for (second model).\n    b_split : bool\n        Should the models be based on the full data set (b_split = False) or on a train/test split (b_split = True).\n\n    Returns\n    -------\n    plot\n        a plot of the data with two predicted models.\n    \"\"\"\n\n    # In case of the full data set each row in df_result is an iteration.\n    # In case of a train/test split each three rows in df_result constitute an iteration.\n    if b_split:\n\n        index_X_hat    = 2\n        index_Y_hat_i  = (n_i  - 1) * 3 + 2\n        index_Y_hat_ii = (n_ii - 1) * 3 + 2\n    \n    else:\n\n        index_X_hat    = 0\n        index_Y_hat_i  = n_i  - 1\n        index_Y_hat_ii = n_ii - 1\n\n\n    # Define plot results data frame.\n    df_plot_results = pd.DataFrame({\n        \n        \"X_hat\":    df_results.loc[index_X_hat,    \"X_hat\"],  \n        \"Y_hat_i\":  df_results.loc[index_Y_hat_i,  \"Y_hat\"],\n        \"Y_hat_ii\": df_results.loc[index_Y_hat_ii, \"Y_hat\"]\n    })\n\n\n    # POINTS: Plot point markers\n    plot_data = alt.Chart(df_data).mark_point(color = \"grey\").encode(x = \"X1\", y = \"Y\")\n\n    # LINES: Plot first fitted model\n    plot_n_i = alt.Chart(df_plot_results).mark_line(color = \"blue\").encode(x = \"X_hat\", y = \"Y_hat_i\")\n\n    plot_n_ii = alt.Chart(df_plot_results).mark_line(color = \"green\").encode(x = \"X_hat\", y = \"Y_hat_ii\")\n\n    # Simply layer the three plots into one.\n    return plot_data + plot_n_i + plot_n_ii\n\n\n\n3 - Plot RMSE\nCreate a plot of the RMSE against the flexibility of the model (degrees of freedom).\n\n3a - Plot RMSE - Evaluate model based full data set\n\ndef f_plot_rmse_full_dataset(df_results):\n\n    \"\"\"Make a plot of RMSE against the flexibility of the model (degrees of freedom)\n    in case of evaluating the model based on the full data set.\n\n    Parameters\n    ----------\n    df_results : dataframe\n        Dataframe with results from f_create_and_evaluate_model().\n\n    Returns\n    -------\n    plot\n        a plot of the RMSE.\n    \"\"\"\n    \n    return alt.Chart(df_results).mark_line(point = alt.OverlayMarkDef()).encode(x = \"degree\", y = \"rmse\", tooltip = [\"degree\", \"rmse\"])\n\n\n\n3b - Plot RMSE - Evaluate model based on train and test set\n\n# Define function to plot performance metric (RMSE)\ndef f_plot_rmse_train_test_set(df_results):\n\n    \"\"\"Make a plot of RMSE against the flexibility of the model (degrees of freedom)\n    in case of evaluating the model based on the train/test set.\n\n    Parameters\n    ----------\n    df_results : dataframe\n        Dataframe with results from f_create_and_evaluate_model().\n\n    Returns\n    -------\n    plot\n        a plot of the RMSE.\n    \"\"\"\n\n    # Remove full_set data, leaving only the train and test data.\n    df_results = df_results[df_results.fold != \"full_set\"]\n\n    # Define base line chart.\n    base = alt.Chart(df_results).mark_line(\n            \n            point=alt.OverlayMarkDef()\n\n        ).encode(x = \"degree\",y = \"rmse\", color = \"fold\"\n    )\n\n    label = alt.selection_point(\n\n        encodings = ['x'],       # Limit selection to x-axis value\n        on        = 'mouseover', # Select on mouseover events\n        nearest   = True,        # Select data point nearest to the cursor\n        empty     = 'none'       # Empty selection includes no data points\n    )\n\n    return alt.layer(\n\n        # Base line chart.\n        base, \n        \n        ######################################################################################\n        #\n        # I '#'-ed the definition of label.  I think the \n        ######################################################################################\n        \n        # Add a rule mark to serve as a guide line\n        alt.Chart().mark_rule(color = '#aaa').encode(x = 'degree').transform_filter(label),\n        \n        # Add circle marks for selected time points, hide unselected points\n        base.mark_circle().encode(\n            \n            opacity = alt.condition(label, alt.value(1), alt.value(0))\n\n        ).add_params(label),\n\n        # Add white stroked text to provide a legible background for labels\n        base.mark_text(\n            align       = 'left',\n            dx          = 5,\n            dy          = -5,\n            stroke      = 'white',\n            strokeWidth = 2\n        ).encode(text='rmse:Q').transform_filter(label),\n\n        # Add text labels for stock prices.\n        base.mark_text(\n            align = 'left',\n            dx    = 5,\n            dy    = -5\n        ).encode(text='rmse:Q').transform_filter(label),\n        \n        data = df_results\n    )",
    "crumbs": [
      "Notebooks",
      "Demonstration of overfitting and underfitting"
    ]
  },
  {
    "objectID": "notebooks/overfitting/index.html#section-1---overfitting",
    "href": "notebooks/overfitting/index.html#section-1---overfitting",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Section 1 - Overfitting",
    "text": "Section 1 - Overfitting\nWe simulate a true function \\(Y\\) as a third-order polynomial of \\(X1\\). We show that the error of the fitted function decreases as you increase the complexity of the fitted functions, i.e. fitting polynomials with degree &gt; 3.\n\n# Number of data\nn_data = 50\n\n# We set a fixed random seed to reproduce our results below. In which cases would we not want to set a fixed seed?\nnp.random.seed(123)\n\n# Define X, error, and model including error.\nx = np.sort(np.random.normal(loc = 10, scale = 5, size = n_data))\ne = np.random.normal(loc = 0, scale = 2000, size = n_data)\ny = (2530 + 20*x - 10*(x**2) + 5*(x**3) + e)\n\n# Combine X1 and Y in a data frame.\ndf_data1 = pd.DataFrame({\"Y\": y, \"X1\": x})\n\n# Create and evaluate models of various flexibility.\ndf_results1 = f_create_and_evaluate_model(df_data = df_data1, b_split = False)\n\n\n# Plot data and two fitted models. Select fitted models by updating 'n_i' and 'n_ii'.\nf_plot_data_and_two_models(df_data = df_data1, df_results = df_results1, n_i = 1, n_ii = 4, b_split = False)\n\n\n\n\n\n\n\n\n# Plot RMSE against model flexibility.\nf_plot_rmse_full_dataset(df_results = df_results1)\n\n\n\n\n\n\n\nWe observe that the RMSE increaes again at degree &gt; 16. This is due to the function becoming overly flexible that it starts missing (overshooting) data points.\nQuestion - Why does the error not drop below 2000 up to 20 degrees of freedom?",
    "crumbs": [
      "Notebooks",
      "Demonstration of overfitting and underfitting"
    ]
  },
  {
    "objectID": "notebooks/overfitting/index.html#section-2---overfitting-with-random-variables",
    "href": "notebooks/overfitting/index.html#section-2---overfitting-with-random-variables",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Section 2 - Overfitting with random variables",
    "text": "Section 2 - Overfitting with random variables\nWe now add features \\(X2, X3\\) but keep the true function \\(Y\\) unchanged, i.e. only dependent on \\(X1\\). We show that these random variables may lead to even more severe overfitting.\n\n# Make a copy of df_data1.\ndf_data2 = df_data1.copy()\n\n# Add two more features, X2 and X3. Response variable Y is independent of X2 and X3.\nfor i in [2, 3]:\n\n    df_data2[f\"X{str(i)}\"] = np.random.normal(loc = 10, scale = 5, size = n_data)\n\n# Create and evaluate models of various flexibility.\ndf_results2 = f_create_and_evaluate_model(df_data = df_data2, b_split = False)\n\n\n# Plot data and two fitted models. Select fitted models by updating 'n_i' and 'n_ii'.\nf_plot_data_and_two_models(df_data = df_data2, df_results = df_results2, n_i = 1, n_ii = 2, b_split = False)\n\n\n\n\n\n\n\n\n# Plot RMSE against model flexibility.\nf_plot_rmse_full_dataset(df_results2)\n\n\n\n\n\n\n\nWith additional features we need fewer degrees of freedom in the polynomial. The additional features result in more flexibility in the model.",
    "crumbs": [
      "Notebooks",
      "Demonstration of overfitting and underfitting"
    ]
  },
  {
    "objectID": "notebooks/overfitting/index.html#section-3---testing-to-prevent-overfitting",
    "href": "notebooks/overfitting/index.html#section-3---testing-to-prevent-overfitting",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Section 3 - Testing to prevent overfitting",
    "text": "Section 3 - Testing to prevent overfitting\nWe follow example from sklearn library, with cosine as true function.\n\n# Define function with the true model.\ndef true_fun(x, e):\n    return np.cos(1.5 * np.pi * x) + e\n\n# We set a fixed random seed to reproduce our results below. In which cases would we not want to set a fixed seed?\nnp.random.seed(12)\n\n# Number of data.\nn_data = 50\n\n# Define X, error, and model including error.\nx = np.sort(np.random.rand(n_data))\ne = np.random.randn(n_data) * 0.1\ny = true_fun(x, e)\n\n# Put data in data frame.\ndf_data3 = pd.DataFrame({\"Y\": y, \"X1\": x})\n\n# Create and evaluate models of various flexibility.\ndf_results3 = f_create_and_evaluate_model(df_data = df_data3, b_split = True)\n\n\n# Plot data and two fitted models. Select fitted models by updating 'n_i' and 'n_ii'.\n# The plot shows all data (train + test), while the models are fitted on the training data.\n# Try out different n_ii: start at 16 and increase with steps of 1. What do we see?\nf_plot_data_and_two_models(df_data = df_data3, df_results = df_results3, n_i = 1, n_ii = 5, b_split = True)\n\n\n\n\n\n\n\n\n# Plot RMSE against model flexibility.\nf_plot_rmse_train_test_set(df_results3)\n\n/Users/macstudio/opt/anaconda3/envs/py310/lib/python3.10/site-packages/altair/vegalite/v5/api.py:355: AltairDeprecationWarning: The value of 'empty' should be True or False.\n  warnings.warn(",
    "crumbs": [
      "Notebooks",
      "Demonstration of overfitting and underfitting"
    ]
  },
  {
    "objectID": "notebooks/overfitting/index.html#section-4---traintest-split-can-still-lead-to-overfitting",
    "href": "notebooks/overfitting/index.html#section-4---traintest-split-can-still-lead-to-overfitting",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Section 4 - Train/test split can still lead to overfitting",
    "text": "Section 4 - Train/test split can still lead to overfitting\nTrain/test split isn’t a 100% safe-guard against overfitting. Given a quadratic (second order) true function, testing still results in a fourth order fitted function.\n\n# Set random seed, to reproduce results.\nnp.random.seed(456)\n\n# Number of data.\nn_data = 150\n\n# Define X, error, and model including error.\nx = np.random.normal(-10, 6, n_data)\ne = np.random.normal(0, 75, n_data)\ny = 25 + 2*x - 4*(x**2) + e\n\n# Bring X and y together in a data frame.\ndf_data4 = pd.DataFrame({\"Y\": y, \"X1\": x})\n\n# Create and evaluate models of various flexibility.\ndf_results4 = f_create_and_evaluate_model(df_data = df_data4, b_split = True)\n\n\n# Plot data and two fitted models. Select fitted models by updating 'n_i' and 'n_ii'.\n# The plot shows all data (train + test), while the models are fitted on the training data.\nf_plot_data_and_two_models(df_data = df_data4, df_results = df_results4, n_i = 1, n_ii = 2, b_split = True)\n\n\n\n\n\n\n\n\n# Plot RMSE against model flexibility.\nf_plot_rmse_train_test_set(df_results4)\n\n/Users/macstudio/opt/anaconda3/envs/py310/lib/python3.10/site-packages/altair/vegalite/v5/api.py:355: AltairDeprecationWarning: The value of 'empty' should be True or False.\n  warnings.warn(",
    "crumbs": [
      "Notebooks",
      "Demonstration of overfitting and underfitting"
    ]
  },
  {
    "objectID": "notebooks/overfitting/index.html#cross-validation---work-in-progress",
    "href": "notebooks/overfitting/index.html#cross-validation---work-in-progress",
    "title": "Demonstration of overfitting and underfitting",
    "section": "Cross-validation - Work in Progress",
    "text": "Cross-validation - Work in Progress\nBy using k-fold cross-validation, we have a better safeguard against overfitting. We reproduce the example from Will Koehrsen.\nNB: WORK IN PROGRESS. Example is not the best in terms of train-validation curves.\n\n# def fit_poly(train, y_train, test, y_test, degrees, plot='train', return_scores=False):\n    \n#     # Create a polynomial transformation of features\n#     features = PolynomialFeatures(degree=degrees, include_bias=False)\n    \n#     # Reshape training features for use in scikit-learn and transform features\n#     train = train.reshape((-1, 1))\n#     train_trans = features.fit_transform(train)\n    \n#     # Create the linear regression model and train\n#     model = LinearRegression()\n#     model.fit(train_trans, y_train)\n    \n#     # Calculate the cross validation score\n#     cross_valid = cross_val_score(model, train_trans, y_train, scoring='neg_mean_squared_error', cv = 5)\n    \n#     # Training predictions and error\n#     train_predictions = model.predict(train_trans)\n#     training_error    = mean_squared_error(y_train, train_predictions)\n    \n#     # Format test features\n#     test = test.reshape((-1, 1))\n#     test_trans = features.fit_transform(test)\n    \n#     # Test set predictions and error\n#     test_predictions = model.predict(test_trans)\n#     testing_error = mean_squared_error(y_test, test_predictions)\n    \n#     # Find the model curve and the true curve\n#     x_curve = np.linspace(0, 1, 100)\n#     x_curve = x_curve.reshape((-1, 1))\n#     x_curve_trans = features.fit_transform(x_curve)\n    \n#     # Model curve\n#     model_curve = model.predict(x_curve_trans)\n    \n#     # True curve\n#     y_true_curve = true_fun(x_curve[:, 0])\n\n#      # Return the metrics\n#     if return_scores:\n#         return training_error, testing_error, -np.mean(cross_valid)\n\n\n# x = np.sort(np.random.rand(120))\n# y = true_fun(x) + 0.1 * np.random.randn(len(x))\n\n# # Random indices for creating training and testing sets\n# random_ind = np.random.choice(list(range(120)), size = 120, replace=False)\n# xt = x[random_ind]\n# yt = y[random_ind]\n\n# # Training and testing observations\n# train = xt[:int(0.7 * len(x))]\n# test = xt[int(0.7 * len(x)):]\n\n# y_train = yt[:int(0.7 * len(y))]\n# y_test = yt[int(0.7 * len(y)):]\n\n# # Model the true curve\n# x_linspace = np.linspace(0, 1, 1000)\n# y_true = true_fun(x_linspace)\n\n# # Range of model degrees to evaluate\n# degrees = [int(x) for x in np.linspace(1, 40, 40)]\n\n# # Results dataframe\n# results5 = pd.DataFrame(0, columns = ['train_error', 'test_error', 'cross_valid'], index = degrees)\n\n# # Try each value of degrees for the model and record results\n# for degree in degrees:\n#     degree_results = fit_poly(train, y_train, test, y_test, degree, plot=False, return_scores=True)\n#     results5.loc[degree, 'train_error'] = degree_results[0]\n#     results5.loc[degree, 'test_error'] = degree_results[1]\n#     results5.loc[degree, 'cross_valid'] = degree_results[2]\n\n# # print('10 Lowest Cross Validation Errors\\n')\n# # train_eval = results5.sort_values('cross_valid').reset_index(level=0).rename(columns={'index': 'degrees'})\n# # train_eval.loc[:,['degrees', 'cross_valid']].head(10)\n\n\n# import matplotlib.pyplot as plt\n\n# plt.plot(results5.index, results5['train_error'], 'b-o', ms=6, label = 'Training Error')\n# plt.plot(results5.index, results5['test_error'], 'r-*', ms=6, label = 'Testing Error')\n# plt.legend(loc=2); plt.xlabel('Degrees'); plt.ylabel('Mean Squared Error'); plt.title('Training and Testing Curves');\n# plt.ylim(0, 0.05); plt.show()\n\n# print('\\nMinimum Training Error occurs at {} degrees.'.format(int(np.argmin(results5['train_error']))))\n# print('Minimum Testing Error occurs at {} degrees.\\n'.format(int(np.argmin(results5['test_error']))))",
    "crumbs": [
      "Notebooks",
      "Demonstration of overfitting and underfitting"
    ]
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Anthology of Data Science",
    "section": "",
    "text": "An Introduction to Statistical Learning\n\n\nStatistical learning has become a critical toolkit for anyone who wishes to understand data. This book provides a broad and less technical treatment of key topics in…\n\n\n\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Jonathan Taylor\n\n\nJul 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Principles & Practice\n\n\nThis textbook is provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to be able to use them sensibly.\n\n\n\nRob J Hyndman, George Athanasopoulos\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentals of Data Visualization\n\n\nThe book is meant as a guide to making visualizations that accurately reflect the data, tell a story, and look professional.\n\n\n\nClaus O. Wilke\n\n\nAug 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretable Machine Learning\n\n\nThis book is about making machine learning models and their decisions interpretable. It will enable you to select and correctly apply the interpretation method that is most…\n\n\n\nChristoph Molnar\n\n\nMar 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython of Data Analysis, Third Edition\n\n\nThis book is concerned with the nuts and bolts of manipulating, processing, cleaning, and crunching data in Python.\n\n\n\nWes McKinney\n\n\nApr 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR for Data Science, Second Edition\n\n\nThis book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it and visualize.\n\n\n\nHadley Wickham, Mine Çetinkaya-Rundel, Garrett Grolemenund\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Turing Way\n\n\nAn open source, open collaboration, community-driven handbook to reproducible, ethical and collaborative data science.\n\n\n\nThe Turing Way Community\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\nThis book explains the ideas that underlie deep learning, distinguishing it from volumes that cover coding and other practical aspects. More resources can be found on &lt;a…\n\n\n\nSimon Prince\n\n\nOct 23, 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Books"
    ]
  }
]