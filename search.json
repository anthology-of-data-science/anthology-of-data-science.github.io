[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An anthology of open access data science materials",
    "section": "",
    "text": "Machine learning is permeating many fields of work. As a new ‘system technology’1, its expected impact on organizations and society is expected to that of the steam engine or electricity.\nMore and more professionals are seeking to acquire the necessary understanding and skills to apply machine learning in their day-to-day work. As such, more people without a background in either computer science or statistics - let alone both - have a need for high-quality, open access content to explore and learn data science by themselves.\nNow there is a lot of machine learning learning materials out there, so why this anthology? Based on my experience in teaching professional education course on data & AI, I am continouly challenged to:\n\ncurate content for different professional learning paths, combining various existing open access materials. Key books, papers, libraries\nbalance between too technical vs too vague, handwaving or even downright wrong\nproblem-based approach: rather than, say, explaining the principles underlying regularization, we choose to demonstrate these principles using the simplest algorithms. With a little math, everyone should be able to understand how LASSO performs regularisation for regression models. With this intuitive understanding, you can move on to more complex algorithms and applications, and intuitively reason where and how to use regularisation."
  },
  {
    "objectID": "index.html#why-this-anthology",
    "href": "index.html#why-this-anthology",
    "title": "An anthology of open access data science materials",
    "section": "",
    "text": "Machine learning is permeating many fields of work. As a new ‘system technology’1, its expected impact on organizations and society is expected to that of the steam engine or electricity.\nMore and more professionals are seeking to acquire the necessary understanding and skills to apply machine learning in their day-to-day work. As such, more people without a background in either computer science or statistics - let alone both - have a need for high-quality, open access content to explore and learn data science by themselves.\nNow there is a lot of machine learning learning materials out there, so why this anthology? Based on my experience in teaching professional education course on data & AI, I am continouly challenged to:\n\ncurate content for different professional learning paths, combining various existing open access materials. Key books, papers, libraries\nbalance between too technical vs too vague, handwaving or even downright wrong\nproblem-based approach: rather than, say, explaining the principles underlying regularization, we choose to demonstrate these principles using the simplest algorithms. With a little math, everyone should be able to understand how LASSO performs regularisation for regression models. With this intuitive understanding, you can move on to more complex algorithms and applications, and intuitively reason where and how to use regularisation."
  },
  {
    "objectID": "index.html#how-is-this-anthology-set-up",
    "href": "index.html#how-is-this-anthology-set-up",
    "title": "An anthology of open access data science materials",
    "section": "How is this anthology set up?",
    "text": "How is this anthology set up?\n\nIntroduction to Statistical Learning (ISL)\n“Introduction to Statistical Learning” (ISL) is a valuable resource not only for technical professionals but also for business managers and individuals with limited technical backgrounds who are interested in understanding machine learning.\n\nAccessible Language and Concepts: ISL is written in an accessible and non-technical language. It avoids complex jargon and assumes little prior technical knowledge. This makes it easier for business managers and non-technical individuals to grasp the core concepts of machine learning without feeling overwhelmed.\nPractical Examples and Case Studies: The book includes numerous practical examples and case studies that illustrate how machine learning can be applied to real-world business problems. Business managers can relate to these examples and see how machine learning can add value to their organizations.\nIntuitive Explanations: ISL provides intuitive explanations of statistical and machine learning concepts. It uses visual aids, graphs, and figures to simplify complex ideas. This approach helps non-technical readers understand the key principles behind various algorithms and techniques.\nStep-by-Step Approach: The book takes a step-by-step approach to explain the processes involved in machine learning, from data preprocessing to model evaluation. This helps non-technical readers follow the logical progression of building and evaluating machine learning models.\nPractical Labs and Exercises: ISL includes practical labs and exercises with accompanying datasets. These exercises enable readers to gain hands-on experience with implementing machine learning techniques, even if they have limited technical skills. This practical exposure can be valuable for understanding the actual process of model building.\nUnderstanding Model Outputs: The book helps non-technical individuals understand how to interpret the outputs of machine learning models. It covers topics like model accuracy, variable importance, and model selection, which are crucial for making informed business decisions based on machine learning results.\nDecision-Making Support: Business managers can benefit from ISL by learning how to leverage machine learning for data-driven decision-making. Understanding the basics of machine learning allows them to collaborate more effectively with technical teams, set realistic expectations, and make informed decisions about which machine learning techniques to apply in specific business scenarios.\nComplementary to Data Literacy: ISL can be a valuable complement to data literacy efforts within an organization. It helps individuals understand how data is transformed into actionable insights through machine learning, enhancing their data-driven decision-making capabilities.\nCross-Functional Collaboration: By reading ISL, business managers and non-technical individuals can bridge the communication gap between technical and non-technical teams. This can facilitate more effective collaboration on machine learning projects within an organization.\n\n\n\nInterpretable Machine Learning (IML)\n\n\nForecasting Principles & Practice (FPP)\n\n\nData workflow\n\nThe Turing Way\nTidy data, Wickham\n\n\n\nData visualization\n\nGrammar of Graphics\nAltair\n\n\n\nGetting into Python with realpython.com"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "An anthology of open access data science materials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSheikh et al. (2023), Mission AI: the New System Technology, https://doi.org/10.1007/978-3-031-21448-6↩︎"
  },
  {
    "objectID": "basics/index.html",
    "href": "basics/index.html",
    "title": "The basics of machine learning",
    "section": "",
    "text": "The Precision-Recall Plot Is More Informative than the ROC Plot\n\n\nAn introduction to performance metrics for binary classification. Originally published on &lt;a…\n\n\n\nPaul van der Laken, Daniel Kapitan\n\n\nAug 16, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nA visual introduction to machine learning\n\n\nOriginally published on r2d3.us.\n\n\n\nStephanie Yee, Tony Chu\n\n\nJul 27, 2015\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "visualisation/stop-aggregating-signal.html",
    "href": "visualisation/stop-aggregating-signal.html",
    "title": "Stop aggregating away the signal in your data",
    "section": "",
    "text": "For five years as a data analyst, I forecasted and analyzed Google’s revenue. For six years as a data visualization specialist, I’ve helped clients and colleagues discover new features of the data they know best. Time and time again, I’ve found that by being more specific about what’s important to us and embracing the complexity in our data, we can discover new features in that data. These features can lead us to ask better data-driven questions that change how we analyze our data, the parameters we choose for our models, our scientific processes, or our business strategies.\nMy colleagues Ian Johnson, Mike Freeman, and I recently collaborated on a series of data-driven stories about electricity usage in Texas and California to illustrate best practices of Analyzing Time Series Data. We found ourselves repeatedly changing how we visualized the data to reveal the underlying signals, rather than treating those signals as noise by following the standard practice of aggregating the hourly data to days, weeks, or months.Behind many of the best practices we recommended for time series analysis was a deeper theme: actually embracing the complexity of the data.\nAggregation is the standard best practice for analyzing time series data, but it can create problems by stripping away crucial context so that you’re not even aware of how much potential insight you’ve lost. In this article, I’ll start by discussing how aggregation can be problematic, before walking through three specific alternatives to aggregation with before/after examples that illustrate:"
  },
  {
    "objectID": "visualisation/stop-aggregating-signal.html#what-is-the-problem-with-aggregation",
    "href": "visualisation/stop-aggregating-signal.html#what-is-the-problem-with-aggregation",
    "title": "Stop aggregating away the signal in your data",
    "section": "What is the problem with aggregation?",
    "text": "What is the problem with aggregation?\nWe praise the importance of large, rich datasets when we talk about algorithms and teaching machines to learn from data. However, too often when we visualize data so that we as humans can make sense of it, especially time series data, we make the data smaller and smaller.\nAggregation is the default for a reason. It can feel overwhelming to handle the quantities of data we now have at our fingertips. It doesn’t have to be very “big data” to have more than 1M data points, more than the number of pixels on a basic laptop screen. There are many robust statistical approaches to effective aggregation and aggregation that can provide valuable context (comparing to median, for example). In other cases, we need to see more details while trying to find the key insight, but once we’ve finished the analysis and know which features of the data matter most, then aggregation can be a useful tool for focusing attention to communicate that insight.\nBut every time you aggregate, you make a decision about which features of your data matter and which ones you are willing to drop: which are the signal and which are the noise. When you smooth out a line chart, are you doing it because you’ve decided that the daily average is most important and that you don’t care about the distribution or seasonal variation in your peak usage hours? Or are you doing it because it’s the only way you know how to make the jagged lines in your chart go away?\nInformed aggregation simplifies and prioritizes. Uninformed aggregation means you’ll never know what insights you lost.\nIn our rush to aggregate, we sometimes forget that the numbers are tied to real things. To people’s actions. To the hourly, daily, weekly, monthly, and seasonal patterns that are so familiar that they’re almost forgettable. Or maybe it’s that we so rarely see disaggregated data presented effectively in practice that we don’t even realize it’s an option. By considering these seasonal patterns, these human factors, we could embrace complexity in more meaningful ways.\nConsider how much energy we use. If we take a moment to think about it, it’s obvious that we use a lot more energy in the late afternoon than the early morning, so we’d expect big dips and troughs every day. It also shouldn’t be a surprise that the daily energy usage patterns on a summer day and a winter day are different. These patterns aren’t noise, but rather are critical to making sense of this data. We especially need this context to tell what is expected and what is noteworthy.\nHowever, when our dataset has big, regular fluctuations from day to day or hour to hour, our line charts end up looking like an overwhelming mess of jagged lines. Consider this chart showing the 8,760 data points representing one year of data on hourly energy use in California.\n\nA standard way to deal with this overwhelming chart is to apply a moving average by day, week, or month (defined as a four-week period).\n\nYes, now we have a simple chart, and can easily see that the lowest energy usage is in April, and the peak in late August. But we could see that in the first chart. Moreover, we’ve smoothed out anything else of interest. We’ve thrown away so much information that we don’t even know what we’ve lost.\nIs this annual pattern, with a dip in April and a peak in August, consistent for all hours of the day? Do some hours of day or days of week change more than others through the seasons? Were there any hours, days, or weeks that were unusual for their time of year/time of day? What are the outliers? Is energy usage equally variable through all times of year, or are some weeks/seasons/hours more consistent than others?\nDespite starting with data that should contain the answers to these questions, we can’t answer them. Moreover, the smoothed line doesn’t even give us any hints about what questions to ask or what’s worth digging into more."
  },
  {
    "objectID": "visualisation/stop-aggregating-signal.html#solution-embrace-the-complexity-by-rearranging-augmenting-and-using-the-data-itself-to-provide-context.",
    "href": "visualisation/stop-aggregating-signal.html#solution-embrace-the-complexity-by-rearranging-augmenting-and-using-the-data-itself-to-provide-context.",
    "title": "Stop aggregating away the signal in your data",
    "section": "Solution: Embrace the complexity by rearranging, augmenting, and using the data itself to provide context.",
    "text": "Solution: Embrace the complexity by rearranging, augmenting, and using the data itself to provide context.\n\n#1: Don’t aggregate: Rearrange\nWhat if we considered what categories likely matter based on what we know of human behavior and environmental factors, especially temperature? Factors like time of day and time of year? In Discovering Data Patterns, we grouped the data into 96 small, aligned tick charts, one for each hour of the day for each season of the year and organized the visualization around the concepts most likely to matter. The x-axis for each mini chart is the amount of electricity used, and each tick mark represents a single hour on a specific day.\n\nThis way we can immediately see what’s typical or unusual for each hour and quarter. For example, generally more energy is used at midnight in winter than at 3am. Skimming down a column, we can see the shape of a day for each season. And, we can see how energy demand by hour changes across seasons by comparing each column to the next.\nNow the “noise” has become the signal. We can clearly answer the questions we posed above:\n\nIs this annual pattern consistent for all hours of the day?\n\nNo, the “shape” of energy used during the course of the day is different in winter vs. summer, with a double peak in Q1 and a single peak in Q3. Also, Q4 looks a lot like Q1, except for a few unusual days. And Q2 shows the most variability in “shape” of day.\n\nDo some hours of day or days of week change more than other hours through the seasons?\n\nYes, late afternoon and evening hours show much more of an increase in energy usage from Q1 to Q3 than early morning hours.\n\nWere there any hours, days, or weeks that were unusual for their time of year/time of day?\n\nYes. For example, in Q4 some very unusual days saw high energy usage in the evening.\nYes. In Q3 in the early morning hours (between 4am and 6am), there were some outlier days with much higher energy usage.\n\nIs energy usage equally variable through all times of year, or are some weeks/seasons/hours more consistent than others?\n\nNo! Q1 has very more consistent energy usage, with a very narrow range of energy used for any particular hour of the day. Meanwhile, Q2 shows very inconsistent energy usage, with a lot of variability especially in the higher-energy evening hours.\n\n\nNot only do we notice some patterns immediately, but this view of the data also gives us the chance to go deeper just by looking more closely (and doing some basic research into what was going on in California at that time).\nLet’s look closer at the early morning hours of Q3. There were some abnormally high values between 4pm and 6pm. Interactive tooltips reveal that these took place on August 19. A quick Google search for “California Aug 19th 2020” shows that the region was suffering from wildfires, so perhaps people kept windows closed and the AC on instead of opening their windows to the cooler nighttime air. September 6 also shows up among the highest values, and a search indicates a likely cause: a record-breaking heat wave in California that hit the national news while the fires continued to burn.\n\nOverall, our faceted tick heatmap chart has the same number of data points as the original jagged line, but now we can see the underlying daily and seasonal patterns (and how daily patterns vary by season) as well as the relative outliers. The more time we spend with the chart, the more we notice, as it invites us to ask new data-driven questions.\n\n\n#2: Augment first, then group or color.\n\nBring in common knowledge: augment with familiar classifications\n\nAt another point in our exploratory analysis, we looked at a chart showing 52 weeks of hourly energy usage in California (shown above) and noticed that the higher-energy weeks seemed to have a single bump each day in the evening while lower-energy weeks seemed to have more of a double bump (see above). This is actually the same pattern revealed in section #1 on rearranging.\nWe guessed that the single bump/double hump might be related to seasonal differences in temperatures. To test this hypothesis we added a column to the dataset to designate “summer” vs. “winter,” and then made two charts (faceted) by splitting data on that parameter. Suddenly it was obvious. No longer were we squinting to notice a pattern hidden in the squiggles.\n\nThe “faceting” itself was simple, a feature built into many charting APIs including Observable Plot (which we were using to visualize our data). In hindsight, this seems like an obvious way to split the data. But how often do we step back and actually augment our data with these human-relevant concepts? The key was having the summer/winter parameter to facet on. It doesn’t have to be perfect. Guessing at a date boundary for summer/winter is enough to see that a distinct pattern emerges. Once we have the double-bump/single-bump insight visible here, we can use that insight to go back and look at our data more closely. For example, it appears that there are some daily “double-bump” weeks in the “summer.” Are those boundary weeks that should be classified as winter (or fall or spring)? Or are they unusual weeks during the summer? Moreover, now that we know a defining signal, we could use that signal to classify the data and thereby use the data to identify when energy usage transitions from a “summer” pattern to a “winter” pattern.\n\n\nAugment with data-driven classifications\nThis line chart shows the daily energy use by a single household in Atlanta from March through July 2021. What do you notice? Lots of spikes? Higher energy use in the summer months?\n\nSwitching to a scatterplot makes it more obvious that there are normal-energy days and also high-energy days. Drawing in a line for the moving average plus a (5kwh) buffermakes this split between “normal” and “high-energy” days more clear and shows that the gap is maintained even as energy use overall increases in the summer months.\n\nNow that our exploratory data analysis has revealed two distinct categories (normal and high-energy), we can augment our data by using the moving average to define which points fall into each category. We can then color the points by those new classifications to make it easier to analyze.\n\nIn this way, we complete the circle: we use visualization to notice a key feature of the data and leverage this insight to further classify our data, making the visualization easier to read. And we can take it a step further, continuing to analyze our data based on this classification by creating a histogram showing the frequency of high-energy vs. normal usage days by month. In this view, we can see that in the summer the amount of energy used on normal days went up, and that there were more high-energy days in June and July than in March and April (even after taking into account that the baseline energy usage also went up). Therefore, we can now say with confidence that overall energy consumption increased for two reasons: (1) baseline energy usage increased and (2) a higher percent of days were high-energy days.\n\nThis pattern of looking, then augmenting, then looking again using the classification can also reveal any issues with our classification, like the high point that occurred on our sixth day of data which is mislabeled because the moving average was not defined until the seventh day (as a trailing moving average). This gives us a chance to improve our classification algorithm.\nWhile this example used a very simple algorithm of “moving average + 5kwh” to classify days as “normal” or “high-energy,” this cycle of “look, augment, look, refine classification” becomes more important for machine learning as our algorithms become more opaque.\n\n\n\n#3: Split your data into foreground and background\n\nSplit based on a time period of interest\nWe also dug into data on energy generated by fuel type in Texas in January and February of 2021, including a critical time period in February leading up to and during the rolling blackouts that were initiated to save the electricity grid from collapse following an unusual winter storm. In the analysis story, my colleague Ian faceted the data, creating a chart for each fuel type. This was quite effective: you can immediately see which fuels make up the bulk of energy in Texas, as well as some of the abnormal patterns in mid-February.\n\nKnowing that the critical time period was around February 7 to February 21, Ian further focused attention on those two weeks by making the weeks before and after partially transparent and adding vertical gridlines. He might have been tempted to delete the data outside the period. After all, why waste space on data outside the time period of interest?\n\nBut it’s that granular background data that helps us understand what is so unusual for each fuel type during the critical time period. For example, in coal we’d notice the dip after February 15 regardless, but we need the data from January to notice how unusual the nearly flat plateau between February 1 and February 15 is. Similarly, the January and late-February data for nuclear shows how steady that fuel source typically is, helping us to understand just how strange the dip that we see after February 15 is.\n\n\n\nSplit by comparing each category of interest to the full dataset\nWhen we want to know if there is a relationship between metric A and metric B, the first step is to create a scatterplot. For example, the scatterplot below shows the outdoor temperature and energy demand in Texas for each hour over the course of a year. It’s immediately clear that there is a strong relationship between temperature and energy use (even though that relationship is also obviously non-linear!).\n\nWhile there is clearly a correlation between temperature and electricity demand, it’s also clear that temperature doesn’t tell the whole story. For any given temperature, there is a roughly 10-15K MWh difference from minimum to maximum energy use. Knowing that in our own homes we crank the thermostat a lot higher on a cold afternoon than on a cold night, we guessed that the hour of day could play a key role in the relationship between temperature and energy use.\nThe standard approach to adding an additional category to a scatterplot is to apply a categorical color, thereby comparing everything to everything (comparing all hours, temperatures, and energy demand in one chart). If we do that, we do see that something is going on. More greens and blues in the top right, more pinks low. But to understand what the colors refer to, you have to look back and forth between the legend and the data a lot. Moreover, it’s impossible to answer a question like, “What’s the relationship between temperature and energy at 10am?” Or, “How does early morning compare to evening?\n\nInstead we can apply two techniques: grouping and splitting the data into foreground and background.\nIn the three charts below, the dots representing 5am, 10am, and 6pm are colored brightly. Meanwhile, the entire dataset is shown in grey in the background. This gives us the context to see the relationship between temp and energy for each hour, and see that in the context of the full dataset.\nBy specifically comparing “5am” to “all other times of day,” we can see that 5am is relatively low energy use regardless of temperature (and temperatures are never very high at 5am). Meanwhile, at 6pm energy use is generally higher at all temperatures.\n10am is in some ways the most interesting: at lower temperatures (in the left half of the graph) the yellow dots are relatively high compared to the grey dots, indicating high energy use relative to other hours of the day at the same temperature. Meanwhile, for high temperatures on the right half of the graph, the yellow dots hug the bottom of the grey area. At hot temperatures, relatively little energy is used at 10am compared to the rest of the day. This type of insight is made possible not just by grouping, but also by using the full “noisy” dataset as a consistent background providing context for all the faceted charts."
  },
  {
    "objectID": "visualisation/stop-aggregating-signal.html#summary-embrace-the-complexity-of-your-data",
    "href": "visualisation/stop-aggregating-signal.html#summary-embrace-the-complexity-of-your-data",
    "title": "Stop aggregating away the signal in your data",
    "section": "Summary: Embrace the complexity of your data",
    "text": "Summary: Embrace the complexity of your data\nIn the course of creating the Analyzing Time Series Data collection, Ian Johnson, Mike Freeman, and I employed a range of strategies to embrace the complexity of the data instead of relying on standard methods that aggregate it away. Those frustratingly jagged lines are the signal, not the noise.\nWe embraced complexity by:\n\nRearranging data to compare “like to like.”\nAugmenting our data based on the concepts that we know matter and on what we discovered in the data.\nUsing the larger dataset to provide background context for the data of interest (the foreground).\n\nThese approaches are especially powerful for time series data because the underlying daily, weekly, and seasonal patterns can feel so distracting. In particular, consider how these strategies might power real-time data analysis by putting incoming data in a richer historical context for quick visual pattern-matching to identify normal vs. worrisome patterns. At the same time, these foundational techniques also apply to any data that can feel overwhelming and noisy, like machine learning classifications or data resulting from high-throughput scientific experiments.\nAfter seeing each of these techniques in action, perhaps the next time you are about to aggregate your data in order to simplify it, you might instead try to rearrange, augment, or split your data into foreground/background. See the data in its full context to reveal unexpected patterns and prompt new data-driven questions. Embrace complexity by (literally) changing how you look at your data."
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#objectives",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#objectives",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Objectives",
    "text": "Objectives\n\nExample end-to-end supervised learning workflow with Pima Indians\nFocus on conceptual understanding of machine learning\nDemonstrate use of Predictive Power Score (PPS)\nDemonstrate capabilities of low-code tools\nDemonstrate use of average_precision_score (link)\nDemonstrate SHAP values"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#attribution",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#attribution",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Attribution",
    "text": "Attribution\n\nDataset\n\nPima Indians paper (original paper)\nKaggle datacard (link)\n\n\n\nPython libraries\n\nAltair (docs)\nydata-profiling (docs)\nPredictive Power Score (PPS, GitHub, blog)\nPyCaret: open-source, low-code machine learning library in Python that automates machine learning workflows (link)\n\n::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _kg_hide-output=‘true’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ cell_id=‘12b90763c6d74219965fb8e6c934fae6’ deepnote_cell_type=‘code’ editable=‘true’ execution_millis=‘4438’ execution_start=‘1698129191629’ papermill=‘{“duration”:37.436579,“end_time”:“2022-03-01T18:00:17.659547”,“exception”:false,“start_time”:“2022-03-01T17:59:40.222968”,“status”:“completed”}’ slideshow=‘{“slide_type”:“skip”}’ tags=‘[]’ execution_count=1}\nimport altair as alt\nimport pandas as pd\nimport ppscore as pps\nfrom pycaret.classification import *\nimport shap\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import train_test_split\nfrom ydata_profiling import ProfileReport\n\n\n# customize Altair\ndef y_axis():\n    return {\n        \"config\": {\n            \"axisX\": {\"grid\": False},\n            \"axisY\": {\n                \"domain\": False,\n                \"gridDash\": [2, 4],\n                \"tickSize\": 0,\n                \"titleAlign\": \"right\",\n                \"titleAngle\": 0,\n                \"titleX\": -5,\n                \"titleY\": -10,\n            },\n            \"view\": {\n                \"stroke\": \"transparent\",\n                # To keep the same height and width as the default theme:\n                \"continuousHeight\": 300,\n                \"continuousWidth\": 400,\n            },\n        }\n    }\n\n\nalt.themes.register(\"y_axis\", y_axis)\nalt.themes.enable(\"y_axis\");\n:::"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#read-and-explore-the-data",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#read-and-explore-the-data",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Read and explore the data",
    "text": "Read and explore the data\n\n%%time\ndf = pd.read_csv(\"diabetes.csv\").astype({\"Outcome\": bool})\ntrain, test = train_test_split(df, test_size=0.3)\nprofile = ProfileReport(train, minimal=True, title=\"Pima Indians Profiling Report\")\nprofile.to_file(\"pima-indians-profiling-report-minimal.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 5.54 s, sys: 184 ms, total: 5.72 s\nWall time: 1.7 s\n\n\n\nprofile.to_notebook_iframe()"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Investigate features with largest predictive power",
    "text": "Investigate features with largest predictive power\nWe use the Predictive Power Score to evaluate which features have the highest predictive power with respect to Outcome.\n\npredictors = (\n    pps.predictors(train, \"Outcome\")\n    .round(3)\n    .iloc[:, :-1]\n)\nbase = (\n    alt.Chart(predictors)\n    .encode(\n        y=alt.Y(\"x:N\").sort(\"-x\"),\n        x=\"ppscore\",\n        tooltip=[\"x\", \"ppscore\"],\n    )\n)\nbase.mark_bar() + base.mark_text(align=\"center\", dy=-5)"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-colinearity",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#investigate-colinearity",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Investigate colinearity",
    "text": "Investigate colinearity\n\npps.matrix(train)\n\n\n\n\n\n\n\n\nx\ny\nppscore\ncase\nis_valid_score\nmetric\nbaseline_score\nmodel_score\nmodel\n\n\n\n\n0\nPregnancies\nPregnancies\n1.000000\npredict_itself\nTrue\nNone\n0.000000\n1.000000\nNone\n\n\n1\nPregnancies\nGlucose\n0.000000\nregression\nTrue\nmean absolute error\n23.651769\n24.024994\nDecisionTreeRegressor()\n\n\n2\nPregnancies\nBloodPressure\n0.000000\nregression\nTrue\nmean absolute error\n12.748603\n12.961633\nDecisionTreeRegressor()\n\n\n3\nPregnancies\nSkinThickness\n0.000000\nregression\nTrue\nmean absolute error\n13.467412\n13.630828\nDecisionTreeRegressor()\n\n\n4\nPregnancies\nInsulin\n0.000000\nregression\nTrue\nmean absolute error\n79.240223\n85.015086\nDecisionTreeRegressor()\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n76\nOutcome\nInsulin\n0.000000\nregression\nTrue\nmean absolute error\n79.240223\n84.802415\nDecisionTreeRegressor()\n\n\n77\nOutcome\nBMI\n0.035133\nregression\nTrue\nmean absolute error\n5.672812\n5.473511\nDecisionTreeRegressor()\n\n\n78\nOutcome\nDiabetesPedigreeFunction\n0.000000\nregression\nTrue\nmean absolute error\n0.226384\n0.234531\nDecisionTreeRegressor()\n\n\n79\nOutcome\nAge\n0.000000\nregression\nTrue\nmean absolute error\n8.888268\n8.997094\nDecisionTreeRegressor()\n\n\n80\nOutcome\nOutcome\n1.000000\npredict_itself\nTrue\nNone\n0.000000\n1.000000\nNone\n\n\n\n\n81 rows × 9 columns\n\n\n\n\npps_matrix = (\n    pps.matrix(\n        train.loc[:, predictors.query(\"ppscore &gt; 0\")[\"x\"].tolist()],\n    )\n    .loc[:, [\"x\", \"y\", \"ppscore\"]]\n    .round(3)\n)\n(\n    alt.Chart(pps_matrix)\n    .mark_rect()\n    .encode(\n        x=\"x:O\",\n        y=\"y:O\",\n        color=\"ppscore:Q\",\n        tooltip=[\"x\", \"y\", \"ppscore\"])\n).properties(width=500, height=500)"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#build-models",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#build-models",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Build models",
    "text": "Build models\n\ncls = setup(data = train, \n             target = 'Outcome',\n             numeric_imputation = 'mean',\n             feature_selection = False,\n             pca=False,\n             remove_multicollinearity=True,\n             remove_outliers = False,\n             normalize = True,\n             )\nadd_metric('apc', 'APC', average_precision_score, target = 'pred_proba');\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n5752\n\n\n1\nTarget\nOutcome\n\n\n2\nTarget type\nBinary\n\n\n3\nOriginal data shape\n(537, 9)\n\n\n4\nTransformed data shape\n(537, 9)\n\n\n5\nTransformed train set shape\n(375, 9)\n\n\n6\nTransformed test set shape\n(162, 9)\n\n\n7\nNumeric features\n8\n\n\n8\nPreprocess\nTrue\n\n\n9\nImputation type\nsimple\n\n\n10\nNumeric imputation\nmean\n\n\n11\nCategorical imputation\nmode\n\n\n12\nRemove multicollinearity\nTrue\n\n\n13\nMulticollinearity threshold\n0.900000\n\n\n14\nNormalize\nTrue\n\n\n15\nNormalize method\nzscore\n\n\n16\nFold Generator\nStratifiedKFold\n\n\n17\nFold Number\n10\n\n\n18\nCPU Jobs\n-1\n\n\n19\nUse GPU\nFalse\n\n\n20\nLog Experiment\nFalse\n\n\n21\nExperiment Name\nclf-default-name\n\n\n22\nUSI\n10e4\n\n\n\n\n\n\n%%time\nbest_model = compare_models(include=[\"et\", \"lightgbm\", \"rf\", \"dt\"], sort=\"APC\")\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nAPC\nTT (Sec)\n\n\n\n\net\nExtra Trees Classifier\n0.7569\n0.8039\n0.5090\n0.6832\n0.5719\n0.4106\n0.4261\n0.6795\n0.1740\n\n\nrf\nRandom Forest Classifier\n0.7412\n0.7893\n0.5013\n0.6420\n0.5538\n0.3784\n0.3896\n0.6661\n0.0330\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.7252\n0.7776\n0.5090\n0.5987\n0.5452\n0.3519\n0.3574\n0.6378\n0.1310\n\n\ndt\nDecision Tree Classifier\n0.6691\n0.6275\n0.5019\n0.5002\n0.4904\n0.2503\n0.2559\n0.4269\n0.0050\n\n\n\n\n\n\n\n\nCPU times: user 537 ms, sys: 135 ms, total: 672 ms\nWall time: 3.96 s"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#evaluation",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#evaluation",
    "title": "Predicting diabetes with Pima Indians",
    "section": "Evaluation",
    "text": "Evaluation\n\npredictions = (\n    predict_model(best_model, data=test.iloc[:, :-1])\n)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nprediction_label\nprediction_score\n\n\n\n\n752\n3\n108\n62\n24\n0\n26.000000\n0.223\n25\n0\n0.76\n\n\n295\n6\n151\n62\n31\n120\n35.500000\n0.692\n28\n0\n0.57\n\n\n532\n1\n86\n66\n52\n65\n41.299999\n0.917\n29\n0\n0.88\n\n\n426\n0\n94\n0\n0\n0\n0.000000\n0.256\n25\n0\n0.89\n\n\n68\n1\n95\n66\n13\n38\n19.600000\n0.334\n25\n0\n0.97\n\n\n\n\n\n\n\n\nevaluate_model(best_model)"
  },
  {
    "objectID": "notebooks/pima-indians/pima-indians-with-pycaret.html#shap",
    "href": "notebooks/pima-indians/pima-indians-with-pycaret.html#shap",
    "title": "Predicting diabetes with Pima Indians",
    "section": "SHAP",
    "text": "SHAP\n\ninterpret_model(best_model)\n\n\n\n\n\ninterpret_model(best_model, plot=\"reason\", observation=1)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\ninterpret_model(best_model, plot=\"reason\")\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written."
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#objectives",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#objectives",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Objectives",
    "text": "Objectives\n\nExample end-to-end supervised learning workflow with Ames Housing dataset\nFocus on conceptual understanding of machine learning\nDemonstrate use of Predictive Power Score (PPS)\nDemonstrate capabilities of low-code tools"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#attribution",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#attribution",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Attribution",
    "text": "Attribution\n\nDataset\n\nAmes Housing dataset paper (original paper)\nKaggle competition advanced regression techniques (link)\n\n\n\nPython libraries\n\nAltair (docs)\nydata-profiling (docs)\nPredictive Power Score (PPS, GitHub, blog)\nPyCaret: open-source, low-code machine learning library in Python that automates machine learning workflows (link)\n\n\nimport altair as alt\nimport pandas as pd\nimport ppscore as pps\nfrom pycaret.regression import *\nfrom ydata_profiling import ProfileReport\n\n\n# customize Altair\ndef y_axis():\n    return {\n        \"config\": {\n            \"axisX\": {\"grid\": False},\n            \"axisY\": {\n                \"domain\": False,\n                \"gridDash\": [2, 4],\n                \"tickSize\": 0,\n                \"titleAlign\": \"right\",\n                \"titleAngle\": 0,\n                \"titleX\": -5,\n                \"titleY\": -10,\n            },\n            \"view\": {\n                \"stroke\": \"transparent\",\n                # To keep the same height and width as the default theme:\n                \"continuousHeight\": 300,\n                \"continuousWidth\": 400,\n            },\n        }\n    }\n\n\nalt.themes.register(\"y_axis\", y_axis)\nalt.themes.enable(\"y_axis\")\n\n\ndef get_descriptions():\n    \"Parse descriptions of columns of Ames Housing dataset\"\n    with open(\"data_description.txt\") as reader:\n        descriptions = {}\n        for line in reader.readlines():\n            if \":\" in line and \"2nd level\" not in line:\n                descriptions[line.split(\": \")[0].strip()] = line.split(\": \")[1].strip()\n    return pd.Series(descriptions).rename(\"descriptions\")\n\n\ndescriptions = get_descriptions()"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#read-and-explore-the-data",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#read-and-explore-the-data",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Read and explore the data",
    "text": "Read and explore the data\n\n%%time\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\nprofile = ProfileReport(train, minimal=True, title=\"Ames Housing Profiling Report\")\nprofile.to_file(\"ames-housing-profiling-report-minimal.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 45 s, sys: 1.78 s, total: 46.7 s\nWall time: 16.4 s\n\n\n\nprofile.to_notebook_iframe()"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-features-with-largest-predictive-power",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Investigate features with largest predictive power",
    "text": "Investigate features with largest predictive power\nWe use the Predictive Power Score to evaluate which features have the highest predictive power with respect to SalePrice.\n\npredictors = (\n    pps.predictors(train, \"SalePrice\")\n    .round(3)\n    .iloc[:, :-1]\n    .merge(descriptions, how=\"left\", left_on=\"x\", right_index=True)\n)\nbase = (\n    alt.Chart(predictors)\n    .encode(\n        x=alt.Y(\"x:N\").sort(\"-y\"),\n        y=\"ppscore\",\n        tooltip=[\"x\", \"ppscore\", \"descriptions\"],\n    )\n    .transform_filter(\"datum.ppscore &gt; 0\")\n)\nbase.mark_bar() + base.mark_text(align=\"center\", dy=-5)"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-colinearity",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#investigate-colinearity",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Investigate colinearity",
    "text": "Investigate colinearity\n\npps_matrix = (\n    pps.matrix(\n        train.loc[:, predictors.query(\"ppscore &gt; 0\")[\"x\"].tolist()],\n    )\n    .loc[:, [\"x\", \"y\", \"ppscore\"]]\n    .round(3)\n)\n(\n    alt.Chart(pps_matrix)\n    .mark_rect()\n    .encode(\n        x=\"x:O\",\n        y=\"y:O\",\n        color=\"ppscore:Q\",\n        tooltip=[\"x\", \"y\", \"ppscore\"])\n)"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#build-models",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#build-models",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Build models",
    "text": "Build models\nWe select the 30 features that have the highest predictive power score\n\nselected_predictors = (\n    predictors.sort_values(\"ppscore\", ascending=False).head(30)[\"x\"].to_list()\n)\nreg = setup(data = train.loc[:, selected_predictors + [\"SalePrice\"]], \n             target = 'SalePrice',\n             numeric_imputation = 'mean',\n             categorical_features =  list(train.loc[:, selected_predictors].select_dtypes(\"object\").columns), \n             feature_selection = False,\n             pca=False,\n             remove_multicollinearity=True,\n             remove_outliers = False,\n             normalize = True,\n             )\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n8378\n\n\n1\nTarget\nSalePrice\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(1460, 31)\n\n\n4\nTransformed data shape\n(1460, 116)\n\n\n5\nTransformed train set shape\n(1021, 116)\n\n\n6\nTransformed test set shape\n(439, 116)\n\n\n7\nOrdinal features\n1\n\n\n8\nNumeric features\n16\n\n\n9\nCategorical features\n14\n\n\n10\nRows with missing values\n94.7%\n\n\n11\nPreprocess\nTrue\n\n\n12\nImputation type\nsimple\n\n\n13\nNumeric imputation\nmean\n\n\n14\nCategorical imputation\nmode\n\n\n15\nMaximum one-hot encoding\n25\n\n\n16\nEncoding method\nNone\n\n\n17\nRemove multicollinearity\nTrue\n\n\n18\nMulticollinearity threshold\n0.900000\n\n\n19\nNormalize\nTrue\n\n\n20\nNormalize method\nzscore\n\n\n21\nFold Generator\nKFold\n\n\n22\nFold Number\n10\n\n\n23\nCPU Jobs\n-1\n\n\n24\nUse GPU\nFalse\n\n\n25\nLog Experiment\nFalse\n\n\n26\nExperiment Name\nreg-default-name\n\n\n27\nUSI\n81f6\n\n\n\n\n\n\n%%time\nselected_models = [model for model in models().index if model not in [\"lar\", \"lr\", \"ransac\"]]\nbest_model = compare_models(sort='RMSLE', include=selected_models)\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\nlightgbm\nLight Gradient Boosting Machine\n18267.8967\n969345616.0929\n30245.0381\n0.8400\n0.1474\n0.1051\n0.3780\n\n\ngbr\nGradient Boosting Regressor\n18349.4461\n1064907228.1139\n31464.4286\n0.8221\n0.1497\n0.1059\n0.0810\n\n\nrf\nRandom Forest Regressor\n18834.6022\n1052157810.7295\n31669.8884\n0.8263\n0.1530\n0.1091\n0.1370\n\n\npar\nPassive Aggressive Regressor\n18695.3332\n1145943934.1128\n32527.7429\n0.8093\n0.1535\n0.1061\n0.0560\n\n\nen\nElastic Net\n19941.1185\n1212771199.2709\n33679.9238\n0.8018\n0.1536\n0.1131\n0.0370\n\n\net\nExtra Trees Regressor\n19749.8604\n1158574471.9795\n33510.6172\n0.8073\n0.1591\n0.1138\n0.1370\n\n\nhuber\nHuber Regressor\n18580.7407\n1172797296.1965\n32571.8573\n0.8024\n0.1602\n0.1069\n0.0420\n\n\nbr\nBayesian Ridge\n20557.3468\n1251454965.3245\n34036.3809\n0.7934\n0.1715\n0.1191\n0.0380\n\n\nard\nAutomatic Relevance Determination\n20446.5401\n1229331466.4696\n33711.3986\n0.7969\n0.1747\n0.1193\n0.2740\n\n\nomp\nOrthogonal Matching Pursuit\n21882.7966\n1294135379.9217\n34955.8947\n0.7847\n0.1849\n0.1296\n0.0340\n\n\nada\nAdaBoost Regressor\n24866.3282\n1379609584.9159\n36498.7175\n0.7707\n0.2036\n0.1621\n0.0580\n\n\nknn\nK Neighbors Regressor\n26571.2016\n1730405638.7521\n40931.3774\n0.7200\n0.2050\n0.1518\n0.0360\n\n\ndt\nDecision Tree Regressor\n27747.5148\n2157234242.4490\n45330.0191\n0.6512\n0.2169\n0.1564\n0.0350\n\n\nllar\nLasso Least Angle Regression\n21458.2025\n1320695830.3446\n35006.4301\n0.7809\n0.2187\n0.1268\n0.0380\n\n\nlasso\nLasso Regression\n21455.6793\n1320742178.3808\n35006.1951\n0.7809\n0.2189\n0.1268\n0.2100\n\n\nridge\nRidge Regression\n21439.2241\n1318937040.3720\n34981.0548\n0.7812\n0.2196\n0.1266\n0.0360\n\n\nsvm\nSupport Vector Regression\n55543.3805\n6417749387.4994\n79739.3850\n-0.0524\n0.3979\n0.3195\n0.0450\n\n\ndummy\nDummy Regressor\n57352.4774\n6133919031.8184\n78021.7431\n-0.0086\n0.4061\n0.3635\n0.0340\n\n\ntr\nTheilSen Regressor\n29178.3219\n2564758742.0908\n49572.3895\n0.5667\n0.4258\n0.1978\n4.0290\n\n\nkr\nKernel Ridge\n182040.0692\n34133507087.4154\n184731.2672\n-4.7500\n1.7994\n1.1623\n0.0380\n\n\nmlp\nMLP Regressor\n166456.5847\n32851392125.7179\n181040.0031\n-4.4796\n2.7703\n0.9182\n0.2890\n\n\n\n\n\n\n\n\nCPU times: user 4.09 s, sys: 446 ms, total: 4.53 s\nWall time: 1min 3s"
  },
  {
    "objectID": "notebooks/ames-housing/ames-housing-with-pycaret.html#evaluation",
    "href": "notebooks/ames-housing/ames-housing-with-pycaret.html#evaluation",
    "title": "Predicting house prices in Ames, Iowa",
    "section": "Evaluation",
    "text": "Evaluation\n\nWith a standard, AutoML-like workflow, we achive RMSLE of 0.13 - 0.14 (over different runs), which is already in the top 25% of the 4,200 submissions on the leaderboard\nWe can now make predictions on the test set\n\n\npredictions = (\n    predict_model(best_model, data=test)\n    .rename(columns={\"prediction_label\": \"SalePrice\"})\n    .loc[:, [\"Id\", \"SalePrice\"]]\n)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nId\nSalePrice\n\n\n\n\n0\n1461\n126951.931078\n\n\n1\n1462\n142402.002648\n\n\n2\n1463\n185086.014955\n\n\n3\n1464\n191718.590497\n\n\n4\n1465\n186412.972060\n\n\n\n\n\n\n\n\nPipeline\n\nplot_model(best_model, 'pipeline')\n\n\n\n\n\nplot_model(best_model, 'feature')\n\n\n\n\n\n\n\n\nplot_model(best_model, 'residuals')"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Example notebooks",
    "section": "",
    "text": "Predicting diabetes with Pima Indians\n\n\n\n\n\n\nDaniel Kapitan\n\n\nNov 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting house prices in Ames, Iowa\n\n\n\n\n\n\nDaniel Kapitan\n\n\nOct 21, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "visualisation/index.html",
    "href": "visualisation/index.html",
    "title": "Data visualisation",
    "section": "",
    "text": "Stop aggregating away the signal in your data\n\n\nBy aggregating our data in an effort to simplify it, we lose the signal and the context we need to make sense of what we’re seeing. Originally published on &lt;a…\n\n\n\nZan Armstrong\n\n\nMar 3, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "basics/precision-recall.html",
    "href": "basics/precision-recall.html",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "",
    "text": "A receiver operating characteristic (ROC) curve displays how well a model can classify binary outcomes. An ROC curve is generated by plotting the false positive rate of a model against its true positive rate, for each possible cutoff value. Often, the area under the curve (AUC) is calculated and used as a metric showing how well a model can classify data points.\nIf you’re interest in learning more about ROC and AUC, I recommend this short Medium blog, which contains this neat graphic.\n\nDariya Sydykova, graduate student at the Wilke lab at the University of Texas at Austin, shared some great visual animations of how model accuracy and model cutoffs alter the ROC curve and the AUC metric. The quotes and animations are from the associated github repository."
  },
  {
    "objectID": "basics/precision-recall.html#roc-auc-precision-and-recall-visually-explained",
    "href": "basics/precision-recall.html#roc-auc-precision-and-recall-visually-explained",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "",
    "text": "A receiver operating characteristic (ROC) curve displays how well a model can classify binary outcomes. An ROC curve is generated by plotting the false positive rate of a model against its true positive rate, for each possible cutoff value. Often, the area under the curve (AUC) is calculated and used as a metric showing how well a model can classify data points.\nIf you’re interest in learning more about ROC and AUC, I recommend this short Medium blog, which contains this neat graphic.\n\nDariya Sydykova, graduate student at the Wilke lab at the University of Texas at Austin, shared some great visual animations of how model accuracy and model cutoffs alter the ROC curve and the AUC metric. The quotes and animations are from the associated github repository."
  },
  {
    "objectID": "basics/precision-recall.html#roc-auc",
    "href": "basics/precision-recall.html#roc-auc",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "ROC & AUC",
    "text": "ROC & AUC\nThe plot on the left shows the distributions of predictors for the two outcomes, and the plot on the right shows the ROC curve for these distributions. The vertical line that travels left-to-right is the cutoff value. The red dot that travels along the ROC curve corresponds to the false positive rate and the true positive rate for the cutoff value given in the plot on the left.\nThe traveling cutoff demonstrates the trade-off between trying to classify one outcome correctly and trying to classify the other outcome correcly. When we try to increase the true positive rate, we also increase the false positive rate. When we try to decrease the false positive rate, we decrease the true positive rate.\n\nThe shape of an ROC curve changes when a model changes the way it classifies the two outcomes.\nThe animation [below] starts with a model that cannot tell one outcome from the other, and the two distributions completely overlap (essentially a random classifier). As the two distributions separate, the ROC curve approaches the left-top corner, and the AUC value of the curve increases. When the model can perfectly separate the two outcomes, the ROC curve forms a right angle and the AUC becomes 1."
  },
  {
    "objectID": "basics/precision-recall.html#precision-recall",
    "href": "basics/precision-recall.html#precision-recall",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Precision-recall",
    "text": "Precision-recall\nTwo other metrics that are often used to quantify model performance are precision and recall.\nPrecision (also called positive predictive value) is defined as the number of true positives divided by the total number of positive predictions. Hence, precision quantifies what percentage of the positive predictions were correct: How correct your model’s positive predictions were.\nRecall (also called sensitivity) is defined as the number of true positives divided by the total number of true postives and false negatives (i.e. all actual positives). Hence, recall quantifies what percentage of the actual positives you were able to identify: How sensitive your model was in identifying positives.\nDariya also made some visualizations of precision-recall curves: precision-recall curves also displays how well a model can classify binary outcomes. However, it does it differently from the way an ROC curve does. Precision-recall curve plots true positive rate (recall or sensitivity) against the positive predictive value (precision).\nIn the middle, here below, the ROC curve with AUC. On the right, the associated precision-recall curve. Similarly to the ROC curve, when the two outcomes separate, precision-recall curves will approach the top-right corner. Typically, a model that produces a precision-recall curve that is closer to the top-right corner is better than a model that produces a precision-recall curve that is skewed towards the bottom of the plot."
  },
  {
    "objectID": "basics/precision-recall.html#class-imbalance",
    "href": "basics/precision-recall.html#class-imbalance",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Class imbalance",
    "text": "Class imbalance\nClass imbalance happens when the number of outputs in one class is different from the number of outputs in another class. For example, one of the distributions has 1000 observations and the other has 10. An ROC curve tends to be more robust to class imbalanace that a precision-recall curve.\nIn this animation [below], both distributions start with 1000 outcomes. The blue one is then reduced to 50. The precision-recall curve changes shape more drastically than the ROC curve, and the AUC value mostly stays the same. We also observe this behaviour when the other disribution is reduced to 50.\n\nHere’s the same, but now with the red distribution shrinking to just 50 samples."
  },
  {
    "objectID": "basics/precision-recall.html#further-reading",
    "href": "basics/precision-recall.html#further-reading",
    "title": "The Precision-Recall Plot Is More Informative than the ROC Plot",
    "section": "Further reading",
    "text": "Further reading\n\nSaito & Rehmsmeier (2015), The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets"
  }
]